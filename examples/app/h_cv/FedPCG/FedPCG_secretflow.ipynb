{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FedPCG:一种利用聚类采样和全局原型的个性化联邦学习方法,基于隐语框架和FedNH baseline实现, reference:Dai, Y., Chen, Z., Li, J., Heinecke, S., Sun, L., & Xu, R. (2023, June). Tackling data heterogeneity in federated learning with class prototypes. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 6, pp. 7314-7322). https://github.com/Yutong-Dai/FedNH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secretflow as sf\n",
    "from secretflow import PYUObject, proxy\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import compress, product\n",
    "from PIL import Image\n",
    "from collections import OrderedDict, Counter\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "global wandb_installed\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    wandb_installed = True\n",
    "except ModuleNotFoundError:\n",
    "    wandb_installed = False\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些超参数设置，可以通过同级目录下的config.ini设置对应的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Test Algorithms.\")\n",
    "    # general settings\n",
    "    parser.add_argument(\n",
    "        \"--purpose\", default=\"experiments\", type=str, help=\"purpose of this run\"\n",
    "    )\n",
    "    parser.add_argument(\"--device\", default=\"cuda:1\", type=str, help=\"cuda device\")\n",
    "    parser.add_argument(\n",
    "        \"--global_seed\", default=2022, type=int, help=\"Global random seed.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_wandb\",\n",
    "        default=True,\n",
    "        type=lambda x: (str(x).lower() in [\"true\", \"1\", \"yes\"]),\n",
    "        help=\"Use wandb pkg\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--keep_clients_model\",\n",
    "        default=False,\n",
    "        type=lambda x: (str(x).lower() in [\"true\", \"1\", \"yes\"]),\n",
    "        help=\"Keep FedAVG local model\",\n",
    "    )\n",
    "    # model architecture\n",
    "    parser.add_argument(\n",
    "        \"--no_norm\",\n",
    "        default=False,\n",
    "        type=lambda x: (str(x).lower() in [\"true\", \"1\", \"yes\"]),\n",
    "        help=\"Use group/batch norm or not\",\n",
    "    )\n",
    "    # optimizer\n",
    "    parser.add_argument(\"--optimizer\", default=\"SGD\", type=str, help=\"Optimizer\")\n",
    "    parser.add_argument(\"--num_epochs\", default=5, type=int, help=\"num local epochs\")\n",
    "    parser.add_argument(\n",
    "        \"--client_lr\", default=0.1, type=float, help=\"client side initial learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--client_lr_scheduler\",\n",
    "        default=\"stepwise\",\n",
    "        type=str,\n",
    "        help=\"client side learning rate update strategy\",\n",
    "    )\n",
    "    parser.add_argument(\"--sgd_momentum\", default=0.0, type=float, help=\"sgd momentum\")\n",
    "    parser.add_argument(\n",
    "        \"--sgd_weight_decay\", default=1e-5, type=float, help=\"sgd weight decay\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_sam\",\n",
    "        default=False,\n",
    "        type=lambda x: (str(x).lower() in [\"true\", \"1\", \"yes\"]),\n",
    "        help=\"Use SAM optimizer\",\n",
    "    )\n",
    "    # server config\n",
    "    parser.add_argument(\n",
    "        \"--yamlfile\", default=None, type=str, help=\"Configuration file.\"\n",
    "    )\n",
    "    parser.add_argument(\"--strategy\", default=None, type=str, help=\"strategy FL\")\n",
    "    parser.add_argument(\n",
    "        \"--num_clients\", default=100, type=int, help=\"number of clients\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_rounds\", default=200, type=int, help=\"number of communication rounds\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--participate_ratio\", default=0.1, type=float, help=\"participate ratio\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--partition\", default=None, type=str, help=\"method for partition the dataset\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--beta\", default=None, type=str, help=\"Dirichlet Distribution parameter\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_classes_per_client\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"pathological non-iid parameter\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_shards_per_client\",\n",
    "        default=None,\n",
    "        type=int,\n",
    "        help=\"pathological non-iid parameter fedavg simulation\",\n",
    "    )\n",
    "\n",
    "    # strategy parameters\n",
    "    parser.add_argument(\n",
    "        \"--FedNH_smoothing\", default=0.9, type=float, help=\"moving average parameters\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--FedNH_server_adv_prototype_agg\",\n",
    "        default=False,\n",
    "        type=lambda x: (str(x).lower() in [\"true\", \"1\", \"yes\"]),\n",
    "        help=\"FedNH server adv agg\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--FedNH_client_adv_prototype_agg\",\n",
    "        default=False,\n",
    "        type=lambda x: (str(x).lower() in [\"true\", \"1\", \"yes\"]),\n",
    "        help=\"FedNH client adv agg\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--FedROD_hyper_clf\",\n",
    "        default=True,\n",
    "        type=lambda x: (str(x).lower() in [\"true\", \"1\", \"yes\"]),\n",
    "        help=\"FedRod phead uses hypernetwork\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--FedROD_phead_separate\",\n",
    "        default=False,\n",
    "        type=lambda x: (str(x).lower() in [\"true\", \"1\", \"yes\"]),\n",
    "        help=\"FedROD phead separate train\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--FedProto_lambda\",\n",
    "        default=0.1,\n",
    "        type=float,\n",
    "        help=\"FedProto local penalty lambda\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--FedRep_head_epochs\",\n",
    "        default=10,\n",
    "        type=int,\n",
    "        help=\"FedRep local epochs to update head\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--FedBABU_finetune_epoch\",\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help=\"FedBABU local epochs to finetune\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--Ditto_lambda\", default=0.75, type=float, help=\"penalty parameter for Ditto\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--CReFF_num_of_fl_feature\",\n",
    "        default=100,\n",
    "        type=int,\n",
    "        help=\"num of federated feature per class\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--CReFF_match_epoch\",\n",
    "        default=100,\n",
    "        type=int,\n",
    "        help=\"epoch used to minmize gradient matching loss\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--CReFF_crt_epoch\",\n",
    "        default=300,\n",
    "        type=int,\n",
    "        help=\"epoch used to retrain classifier\",\n",
    "    )\n",
    "    parser.add_argument(\"--CReFF_lr_net\", default=0.01, type=float, help=\"lr for head\")\n",
    "    parser.add_argument(\n",
    "        \"--CReFF_lr_feature\", default=0.1, type=float, help=\"lr for feature\"\n",
    "    )\n",
    "\n",
    "    arg_list = None\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    # 其实是个字典:\n",
    "    # print(config['train']['batch_size'])\n",
    "    arg_list = []\n",
    "    for k, v in config[\"train\"].items():\n",
    "        arg_list.append(\"--\" + k)\n",
    "        arg_list.append(v)\n",
    "\n",
    "    args = parser.parse_args(arg_list)\n",
    "    return args\n",
    "\n",
    "\n",
    "args = args_parser()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机种子设置，将args传入的部分参数覆盖config中的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "use_wandb = wandb_installed and args.use_wandb\n",
    "setup_seed(args.global_seed)\n",
    "\n",
    "with open(args.yamlfile, \"r\", encoding=\"utf-8\") as stream:\n",
    "    config = yaml.load(stream, Loader=yaml.Loader)\n",
    "\n",
    "# parse the default setting\n",
    "server_config = config[\"server_config\"]\n",
    "client_config = config[\"client_config\"]\n",
    "\n",
    "# overwrite with inputs\n",
    "server_config[\"strategy\"] = args.strategy\n",
    "server_config[\"num_clients\"] = args.num_clients\n",
    "server_config[\"num_rounds\"] = args.num_rounds\n",
    "server_config[\"participate_ratio\"] = args.participate_ratio\n",
    "server_config[\"partition\"] = args.partition\n",
    "server_config[\"beta\"] = args.beta\n",
    "server_config[\"num_classes_per_client\"] = args.num_classes_per_client\n",
    "server_config[\"num_shards_per_client\"] = args.num_shards_per_client\n",
    "client_config[\"num_rounds\"] = args.num_rounds\n",
    "client_config[\"global_seed\"] = args.global_seed\n",
    "client_config[\"optimizer\"] = args.optimizer\n",
    "client_config[\"client_lr\"] = args.client_lr\n",
    "client_config[\"client_lr_scheduler\"] = args.client_lr_scheduler\n",
    "client_config[\"sgd_momentum\"] = args.sgd_momentum\n",
    "client_config[\"sgd_weight_decay\"] = args.sgd_weight_decay\n",
    "client_config[\"use_sam\"] = args.use_sam\n",
    "client_config[\"no_norm\"] = args.no_norm\n",
    "\n",
    "if server_config[\"partition\"] == \"noniid-label-distribution\":\n",
    "    partition_arg = f\"beta:{args.beta}\"\n",
    "elif server_config[\"partition\"] == \"noniid-label-quantity\":\n",
    "    partition_arg = f\"num_classes_per_client:{args.num_classes_per_client}\"\n",
    "elif server_config[\"partition\"] == \"shards\":\n",
    "    partition_arg = f\"num_shards_per_client:{args.num_shards_per_client}\"\n",
    "else:\n",
    "    raise ValueError(\"not implemented partition\")\n",
    "print(server_config)\n",
    "print(client_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "客户端基类，主要用于联邦学习环境下各客户端的method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, criterion, trainset, testset, client_config, cid, **kwargs):\n",
    "        autoassign(locals())\n",
    "        if trainset is not None:\n",
    "            self.num_train_samples = len(trainset)\n",
    "        else:\n",
    "            self.num_train_samples = 0\n",
    "        if testset is not None:\n",
    "            self.num_test_samples = len(testset)\n",
    "        else:\n",
    "            self.num_test_samples = 0\n",
    "\n",
    "        if not torch.cuda.is_available():\n",
    "            self.device = \"cpu\"\n",
    "            print(\"cuda is not available. use cpu instead.\")\n",
    "        # wrap the trainset and testset with dataloader\n",
    "        self._prepare_data()\n",
    "        # local stats\n",
    "        self.num_rounds_particiapted = 0\n",
    "        self.train_loss_dict = OrderedDict()\n",
    "        self.train_acc_dict = OrderedDict()\n",
    "        self.test_loss_dict = OrderedDict()\n",
    "        self.test_acc_dict = OrderedDict()\n",
    "        self.new_state_dict = None\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        self.label_dist = None\n",
    "        train_batchsize = min(self.client_config[\"batch_size\"], self.num_train_samples)\n",
    "        test_batchsize = min(\n",
    "            self.client_config[\"batch_size\"] * 2, self.num_test_samples\n",
    "        )\n",
    "\n",
    "        if self.num_train_samples > 0:\n",
    "            self.trainloader = DataLoader(\n",
    "                self.trainset, batch_size=train_batchsize, shuffle=True\n",
    "            )\n",
    "            # summarize training set label distribution\n",
    "            self.count_by_class = Counter(self.trainset.targets.numpy())\n",
    "            self.label_dist = {\n",
    "                i: self.count_by_class[i] / self.num_train_samples\n",
    "                for i in sorted(self.count_by_class.keys())\n",
    "            }\n",
    "        else:\n",
    "            self.trainloader = None\n",
    "\n",
    "        if self.num_test_samples > 0:\n",
    "            self.testloader = DataLoader(\n",
    "                self.testset, batch_size=test_batchsize, shuffle=False\n",
    "            )\n",
    "            self.count_by_class_test = Counter(self.testset.targets.numpy())\n",
    "            self.label_dist_test = {\n",
    "                i: self.count_by_class_test[i] / self.num_test_samples\n",
    "                for i in sorted(self.count_by_class_test.keys())\n",
    "            }\n",
    "        else:\n",
    "            self.testloader = None\n",
    "\n",
    "    def set_params(self, model_state_dict, exclude_keys):\n",
    "        self.model.set_params(model_state_dict, exclude_keys)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.model.get_params()\n",
    "\n",
    "    def get_params_values(self):\n",
    "        return self.model.get_params_values()\n",
    "\n",
    "    def get_model_parameters(self):\n",
    "        return list(self.model.get_parameters())\n",
    "\n",
    "    def get_model(self):\n",
    "        self.model.eval()\n",
    "        return self.model\n",
    "\n",
    "    def get_grads(self, dataloader):\n",
    "        return self.model.get_grads(dataloader)\n",
    "\n",
    "    def initialize_model(self):\n",
    "        raise NotImplementedError(\n",
    "            \"Please write a method for the client to initialize the model(s).\"\n",
    "        )\n",
    "\n",
    "    def training(self, round, num_epochs):\n",
    "        raise NotImplementedError(\"Please write a training method for the client.\")\n",
    "\n",
    "    def testing(self, round, testloader=None):\n",
    "        \"\"\"\n",
    "        Provide testloader if one wants to use the externel testing dataset.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Please write a testing method for the client.\")\n",
    "\n",
    "    def upload(self):\n",
    "        \"\"\"\n",
    "        Decide what information to share with the server\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "服务器基类，创建一个fake客户端用于全局模型性能测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_client_pyu = sf.PYU(\"fake_client\")\n",
    "\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, server_config, clients_dict, **kwargs):\n",
    "        \"\"\" \"\"\"\n",
    "        autoassign(locals())\n",
    "        self.server_model_state_dict = None\n",
    "        self.server_model_state_dict_best_so_far = None\n",
    "        self.num_clients = len(self.clients_dict)\n",
    "        self.strategy = None\n",
    "        self.average_train_loss_dict = {}\n",
    "        self.average_train_acc_dict = {}\n",
    "        # global model performance\n",
    "        self.gfl_test_loss_dict = {}\n",
    "        self.gfl_test_acc_dict = {}\n",
    "        # local model performance (averaged across all clients)\n",
    "        self.average_pfl_test_loss_dict = {}\n",
    "        self.average_pfl_test_acc_dict = {}\n",
    "        self.active_clients_indicies = None\n",
    "        self.rounds = 0\n",
    "        #         # create a fake client on the server side; use for testing the performance of the global model\n",
    "        #         # trainset is only used for creating the label distribution\n",
    "        self.server_side_client = kwargs[\"client_cstr\"](\n",
    "            kwargs[\"server_side_criterion\"],\n",
    "            kwargs[\"global_trainset\"],\n",
    "            kwargs[\"global_testset\"],\n",
    "            kwargs[\"server_side_client_config\"],\n",
    "            -1,\n",
    "            device=fake_client_pyu,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def select_clients(self, ratio):\n",
    "        assert (\n",
    "            ratio > 0.0\n",
    "        ), \"Invalid ratio. Possibly the server_config['participate_ratio'] is wrong.\"\n",
    "        num_clients = int(ratio * self.num_clients)\n",
    "        selected_indices = np.random.choice(\n",
    "            range(self.num_clients), num_clients, replace=False\n",
    "        )\n",
    "        return selected_indices\n",
    "\n",
    "    def testing(self, round, active_only, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def collect_stats(self, stage, round, active_only, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def aggregate(self, client_uploads, round):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def run(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, filename, keep_clients_model=False):\n",
    "        if not keep_clients_model:\n",
    "            for client in self.clients_dict.values():\n",
    "                client.model = None\n",
    "                client.trainloader = None\n",
    "                client.trainset = None\n",
    "                client.new_state_dict = None\n",
    "        self.server_side_client.trainloader = None\n",
    "        self.server_side_client.trainset = None\n",
    "        self.server_side_client.testloader = None\n",
    "        self.server_side_client.testset = None\n",
    "        save_to_pkl(self, filename)\n",
    "\n",
    "    def summary_setup(self):\n",
    "        info = \"=\" * 30 + \"Run Summary\" + \"=\" * 30\n",
    "        info += \"\\nDataset:\\n\"\n",
    "        info += f\" dataset:{self.server_config['dataset']} | num_classes:{self.server_config['num_classes']}\"\n",
    "        partition = self.server_config[\"partition\"]\n",
    "        info += f\" | partition:{self.server_config['partition']}\"\n",
    "        if partition == \"iid-equal-size\":\n",
    "            info += \"\\n\"\n",
    "        elif partition in [\"iid-diff-size\", \"noniid-label-distribution\"]:\n",
    "            info += f\" | beta:{self.server_config['beta']}\\n\"\n",
    "        elif partition == \"noniid-label-quantity\":\n",
    "            info += f\" | num_classes_per_client:{self.server_config['num_classes_per_client']}\\n \"\n",
    "        else:\n",
    "            if \"shards\" in partition.split(\"-\"):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\" Invalid dataset partition strategy:{partition}!\")\n",
    "        info += \"Server Info:\\n\"\n",
    "        info += f\" strategy:{self.server_config['strategy']} | num_clients:{self.server_config['num_clients']} | num_rounds: {self.server_config['num_rounds']}\"\n",
    "        info += f\" | participate_ratio:{self.server_config['participate_ratio']} | drop_ratio:{self.server_config['drop_ratio']}\\n\"\n",
    "        info += f\"Clients Info:\\n\"\n",
    "        info += f\" model:{client_config['model']} | num_epochs:{client_config['num_epochs']} | batch_size:{client_config['batch_size']}\"\n",
    "        info += f\" | optimizer:{client_config['optimizer']} | inint lr:{client_config['client_lr']} | lr scheduler:{client_config['client_lr_scheduler']} | momentum: {client_config['sgd_momentum']} | weight decay: {client_config['sgd_weight_decay']}\"\n",
    "        print(info)\n",
    "\n",
    "    def summary_result(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类任务模型架构，对模型架构进行了解耦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"For classification problem\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.parameters()\n",
    "\n",
    "    def get_gradients(self, dataloader):\n",
    "        raise NotImplementedErrorm\n",
    "\n",
    "    def set_params(self, model_state_dict, exclude_keys=set()):\n",
    "        \"\"\"\n",
    "        Reference: Be careful with the state_dict[key].\n",
    "        https://discuss.pytorch.org/t/how-to-copy-a-modified-state-dict-into-a-models-state-dict/64828/4.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for key in model_state_dict.keys():\n",
    "                if key not in exclude_keys:\n",
    "                    self.state_dict()[key].copy_(model_state_dict[key])\n",
    "\n",
    "\n",
    "class ModelWrapper(Model):\n",
    "    def __init__(self, base, head, config):\n",
    "        \"\"\"\n",
    "        head and base should be nn.module\n",
    "        \"\"\"\n",
    "        super(ModelWrapper, self).__init__(config)\n",
    "\n",
    "        self.base = base\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x, return_embedding):\n",
    "        feature_embedding = self.base(x)\n",
    "        out = self.head(feature_embedding)\n",
    "        if return_embedding:\n",
    "            return feature_embedding, out\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class Conv2Cifar(Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.linear1 = nn.Linear(64 * 5 * 5, 384)\n",
    "        self.linear2 = nn.Linear(384, 192)\n",
    "        # intentionally remove the bias term for the last linear layer for fair comparison\n",
    "        self.prototype = nn.Linear(192, config[\"num_classes\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        logits = self.prototype(x)\n",
    "        return logits\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        logits = self.prototype(x)\n",
    "        return x, logits\n",
    "\n",
    "\n",
    "class Conv2CifarNH(Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.return_embedding = config[\"FedNH_return_embedding\"]\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.linear1 = nn.Linear(64 * 5 * 5, 384)\n",
    "        self.linear2 = nn.Linear(384, 192)\n",
    "        temp = nn.Linear(192, config[\"num_classes\"], bias=False).state_dict()[\"weight\"]\n",
    "        self.prototype = nn.Parameter(temp)\n",
    "        self.scaling = torch.nn.Parameter(torch.tensor([1.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        feature_embedding = F.relu(self.linear2(x))\n",
    "        feature_embedding_norm = torch.norm(\n",
    "            feature_embedding, p=2, dim=1, keepdim=True\n",
    "        ).clamp(min=1e-12)\n",
    "        feature_embedding = torch.div(feature_embedding, feature_embedding_norm)\n",
    "        if self.prototype.requires_grad == False:\n",
    "            normalized_prototype = self.prototype\n",
    "        else:\n",
    "            prototype_norm = torch.norm(self.prototype, p=2, dim=1, keepdim=True).clamp(\n",
    "                min=1e-12\n",
    "            )\n",
    "            normalized_prototype = torch.div(self.prototype, prototype_norm)\n",
    "        logits = torch.matmul(feature_embedding, normalized_prototype.T)\n",
    "        logits = self.scaling * logits\n",
    "\n",
    "        if self.return_embedding:\n",
    "            return feature_embedding, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在聚类采样策略的方法上，我们的方法FedPCG结合了模型参数解耦策略，实现了基于head层代表性梯度的聚类采样，有效降低了计算复杂度。reference：Fraboni, Y., Vidal, R., Kameni, L., & Lorenzi, M. (2021, July). Clustered sampling: Low-variance and improved representativity for clients selection in federated learning. In International Conference on Machine Learning (pp. 3407-3416). PMLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientSelection:\n",
    "    def __init__(self, total, device=\"cpu\"):\n",
    "        self.total = total\n",
    "        self.device = device\n",
    "\n",
    "    def select(self, n, client_idxs, metric):\n",
    "        pass\n",
    "\n",
    "    def save_selected_clients(self, client_idxs, results):\n",
    "        tmp = np.zeros(self.total)\n",
    "        tmp[client_idxs] = 1\n",
    "        tmp.tofile(results, sep=\",\")\n",
    "        results.write(\"\\n\")\n",
    "\n",
    "    def save_results(self, arr, results, prefix=\"\"):\n",
    "        results.write(prefix)\n",
    "        np.array(arr).astype(np.float32).tofile(results, sep=\",\")\n",
    "        results.write(\"\\n\")\n",
    "\n",
    "\n",
    "\"\"\"Clustered Sampling Algorithm 1\"\"\"\n",
    "\n",
    "\n",
    "class ClusteredSampling1(ClientSelection):\n",
    "    def __init__(self, total, device, n_cluster):\n",
    "        super().__init__(total, device)\n",
    "        self.n_cluster = n_cluster\n",
    "\n",
    "    def setup(self, n_samples):\n",
    "        \"\"\"\n",
    "        Since clustering is performed according to the clients sample size n_i,\n",
    "        unless n_i changes during the learning process,\n",
    "        Algo 1 needs to be run only once at the beginning of the learning process.\n",
    "        \"\"\"\n",
    "        epsilon = int(10**10)\n",
    "        client_ids = sorted(n_samples.keys())\n",
    "        n_samples = np.array([n_samples[i] for i in client_ids])\n",
    "        weights = n_samples / np.sum(n_samples)\n",
    "        # associate each client to a cluster\n",
    "        augmented_weights = np.array([w * self.n_cluster * epsilon for w in weights])\n",
    "        ordered_client_idx = np.flip(np.argsort(augmented_weights))\n",
    "\n",
    "        distri_clusters = np.zeros((self.n_cluster, self.total)).astype(int)\n",
    "        k = 0\n",
    "        for client_idx in ordered_client_idx:\n",
    "            while augmented_weights[client_idx] > 0:\n",
    "                sum_proba_in_k = np.sum(distri_clusters[k])\n",
    "                u_i = min(epsilon - sum_proba_in_k, augmented_weights[client_idx])\n",
    "                distri_clusters[k, client_idx] = u_i\n",
    "                augmented_weights[client_idx] += -u_i\n",
    "                sum_proba_in_k = np.sum(distri_clusters[k])\n",
    "                if sum_proba_in_k == 1 * epsilon:\n",
    "                    k += 1\n",
    "\n",
    "        distri_clusters = distri_clusters.astype(float)\n",
    "        for l in range(self.n_cluster):\n",
    "            distri_clusters[l] /= np.sum(distri_clusters[l])\n",
    "\n",
    "        self.distri_clusters = distri_clusters\n",
    "\n",
    "    def select(self, n, client_idxs, metric=None):\n",
    "        selected_client_idxs = []\n",
    "        for k in range(n):\n",
    "            weight = np.take(self.distri_clusters[k], client_idxs)\n",
    "            selected_client_idxs.append(\n",
    "                int(np.random.choice(client_idxs, 1, p=weight / sum(weight)))\n",
    "            )\n",
    "        return np.array(selected_client_idxs)\n",
    "\n",
    "\n",
    "\"\"\"Clustered Sampling Algorithm 2\"\"\"\n",
    "\n",
    "\n",
    "class ClusteredSampling2(ClientSelection):\n",
    "    def __init__(self, total, device, dist):\n",
    "        super().__init__(total, device)\n",
    "        self.distance_type = dist\n",
    "\n",
    "    def setup(self, n_samples):\n",
    "        \"\"\"\n",
    "        return the `representative gradient` formed by the difference\n",
    "        between the local work and the sent global model\n",
    "        \"\"\"\n",
    "        client_ids = sorted(n_samples.keys())\n",
    "        n_samples = np.array([n_samples[i] for i in client_ids])\n",
    "        print(\"n_samples\", n_samples)\n",
    "        self.weights = n_samples / sum(n_samples)\n",
    "\n",
    "    def init(self, global_m, local_models):\n",
    "        self.prev_global_m = global_m\n",
    "        self.gradients = self.get_gradients(global_m, local_models)\n",
    "\n",
    "    def select(self, n, client_idxs, metric=None):\n",
    "        # GET THE CLIENTS' SIMILARITY MATRIX\n",
    "        sim_matrix = self.get_matrix_similarity_from_grads(\n",
    "            self.gradients, distance_type=self.distance_type\n",
    "        )\n",
    "        # GET THE DENDROGRAM TREE ASSOCIATED\n",
    "        linkage_matrix = linkage(sim_matrix, \"ward\")\n",
    "\n",
    "        distri_clusters = self.get_clusters_with_alg2(linkage_matrix, n, self.weights)\n",
    "        # sample clients\n",
    "        selected_client_idxs = np.zeros(n, dtype=int)\n",
    "        for k in range(n):\n",
    "            selected_client_idxs[k] = int(\n",
    "                np.random.choice(client_idxs, 1, p=distri_clusters[k])\n",
    "            )\n",
    "\n",
    "        return selected_client_idxs\n",
    "\n",
    "    def update(self, clients_models, sampled_clients_for_grad):\n",
    "        print(\">> update gradients\")\n",
    "        # UPDATE THE HISTORY OF LATEST GRADIENT\n",
    "        gradients_i = self.get_gradients(self.prev_global_m, clients_models)\n",
    "        for idx, gradient in zip(sampled_clients_for_grad, gradients_i):\n",
    "            self.gradients[idx] = gradient\n",
    "\n",
    "    def get_gradients(self, global_m, local_models):\n",
    "        \"\"\"\n",
    "        return the `representative gradient` formed by the difference\n",
    "        between the local work and the sent global model\n",
    "        \"\"\"\n",
    "        local_model_params = []\n",
    "        for model in local_models:\n",
    "            local_model_params += [\n",
    "                [\n",
    "                    tens.detach().to(self.device)\n",
    "                    for tens in list(sf.reveal(model).parameters())\n",
    "                ][0]\n",
    "            ]  # .numpy()\n",
    "\n",
    "        global_model_params = [\n",
    "            tens.detach().to(self.device) for tens in list(sf.reveal(global_m).values())\n",
    "        ][0]\n",
    "\n",
    "        local_model_grads = []\n",
    "        for local_params in local_model_params:\n",
    "            local_model_grads += [\n",
    "                [\n",
    "                    local_weights - global_weights\n",
    "                    for local_weights, global_weights in zip(\n",
    "                        local_params, global_model_params\n",
    "                    )\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "        return local_model_grads\n",
    "\n",
    "    def get_matrix_similarity_from_grads(self, local_model_grads, distance_type):\n",
    "        \"\"\"\n",
    "        return the similarity matrix where the distance chosen to\n",
    "        compare two clients is set with `distance_type`\n",
    "        \"\"\"\n",
    "        n_clients = len(local_model_grads)\n",
    "        metric_matrix = torch.zeros((n_clients, n_clients))\n",
    "        for i, j in tqdm(\n",
    "            product(range(n_clients), range(n_clients)), desc=\">> similarity\", ncols=80\n",
    "        ):\n",
    "            metric_matrix[i, j] = self.get_similarity(\n",
    "                local_model_grads[i], local_model_grads[j], distance_type\n",
    "            )\n",
    "\n",
    "        return metric_matrix\n",
    "\n",
    "    def get_similarity(self, grad_1, grad_2, distance_type=\"L1\"):\n",
    "        if distance_type == \"L1\":\n",
    "            norm = 0\n",
    "            for g_1, g_2 in zip(grad_1, grad_2):\n",
    "                norm += torch.sum(torch.abs(g_1 - g_2))\n",
    "            return norm.cpu().data\n",
    "\n",
    "        elif distance_type == \"L2\":\n",
    "            norm = 0\n",
    "            for g_1, g_2 in zip(grad_1, grad_2):\n",
    "                norm += np.sum((g_1 - g_2) ** 2)\n",
    "            return norm\n",
    "\n",
    "        elif distance_type == \"cosine\":\n",
    "            norm, norm_1, norm_2 = 0, 0, 0\n",
    "            print(grad_1[0].squeeze().shape, grad_2[0].squeeze().shape)\n",
    "            for i in range(len(grad_1)):\n",
    "                norm += np.sum(torch.mul(grad_1[i].squeeze(), grad_2[i].squeeze()))\n",
    "                norm_1 += np.sum(grad_1[i] ** 2)\n",
    "                norm_2 += np.sum(grad_2[i] ** 2)\n",
    "\n",
    "            if norm_1 == 0.0 or norm_2 == 0.0:\n",
    "                return 0.0\n",
    "            else:\n",
    "                norm /= np.sqrt(norm_1 * norm_2)\n",
    "                return np.arccos(norm)\n",
    "\n",
    "    def get_clusters_with_alg2(\n",
    "        self, linkage_matrix: np.array, n_sampled: int, weights: np.array\n",
    "    ):\n",
    "        \"\"\"Algorithm 2\"\"\"\n",
    "        epsilon = int(10**10)\n",
    "\n",
    "        # associate each client to a cluster\n",
    "        link_matrix_p = deepcopy(linkage_matrix)\n",
    "        augmented_weights = deepcopy(weights)\n",
    "\n",
    "        for i in range(len(link_matrix_p)):\n",
    "            idx_1, idx_2 = int(link_matrix_p[i, 0]), int(link_matrix_p[i, 1])\n",
    "\n",
    "            new_weight = np.array(\n",
    "                [\n",
    "                    sf.reveal(augmented_weights)[idx_1]\n",
    "                    + sf.reveal(augmented_weights)[idx_2]\n",
    "                ]\n",
    "            )\n",
    "            augmented_weights = np.concatenate((augmented_weights, new_weight))\n",
    "            link_matrix_p[i, 2] = int(new_weight * epsilon)\n",
    "\n",
    "        clusters = fcluster(\n",
    "            link_matrix_p, int(epsilon / n_sampled), criterion=\"distance\"\n",
    "        )\n",
    "\n",
    "        n_clients, n_clusters = len(clusters), len(set(clusters))\n",
    "\n",
    "        # Associate each cluster to its number of clients in the cluster\n",
    "        pop_clusters = np.zeros((n_clusters, 2), dtype=np.int64)\n",
    "        for i in range(n_clusters):\n",
    "            pop_clusters[i, 0] = i + 1\n",
    "            for client in np.where(clusters == i + 1)[0]:\n",
    "                pop_clusters[i, 1] += int(weights[client] * epsilon * n_sampled)\n",
    "\n",
    "        pop_clusters = pop_clusters[pop_clusters[:, 1].argsort()]\n",
    "\n",
    "        distri_clusters = np.zeros((n_sampled, n_clients), dtype=np.int64)\n",
    "\n",
    "        # n_sampled biggest clusters that will remain unchanged\n",
    "        kept_clusters = pop_clusters[n_clusters - n_sampled :, 0]\n",
    "\n",
    "        for idx, cluster in enumerate(kept_clusters):\n",
    "            for client in np.where(clusters == cluster)[0]:\n",
    "                distri_clusters[idx, client] = int(\n",
    "                    weights[client] * n_sampled * epsilon\n",
    "                )\n",
    "\n",
    "        k = 0\n",
    "        for j in pop_clusters[: n_clusters - n_sampled, 0]:\n",
    "            clients_in_j = np.where(clusters == j)[0]\n",
    "            np.random.shuffle(clients_in_j)\n",
    "\n",
    "            for client in clients_in_j:\n",
    "                weight_client = int(weights[client] * epsilon * n_sampled)\n",
    "\n",
    "                while weight_client > 0:\n",
    "                    sum_proba_in_k = np.sum(distri_clusters[k])\n",
    "                    u_i = min(epsilon - sum_proba_in_k, weight_client)\n",
    "                    distri_clusters[k, client] = u_i\n",
    "                    weight_client += -u_i\n",
    "                    sum_proba_in_k = np.sum(distri_clusters[k])\n",
    "                    if sum_proba_in_k == 1 * epsilon:\n",
    "                        k += 1\n",
    "\n",
    "        distri_clusters = distri_clusters.astype(float)\n",
    "        print(distri_clusters.shape)\n",
    "        for l in range(n_sampled):\n",
    "            distri_clusters[l] /= np.sum(distri_clusters[l])\n",
    "\n",
    "        return distri_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继承自FedNH中的一些工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoassign(lcls):\n",
    "    \"\"\"\n",
    "    Map all inputs to class attributes.\n",
    "    Reference: https://stackoverflow.com/questions/3652851/what-is-the-best-way-to-do-automatic-attribute-assignment-in-python-and-is-it-a\n",
    "    \"\"\"\n",
    "    for key in lcls.keys():\n",
    "        if key != \"self\":\n",
    "            # flattern kwargs\n",
    "            if key == \"kwargs\":\n",
    "                if key in lcls[\"self\"].__dict__:\n",
    "                    for k in lcls[\"self\"].__dict__[key]:\n",
    "                        lcls[\"self\"].__dict__[k] = lcls[\"self\"].__dict__[key][k]\n",
    "            else:\n",
    "                lcls[\"self\"].__dict__[key] = lcls[key]\n",
    "\n",
    "\n",
    "def calculate_model_size(model_state_dict):\n",
    "    \"\"\"Show model size in MB\"\"\"\n",
    "\n",
    "    mdict = model_state_dict\n",
    "    mem = sum(\n",
    "        [mdict[key].nelement() * mdict[key].element_size() for key in mdict.keys()]\n",
    "    )\n",
    "    return mem * 1e-6\n",
    "\n",
    "\n",
    "def calculate_flops(model, inputs_size, device):\n",
    "    \"\"\"inputs_size: bacth size 1 input\"\"\"\n",
    "    stat = summary(model, inputs_size, verbose=0, device=device)\n",
    "    return stat.total_mult_adds\n",
    "\n",
    "\n",
    "def save_to_pkl(obj, path):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_from_pkl(path):\n",
    "    with open(path, \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def mkdirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        # multi-threading\n",
    "        os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "\n",
    "def access_last_added_element(ordered_dict):\n",
    "    \"\"\"\n",
    "    next(reversed(ordered_dict)) returns the last added key\n",
    "    \"\"\"\n",
    "    try:\n",
    "        key = next(reversed(ordered_dict))\n",
    "        return ordered_dict[key]\n",
    "    except StopIteration:\n",
    "        # print(\"The OrderedDict is empty.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "class Initializer:\n",
    "    \"\"\"\n",
    "    ref:\n",
    "    1. https://github.com/3ammor/Weights-Initializer-pytorch/blob/master/weight_initializer.py\n",
    "    2. https://github.com/kevinzakka/pytorch-goodies\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize(model, initialization, **kwargs):\n",
    "        def weights_init(m):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                initialization(m.weight.data, **kwargs)\n",
    "                try:\n",
    "                    initialization(m.bias.data)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                initialization(m.weight.data, **kwargs)\n",
    "                try:\n",
    "                    initialization(m.bias.data)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        model.apply(weights_init)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Split Datasets\n",
    "\n",
    "References:\n",
    "1. https://github.com/Xtra-Computing/NIID-Bench\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "        self.targets = dataset.targets[self.idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        # return torch.tensor(image), torch.tensor(label)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def remove_by_class(trainset, list_of_classes_to_remove):\n",
    "    for cls in list_of_classes_to_remove:\n",
    "        selected = trainset.targets != cls\n",
    "        trainset.idxs = list(compress(trainset.idxs, selected))\n",
    "        trainset.targets = trainset.dataset.targets[trainset.idxs]\n",
    "    return trainset\n",
    "\n",
    "\n",
    "def split_trainset_by_class(client_trainset):\n",
    "    \"\"\"\n",
    "    Input: client_trainset, which is an object of DatasetSplit class\n",
    "    Return: a dictionary of trainset, where the key is the label while the value is an object of the DatasetSplit class\n",
    "    \"\"\"\n",
    "    all_classes = torch.unique(client_trainset.targets).tolist()\n",
    "    class_dataset_dict = {}\n",
    "    for c in all_classes:\n",
    "        selected = client_trainset.targets == c\n",
    "        idx_c = list(compress(client_trainset.idxs, selected))\n",
    "        class_dataset_dict[c] = DatasetSplit(client_trainset.dataset, idx_c)\n",
    "    return class_dataset_dict\n",
    "\n",
    "\n",
    "def sampler(dataset, num_clients, partition, seed=None, minsize=10, **kwargs):\n",
    "    \"\"\"\n",
    "    dataset: torch.utils.data.Dataset object\n",
    "    partition:\n",
    "        iid-equal-size:\n",
    "            uniformly randomly sample from the whole datasets and each party approximately has the same number of samples.\n",
    "\n",
    "        iid-diff-size:\n",
    "            uniformly randomly sample from the whole datasets but each party approximately has different number of samples;\n",
    "            should also set the beta parameter.\n",
    "\n",
    "        noniid-label-quantity:\n",
    "            Each client will only contain `num_classes_per_client` classes of samples; for any two clients that have the same class, the samples will not overlap;\n",
    "            should also set the num_class, num_classes_per_client, and ylabels parameters.\n",
    "            samples in each classes are uniformly diveded. But this could still lead to class imbalance in each client.\n",
    "            See https://arxiv.org/pdf/2102.02079.pdf a) Quantity-based label imbalance\n",
    "\n",
    "        noniid-label-distribution:\n",
    "            The number of classes per client own follow a dirichlet distribution with concentration parameter beta.\n",
    "            should also set the num_class, beta, and ylabels parameters.\n",
    "\n",
    "        shards:\n",
    "            Suppose there are (n clients, c classes, N datapoints) and each clients own s shards of data. Then the\n",
    "            number of data per shard is  size_s = N / (n * s). And each class is split into c/size_s shards.\n",
    "            Shards are randomly assigned to clients.\n",
    "\n",
    "    kwargs:\n",
    "        beta: concentration parameter for the **symmetric** Dirichlet distribution; float, larger than 0\n",
    "        ylabels: 1d tensor of size as the len(dataset)\n",
    "        num_class: int; larger than 0\n",
    "        num_classes_per_client: int;  larger than 0 smaller than total number of classes in ylabels\n",
    "\n",
    "    Return: a dict; {cid: torch.utils.data.Dataset object}\n",
    "\n",
    "    --- Notes ---\n",
    "    Effect of the beta parameter:\n",
    "        When beta = 1, the symmetric Dirichlet distribution is equivalent to a uniform distribution over the open standard (K − 1)-simplex, (the distribution over distributions is uniform)\n",
    "        When beta > 1, it prefers variates that are dense, evenly distributed distributions, i.e. all the values within a single sample are similar to each other.\n",
    "        When beta < 1, it prefers sparse distributions, i.e. most of the values within a single sample will be close to 0, and the vast majority of the mass will be concentrated in a few of the values.\n",
    "\n",
    "    --- References ---\n",
    "    1. https://en.wikipedia.org/wiki/Dirichlet_distribution#The_concentration_parameter\n",
    "    \"\"\"\n",
    "    # process arguments\n",
    "    if partition in [\"iid-diff-size\", \"noniid-label-distribution\"]:\n",
    "        if \"beta\" not in kwargs:\n",
    "            beta = 0.5\n",
    "            warnings.warn(f\"partition:{partition} | beta is not provided. Set to 0.5.\")\n",
    "        else:\n",
    "            beta = kwargs[\"beta\"]\n",
    "            temp = beta.split(\"b\")\n",
    "            if len(temp) == 1:\n",
    "                beta = float(temp[0])\n",
    "                is_balanced = False\n",
    "            elif len(temp) == 2:\n",
    "                beta = float(temp[0])\n",
    "                is_balanced = True\n",
    "            assert beta > 0, \"beta needs to be non-negative\"\n",
    "    if partition == \"shards\":\n",
    "        if \"num_shards_per_client\" not in kwargs:\n",
    "            raise ValueError(\n",
    "                f\"The num_shards_per_client parameter needs to be set for the partition {partition}.\"\n",
    "            )\n",
    "        else:\n",
    "            num_classes = kwargs[\"num_classes\"]\n",
    "\n",
    "    if partition in [\"noniid-label-quantity\", \"noniid-label-distribution\"]:\n",
    "        if \"num_classes\" not in kwargs:\n",
    "            raise ValueError(\n",
    "                f\"The num_classes parameter needs to be set for the partition {partition}.\"\n",
    "            )\n",
    "        else:\n",
    "            num_classes = kwargs[\"num_classes\"]\n",
    "        try:\n",
    "            num_unique_class = len(torch.unique(dataset.targets))\n",
    "        except TypeError:\n",
    "            print(\"dataset.targets is not of tensor type! Proper actions are required.\")\n",
    "            exit()\n",
    "        assert (\n",
    "            num_classes == num_unique_class\n",
    "        ), f\"num_classes is set to {num_classes}, but number of unique class detected in ylables are {num_unique_class}.\"\n",
    "        if \"ylabels\" not in kwargs:\n",
    "            raise ValueError(\n",
    "                f\"The ylabels parameter needs to be set for the partition {partition}.\"\n",
    "            )\n",
    "        else:\n",
    "            ylabels = kwargs[\"ylabels\"]\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    num_samples = len(dataset)\n",
    "    idxs = np.random.permutation(num_samples)\n",
    "    cur_minsize = 0\n",
    "    attemp = 0\n",
    "    max_attemp = 3\n",
    "    stats_dict = {}\n",
    "    if partition == \"iid-equal-size\":\n",
    "        batch_idxs = np.array_split(idxs, num_clients)\n",
    "        if len(batch_idxs[-1]) < minsize:\n",
    "            warnings.warn(\n",
    "                f\"partition:{partition} | Some clients have less than {minsize} samples. Check it before continue.\"\n",
    "            )\n",
    "        cid_idxlst_dict = {cid: batch_idxs[cid].tolist() for cid in range(num_clients)}\n",
    "    elif partition == \"iid-diff-size\":\n",
    "        \"\"\"\n",
    "        The number of samples per client follow a dirichlet distribution with concentration parameter beta.\n",
    "        But the number of samples per classes in each client are approxumately the same\n",
    "        \"\"\"\n",
    "        while cur_minsize < minsize:\n",
    "            attemp += 1\n",
    "            if attemp == max_attemp:\n",
    "                raise RuntimeError(\n",
    "                    f\"partition:{partition} | Exceeds max allowed attempts. Consider change the random seed.\"\n",
    "                )\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_clients))\n",
    "            proportions = proportions / proportions.sum()\n",
    "            cur_minsize = np.min(proportions * len(idxs))\n",
    "\n",
    "        proportions_to_num = (np.cumsum(proportions) * len(idxs)).astype(int)[:-1]\n",
    "        batch_idxs = np.split(idxs, proportions_to_num)\n",
    "        cid_idxlst_dict = {i: batch_idxs[i].tolist() for i in range(num_clients)}\n",
    "        stats_dict[\"proportions\"] = proportions\n",
    "    elif partition == \"noniid-label-quantity\":\n",
    "        \"\"\"\n",
    "        Each client will only contain `num_classes_per_client` classes of samples.\n",
    "        For any two clients that have the same class, the samples will not overlap.\n",
    "        \"\"\"\n",
    "        # use user supplied partition\n",
    "        if (\n",
    "            \"assigned_clients_per_class\" in kwargs\n",
    "            and \"assigned_classes_per_client\" in kwargs\n",
    "        ):\n",
    "            assigned_clients_per_class = kwargs[\"assigned_clients_per_class\"]\n",
    "            assigned_classes_per_client = kwargs[\"assigned_classes_per_client\"]\n",
    "            assert (\n",
    "                type(assigned_clients_per_class) == list\n",
    "            ), \"assigned_clients_per_class has to a list\"\n",
    "            assert (\n",
    "                type(assigned_classes_per_client) == list\n",
    "            ), \"assigned_classes_per_client has to a list\"\n",
    "            assert (\n",
    "                type(assigned_classes_per_client[0]) == set\n",
    "            ), \"the elements of assigned_classes_per_client has to a set\"\n",
    "            cid_idxlst_dict = {cid: [] for cid in range(num_clients)}\n",
    "            num_classes_per_client = [len(s) for s in assigned_classes_per_client]\n",
    "        else:\n",
    "            if \"num_classes_per_client\" not in kwargs:\n",
    "                raise ValueError(\n",
    "                    f\"The num_classes_per_client parameter needs to be set for the partition {partition}.\"\n",
    "                )\n",
    "            else:\n",
    "                num_classes_per_client = kwargs[\"num_classes_per_client\"]\n",
    "            assert (\n",
    "                num_classes_per_client <= num_classes\n",
    "            ), \"`num_classes_per_client` should be no bigger than `num_classes`\"\n",
    "            cid_idxlst_dict = {cid: [] for cid in range(num_clients)}\n",
    "            assigned_clients_per_class = [0 for i in range(num_classes)]\n",
    "            assigned_classes_per_client = []\n",
    "            for cid in range(num_clients):\n",
    "                # assign class `class_idx` to client `cid`\n",
    "                class_idx = cid % num_classes\n",
    "                current = set()\n",
    "                current.add(class_idx)\n",
    "                assigned_clients_per_class[class_idx] += 1\n",
    "                assigned_class_count = 1\n",
    "                while assigned_class_count < num_classes_per_client:\n",
    "                    ind = np.random.randint(0, num_classes)\n",
    "                    if ind not in current:\n",
    "                        assigned_class_count += 1\n",
    "                        current.add(ind)\n",
    "                        assigned_clients_per_class[ind] += 1\n",
    "                assigned_classes_per_client.append(current)\n",
    "\n",
    "        missing_classes = []\n",
    "        for k in range(num_classes):\n",
    "            if assigned_clients_per_class[k] == 0:\n",
    "                missing_classes.append(str(k))\n",
    "        if len(missing_classes) > 0:\n",
    "            warnings.warn(\n",
    "                \"Classes \"\n",
    "                + \",\".join(missing_classes)\n",
    "                + \"are not used. Consider increase either num_clients or num_classes_per_client.\"\n",
    "            )\n",
    "        for k in range(num_classes):\n",
    "            idx_k = np.where(ylabels == k)[0]\n",
    "            np.random.shuffle(idx_k)\n",
    "            try:\n",
    "                split = np.array_split(idx_k, assigned_clients_per_class[k])\n",
    "            except ValueError:\n",
    "                pass\n",
    "            ids = 0\n",
    "            for cid in range(num_clients):\n",
    "                if k in assigned_classes_per_client[cid]:\n",
    "                    cid_idxlst_dict[cid] += split[ids].tolist()\n",
    "                    ids += 1\n",
    "        stats_dict[\"num_classes\"] = num_classes\n",
    "        stats_dict[\"num_classes_per_client\"] = num_classes_per_client\n",
    "        stats_dict[\"assigned_classes_per_client\"] = assigned_classes_per_client\n",
    "        stats_dict[\"assigned_clients_per_class\"] = assigned_clients_per_class\n",
    "    elif partition == \"noniid-label-distribution\":\n",
    "        \"\"\"\n",
    "        The number of classes per client own follow a dirichlet distribution with concentration parameter beta.\n",
    "        feddf: https://github.com/epfml/federated-learning-public-code/blob/7e002ef5ff0d683dba3db48e2d088165499eb0b9/codes/FedDF-code/pcode/datasets/partition_data.py#L197\n",
    "        \"\"\"\n",
    "        if is_balanced:\n",
    "            np.random.seed(2022)\n",
    "            server_config = kwargs[\"server_config\"]\n",
    "            save_dir = f\"../experiments/datapartition/{server_config['dataset']}_{beta}b_{server_config['num_clients']}.pkl\"\n",
    "            print(\"Doing balanced dir sampling\")\n",
    "            if os.path.exists(save_dir):\n",
    "                print(\"Partition is found!\")\n",
    "                cid_idxlst_dict = load_from_pkl(save_dir)\n",
    "            else:\n",
    "                n_data_per_clnt = int(num_samples / num_clients)\n",
    "                clnt_data_list = (np.ones(num_clients) * n_data_per_clnt).astype(int)\n",
    "                cls_priors = np.random.dirichlet(\n",
    "                    alpha=[beta] * num_classes, size=num_clients\n",
    "                )\n",
    "                prior_cumsum = np.cumsum(cls_priors, axis=1)\n",
    "                idx_list = [np.where(ylabels == i)[0] for i in range(num_classes)]\n",
    "                cls_amount = [len(idx_list[i]) for i in range(num_classes)]\n",
    "                cid_idxlst_dict = {cid: [] for cid in range(num_clients)}\n",
    "                while np.sum(clnt_data_list) != 0:\n",
    "                    curr_clnt = np.random.randint(num_clients)\n",
    "                    # If current node is full resample a client\n",
    "                    print(\"Remaining Data: %d\" % np.sum(clnt_data_list))\n",
    "                    if clnt_data_list[curr_clnt] <= 0:\n",
    "                        continue\n",
    "                    clnt_data_list[curr_clnt] -= 1\n",
    "                    curr_prior = prior_cumsum[curr_clnt]\n",
    "                    while True:\n",
    "                        cls_label = np.argmax(np.random.uniform() <= curr_prior)\n",
    "                        # Redraw class label if trn_y is out of that class\n",
    "                        if cls_amount[cls_label] <= 0:\n",
    "                            continue\n",
    "                        cls_amount[cls_label] -= 1\n",
    "                        cid_idxlst_dict[curr_clnt].append(\n",
    "                            idx_list[cls_label][cls_amount[cls_label]]\n",
    "                        )\n",
    "                        break\n",
    "                mkdirs(\"../experiments/datapartition/\")\n",
    "                save_to_pkl(cid_idxlst_dict, save_dir)\n",
    "                print(\"cid_idxlst_dict is saved to\", save_dir)\n",
    "        else:\n",
    "            resample = False\n",
    "            while cur_minsize < minsize or resample:\n",
    "                attemp += 1\n",
    "                if attemp > max_attemp:\n",
    "                    count = 0\n",
    "                    for cid in range(num_clients):\n",
    "                        if allocated_classes[cid] <= 1:\n",
    "                            count += 1\n",
    "                    print(f\" Warning: {count} clients have less than 2 classes\")\n",
    "                    break\n",
    "                batch_idxs = [[] for _ in range(num_clients)]\n",
    "                allocated_classes = [0] * num_clients\n",
    "                for k in range(num_classes):\n",
    "                    idx_k = np.where(ylabels == k)[0]\n",
    "                    np.random.shuffle(idx_k)\n",
    "                    # determine the fraction of samples in class k for each client;\n",
    "                    proportions = np.random.dirichlet(np.repeat(beta, num_clients))\n",
    "                    # if number of samples in client j is already larger than the threshold num_samples / num_clients\n",
    "                    # then the client won't contain any new class including the current class k\n",
    "                    proportions = np.array(\n",
    "                        [\n",
    "                            p * (len(allocated_idxs) < num_samples / num_clients)\n",
    "                            for p, allocated_idxs in zip(proportions, batch_idxs)\n",
    "                        ]\n",
    "                    )\n",
    "                    proportions = proportions / proportions.sum()\n",
    "                    stats_dict[f\"proportions_{k}\"] = proportions\n",
    "                    proportions_to_num = (np.cumsum(proportions) * len(idx_k)).astype(\n",
    "                        int\n",
    "                    )[:-1]\n",
    "                    # reference: https://numpy.org/doc/stable/reference/generated/numpy.split.html\n",
    "                    # batch_idxs = [allocated_idxs + idx.tolist() for allocated_idxs,\n",
    "                    #               idx in zip(batch_idxs, np.split(idx_k, proportions_to_num))]\n",
    "                    chunks = np.split(idx_k, proportions_to_num)\n",
    "                    # hack to fix class deficiency in some clients\n",
    "                    if k >= 2:\n",
    "                        if min(allocated_classes) <= 1:\n",
    "                            cid_has_only_one_or_less_class = []\n",
    "                            for cid in range(num_clients):\n",
    "                                if allocated_classes[cid] <= 1:\n",
    "                                    cid_has_only_one_or_less_class.append(cid)\n",
    "                            replace_index = -1\n",
    "                            for cid in cid_has_only_one_or_less_class:\n",
    "                                temp_chunk = chunks[cid]\n",
    "                                temp_ratio = proportions[cid]\n",
    "                                chunks[cid] = chunks[replace_index]\n",
    "                                chunks[replace_index] = temp_chunk\n",
    "                                proportions[cid] = proportions[replace_index]\n",
    "                                proportions[replace_index] = temp_ratio\n",
    "                                replace_index -= 1\n",
    "                    cid = 0\n",
    "                    for allocated_idxs, idx in zip(batch_idxs, chunks):\n",
    "                        added_samples = idx.tolist()\n",
    "                        if len(added_samples) > 0:\n",
    "                            allocated_idxs += added_samples\n",
    "                            allocated_classes[cid] += 1\n",
    "                        cid += 1\n",
    "                    cur_minsize = min(\n",
    "                        [len(allocated_idxs) for allocated_idxs in batch_idxs]\n",
    "                    )\n",
    "                if min(allocated_classes) <= 1:\n",
    "                    resample = True\n",
    "                    print(\n",
    "                        \" [Info - Dirichlet Sampling]: At leaset one client only has one class label. Perform Resampling...\"\n",
    "                    )\n",
    "                else:\n",
    "                    resample = False\n",
    "            cid_idxlst_dict = {cid: [] for cid in range(num_clients)}\n",
    "            for cid in range(num_clients):\n",
    "                np.random.shuffle(batch_idxs[cid])\n",
    "                cid_idxlst_dict[cid] = batch_idxs[cid]\n",
    "        stats_dict[\"num_classes\"] = num_classes\n",
    "    elif partition == \"shards\":\n",
    "        num_shards_per_client = kwargs[\"num_shards_per_client\"]\n",
    "        dict_users, stats_dict[\"rand_set_all\"] = sshards(\n",
    "            dataset,\n",
    "            num_clients,\n",
    "            num_shards_per_client,\n",
    "            server_data_ratio=0.0,\n",
    "            rand_set_all=[],\n",
    "        )\n",
    "        cid_idxlst_dict = {i: dict_users[i].tolist() for i in range(num_clients)}\n",
    "    else:\n",
    "        raise ValueError(f\"partition:{partition} is not recognized.\")\n",
    "    # generate a set of sub-datasets\n",
    "    dataset_per_client_dict = {\n",
    "        cid: DatasetSplit(dataset, cid_idxlst_dict[cid]) for cid in range(num_clients)\n",
    "    }\n",
    "    stats_dict[\"num_clients\"] = num_clients\n",
    "    stats_dict[\"partition\"] = partition\n",
    "    stats_dict[\"seed\"] = seed\n",
    "    stats_dict[\"minsize\"] = minsize\n",
    "    return dataset_per_client_dict, stats_dict\n",
    "\n",
    "\n",
    "def sampler_reuse(dataset, stats_dict, **kwargs):\n",
    "    partition = stats_dict[\"partition\"]\n",
    "    num_clients = stats_dict[\"num_clients\"]\n",
    "    if stats_dict[\"seed\"] is not None:\n",
    "        np.random.seed(stats_dict[\"seed\"])\n",
    "    if partition in [\"noniid-label-quantity\", \"noniid-label-distribution\"]:\n",
    "        num_classes = stats_dict[\"num_classes\"]\n",
    "        num_unique_class = len(torch.unique(dataset.targets))\n",
    "        assert (\n",
    "            num_classes == num_unique_class\n",
    "        ), f\"num_class is set to, but number of unique class detected in ylables are {num_unique_class}. The dataset may have a different distribution!\"\n",
    "        if \"ylabels\" not in kwargs:\n",
    "            raise ValueError(\n",
    "                f\"The ylabels parameter needs to be set for the partition {partition}.\"\n",
    "            )\n",
    "        else:\n",
    "            ylabels = kwargs[\"ylabels\"]\n",
    "    num_samples = len(dataset)\n",
    "    idxs = np.random.permutation(num_samples)\n",
    "    cur_minsize = 0\n",
    "    attemp = 0\n",
    "    max_attemp = 100\n",
    "    if partition == \"iid-equal-size\":\n",
    "        batch_idxs = np.array_split(idxs, num_clients)\n",
    "        if len(batch_idxs[-1]) < stats_dict[\"minsize\"]:\n",
    "            warnings.warn(\n",
    "                f\"partition:{partition} | Some clients have less than {stats_dict['minsize']} samples. Check it before continue.\"\n",
    "            )\n",
    "        cid_idxlst_dict = {cid: batch_idxs[cid].tolist() for cid in range(num_clients)}\n",
    "    elif partition == \"iid-diff-size\":\n",
    "        proportions_to_num = (np.cumsum(stats_dict[\"proportions\"]) * len(idxs)).astype(\n",
    "            int\n",
    "        )[:-1]\n",
    "        batch_idxs = np.split(idxs, proportions_to_num)\n",
    "        cid_idxlst_dict = {i: batch_idxs[i].tolist() for i in range(num_clients)}\n",
    "    elif partition == \"noniid-label-quantity\":\n",
    "        num_classes_per_client = stats_dict[\"num_classes_per_client\"]\n",
    "        assert (\n",
    "            num_classes_per_client <= num_classes\n",
    "        ), \"`num_classes_per_client` should be no bigger than `num_classes`\"\n",
    "        cid_idxlst_dict = {cid: [] for cid in range(num_clients)}\n",
    "        for k in range(num_classes):\n",
    "            idx_k = np.where(ylabels == k)[0]\n",
    "            np.random.shuffle(idx_k)\n",
    "            try:\n",
    "                split = np.array_split(\n",
    "                    idx_k, stats_dict[\"assigned_clients_per_class\"][k]\n",
    "                )\n",
    "            except ValueError:\n",
    "                pass\n",
    "            ids = 0\n",
    "            for cid in range(num_clients):\n",
    "                if k in stats_dict[\"assigned_classes_per_client\"][cid]:\n",
    "                    cid_idxlst_dict[cid] += split[ids].tolist()\n",
    "                    ids += 1\n",
    "    elif partition == \"noniid-label-distribution\":\n",
    "        batch_idxs = [[] for _ in range(num_clients)]\n",
    "        for k in range(num_classes):\n",
    "            idx_k = np.where(ylabels == k)[0]\n",
    "            np.random.shuffle(idx_k)\n",
    "            proportions = stats_dict[f\"proportions_{k}\"]\n",
    "            proportions_to_num = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
    "            batch_idxs = [\n",
    "                allocated_idxs + idx.tolist()\n",
    "                for allocated_idxs, idx in zip(\n",
    "                    batch_idxs, np.split(idx_k, proportions_to_num)\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        cid_idxlst_dict = {cid: [] for cid in range(num_clients)}\n",
    "        for cid in range(num_clients):\n",
    "            np.random.shuffle(batch_idxs[cid])\n",
    "            cid_idxlst_dict[cid] = batch_idxs[cid]\n",
    "    else:\n",
    "        raise ValueError(f\"partition:{partition} is not recognized.\")\n",
    "    # generate a set of sub-datasets\n",
    "    dataset_per_client_dict = {\n",
    "        cid: DatasetSplit(dataset, cid_idxlst_dict[cid]) for cid in range(num_clients)\n",
    "    }\n",
    "    return dataset_per_client_dict\n",
    "\n",
    "\n",
    "def sshards(dataset, num_users, shard_per_user, server_data_ratio, rand_set_all=[]):\n",
    "    setup_seed(2022)\n",
    "    dict_users, all_idxs = {i: np.array([], dtype=\"int64\") for i in range(num_users)}, [\n",
    "        i for i in range(len(dataset))\n",
    "    ]\n",
    "\n",
    "    idxs_dict = {}\n",
    "    for i in range(len(dataset)):\n",
    "        label = dataset.targets[i].item()\n",
    "        if label not in idxs_dict.keys():\n",
    "            idxs_dict[label] = []\n",
    "        # collect all data in class ``label``\n",
    "        idxs_dict[label].append(i)\n",
    "\n",
    "    num_classes = len(np.unique(dataset.targets))\n",
    "    shard_per_class = int(shard_per_user * num_users / num_classes)\n",
    "    for label in idxs_dict.keys():\n",
    "        x = idxs_dict[label]\n",
    "        num_leftover = len(x) % shard_per_class\n",
    "        leftover = x[-num_leftover:] if num_leftover > 0 else []\n",
    "        x = np.array(x[:-num_leftover]) if num_leftover > 0 else np.array(x)\n",
    "        x = x.reshape((shard_per_class, -1))\n",
    "        x = list(x)\n",
    "\n",
    "        for i, idx in enumerate(leftover):\n",
    "            x[i] = np.concatenate([x[i], [idx]])\n",
    "        idxs_dict[label] = x\n",
    "\n",
    "    if len(rand_set_all) == 0:\n",
    "        rand_set_all = list(range(num_classes)) * shard_per_class\n",
    "        random.shuffle(rand_set_all)\n",
    "        rand_set_all = np.array(rand_set_all).reshape((num_users, -1))\n",
    "\n",
    "    # divide and assign\n",
    "    for i in range(num_users):\n",
    "        rand_set_label = rand_set_all[i]\n",
    "        rand_set = []\n",
    "        for label in rand_set_label:\n",
    "            idx = np.random.choice(len(idxs_dict[label]), replace=False)\n",
    "            rand_set.append(idxs_dict[label].pop(idx))\n",
    "        dict_users[i] = np.concatenate(rand_set)\n",
    "\n",
    "    test = []\n",
    "    for key, value in dict_users.items():\n",
    "        x = np.unique(dataset.targets[value])\n",
    "        assert (len(x)) <= shard_per_user\n",
    "        test.append(value)\n",
    "    test = np.concatenate(test)\n",
    "    assert len(test) == len(dataset)\n",
    "    assert len(set(list(test))) == len(dataset)\n",
    "\n",
    "    if server_data_ratio > 0.0:\n",
    "        dict_users[\"server\"] = set(\n",
    "            np.random.choice(\n",
    "                all_idxs, int(len(dataset) * server_data_ratio), replace=False\n",
    "            )\n",
    "        )\n",
    "    # print(dict_users)\n",
    "    # exit()\n",
    "    return dict_users, rand_set_all\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "visualization tools\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def visualize_sampling(dataset_per_client_dict, num_classes, figsize=(10, 8), **kwargs):\n",
    "    num_clients = len(dataset_per_client_dict)\n",
    "    mat = np.zeros((num_clients, num_classes))\n",
    "    targets = dataset_per_client_dict[0].dataset.targets\n",
    "    for key in dataset_per_client_dict.keys():\n",
    "        subset = dataset_per_client_dict[key]\n",
    "        for k in range(num_classes):\n",
    "            num_samples = torch.sum(torch.eq(targets[subset.idxs], k)).item()\n",
    "            mat[key, k] = num_samples\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    im, _ = heatmap(\n",
    "        mat,\n",
    "        np.arange(num_clients),\n",
    "        np.arange(num_classes),\n",
    "        ax=ax,\n",
    "        cmap=\"YlGn\",\n",
    "        cbarlabel=\"#Samples\",\n",
    "    )\n",
    "    _ = annotate_heatmap(im, valfmt=\"{x:.0f}\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if \"fig_path_name\" in kwargs:\n",
    "        fig_path_name = kwargs[\"fig_path_name\"]\n",
    "        dirpath = \"/\".join(fig_path_name.split(\"/\")[:-1])\n",
    "        mkdirs(dirpath)\n",
    "        plt.savefig(fig_path_name)\n",
    "    else:\n",
    "        plt.show()\n",
    "    return mat\n",
    "\n",
    "\n",
    "def heatmap(data, x_labels, y_labels, ax=None, cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    im = ax.imshow(data.T, **kwargs)\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "    ax.set_xticks(np.arange(data.shape[0]))\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_xlabel(\"Client ID\")\n",
    "    ax.set_yticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    ax.set_ylabel(\"Class label\")\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[0] + 1) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[1] + 1) - 0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle=\"-\", linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(\n",
    "    im,\n",
    "    data=None,\n",
    "    valfmt=\"{x:.2f}\",\n",
    "    textcolors=(\"black\", \"white\"),\n",
    "    threshold=None,\n",
    "    **textkw,\n",
    "):\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max()) / 2.0\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "class MulGaussian(Dataset):\n",
    "    def __init__(self, mean_lst, n_lst):\n",
    "        k = len(mean_lst)\n",
    "        self.data = None\n",
    "        self.targets = None\n",
    "        for i in range(k):\n",
    "            m = MultivariateNormal(\n",
    "                torch.tensor(mean_lst[i]), torch.eye(len(mean_lst[i]))\n",
    "            )\n",
    "            samples = m.sample(sample_shape=(n_lst[i],))\n",
    "            labels = torch.ones((n_lst[i],), dtype=torch.int32) * i\n",
    "            if i == 0:\n",
    "                self.data = samples\n",
    "                self.targets = labels\n",
    "            else:\n",
    "                self.data = torch.cat((self.data, samples))\n",
    "                self.targets = torch.cat((self.targets, labels))\n",
    "        self.targets = self.targets.type(torch.LongTensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "class Spiral(Dataset):\n",
    "    def __init__(self, n_lst, sigma=0.5):\n",
    "        k = len(n_lst)\n",
    "        self.data = None\n",
    "        self.targets = None\n",
    "        for i in range(k):\n",
    "            r = torch.linspace(1, 10, n_lst[i])  # radius\n",
    "            t = (\n",
    "                torch.linspace(\n",
    "                    i / k * 2 * torch.pi, (i + 1) / k * 2 * torch.pi, n_lst[i]\n",
    "                )\n",
    "                + torch.rand(n_lst[i]) * sigma\n",
    "            )\n",
    "            x = r * torch.sin(t)\n",
    "            y = r * torch.cos(t)\n",
    "            samples = torch.stack((x, y), 1)\n",
    "            labels = torch.ones((n_lst[i],), dtype=torch.int32) * i\n",
    "            if i == 0:\n",
    "                self.data = samples\n",
    "                self.targets = labels\n",
    "            else:\n",
    "                self.data = torch.cat((self.data, samples))\n",
    "                self.targets = torch.cat((self.targets, labels))\n",
    "        self.targets = self.targets.type(torch.LongTensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TinyImageNet Dataset\n",
    "\"\"\"\n",
    "EXTENSION = \"JPEG\"\n",
    "NUM_IMAGES_PER_CLASS = 500\n",
    "CLASS_LIST_FILE = \"wnids.txt\"\n",
    "VAL_ANNOTATION_FILE = \"val_annotations.txt\"\n",
    "\n",
    "\n",
    "class TinyImageNet(Dataset):\n",
    "    \"\"\"\n",
    "    Ref: https://github.com/leemengtaiwan/tiny-imagenet/blob/master/TinyImageNet.py\n",
    "    Tiny ImageNet data set available from `http://cs231n.stanford.edu/tiny-imagenet-200.zip`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    root: string\n",
    "        Root directory including `train`, `test` and `val` subdirectories.\n",
    "    split: string\n",
    "        Indicating which split to return as a data set.\n",
    "        Valid option: [`train`, `test`, `val`]\n",
    "    transform: torchvision.transforms\n",
    "        A (series) of valid transformation(s).\n",
    "    in_memory: bool\n",
    "        Set to True if there is enough memory (about 5G) and want to minimize disk IO overhead.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        split=\"train\",\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        in_memory=False,\n",
    "    ):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.in_memory = in_memory\n",
    "        self.split_dir = os.path.join(self.root, self.split)\n",
    "        self.image_paths = sorted(\n",
    "            glob.iglob(\n",
    "                os.path.join(self.split_dir, \"**\", \"*.%s\" % EXTENSION), recursive=True\n",
    "            )\n",
    "        )\n",
    "        self.labels = {}  # fname - label number mapping\n",
    "        self.images = []  # used for in-memory processing\n",
    "        # build class label - number mapping\n",
    "        with open(os.path.join(self.root, CLASS_LIST_FILE), \"r\") as fp:\n",
    "            self.label_texts = sorted([text.strip() for text in fp.readlines()])\n",
    "        self.label_text_to_number = {text: i for i, text in enumerate(self.label_texts)}\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            for label_text, i in self.label_text_to_number.items():\n",
    "                for cnt in range(NUM_IMAGES_PER_CLASS):\n",
    "                    self.labels[\"%s_%d.%s\" % (label_text, cnt, EXTENSION)] = i\n",
    "        elif self.split == \"val\":\n",
    "            with open(os.path.join(self.split_dir, VAL_ANNOTATION_FILE), \"r\") as fp:\n",
    "                for line in fp.readlines():\n",
    "                    terms = line.split(\"\\t\")\n",
    "                    file_name, label_text = terms[0], terms[1]\n",
    "                    self.labels[file_name] = self.label_text_to_number[label_text]\n",
    "\n",
    "        # get targets\n",
    "        self.targets = []\n",
    "        for index in range(len(self.image_paths)):\n",
    "            file_path = self.image_paths[index]\n",
    "            label_numeral = self.labels[os.path.basename(file_path)]\n",
    "            self.targets.append(label_numeral)\n",
    "\n",
    "        # read all images into torch tensor in memory to minimize disk IO overhead\n",
    "        if self.in_memory:\n",
    "            self.images = [self.read_image(path) for path in self.image_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.image_paths[index]\n",
    "\n",
    "        if self.in_memory:\n",
    "            img = self.images[index]\n",
    "        else:\n",
    "            img = self.read_image(file_path)\n",
    "\n",
    "        if self.split == \"test\":\n",
    "            return img\n",
    "        else:\n",
    "            return img, self.labels[os.path.basename(file_path)]\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = \"Dataset \" + self.__class__.__name__ + \"\\n\"\n",
    "        fmt_str += \"    Number of datapoints: {}\\n\".format(self.__len__())\n",
    "        tmp = self.split\n",
    "        fmt_str += \"    Split: {}\\n\".format(tmp)\n",
    "        fmt_str += \"    Root Location: {}\\n\".format(self.root)\n",
    "        tmp = \"    Transforms (if any): \"\n",
    "        fmt_str += \"{0}{1}\\n\".format(\n",
    "            tmp, self.transform.__repr__().replace(\"\\n\", \"\\n\" + \" \" * len(tmp))\n",
    "        )\n",
    "        tmp = \"    Target Transforms (if any): \"\n",
    "        fmt_str += \"{0}{1}\".format(\n",
    "            tmp, self.target_transform.__repr__().replace(\"\\n\", \"\\n\" + \" \" * len(tmp))\n",
    "        )\n",
    "        return fmt_str\n",
    "\n",
    "    def read_image(self, path):\n",
    "        img = Image.open(path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        return self.transform(img) if self.transform else img\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "get datasets\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_datasets(datasetname, **kwargs):\n",
    "    invTrans = None\n",
    "    if datasetname == \"FashionMnist\":\n",
    "        transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "        trainset = torchvision.datasets.FashionMNIST(\n",
    "            root=\"./data\", train=True, download=True, transform=transform\n",
    "        )\n",
    "        testset = torchvision.datasets.FashionMNIST(\n",
    "            root=\"./data\", train=False, download=True, transform=transform\n",
    "        )\n",
    "    elif datasetname == \"Cifar10\":\n",
    "        transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        transform_test = transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        invTrans = transforms.Compose(\n",
    "            [\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.0, 0.0, 0.0], std=[1 / 0.2023, 1 / 0.1994, 1 / 0.2010]\n",
    "                ),\n",
    "                transforms.Normalize(\n",
    "                    mean=[-0.4914, -0.4822, -0.4465], std=[1.0, 1.0, 1.0]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        trainset = torchvision.datasets.CIFAR10(\n",
    "            root=\"./data\", train=True, download=True, transform=transform_train\n",
    "        )\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root=\"./data\", train=False, download=True, transform=transform_test\n",
    "        )\n",
    "        trainset.targets = torch.tensor(trainset.targets)\n",
    "        testset.targets = torch.tensor(testset.targets)\n",
    "    elif datasetname == \"Cifar100\":\n",
    "        transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        transform_test = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR100(\n",
    "            root=\"./data\", train=True, download=True, transform=transform_train\n",
    "        )\n",
    "        testset = torchvision.datasets.CIFAR100(\n",
    "            root=\"./data\", train=False, download=True, transform=transform_test\n",
    "        )\n",
    "        trainset.targets = torch.tensor(trainset.targets)\n",
    "        testset.targets = torch.tensor(testset.targets)\n",
    "\n",
    "    elif datasetname == \"TinyImageNet\":\n",
    "        transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(64, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        transform_test = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        trainset = TinyImageNet(\n",
    "            \"./data/tiny-imagenet-200\",\n",
    "            \"train\",\n",
    "            transform=transform_train,\n",
    "            in_memory=False,\n",
    "        )\n",
    "        testset = TinyImageNet(\n",
    "            \"./data/tiny-imagenet-200\", \"val\", transform=transform_test, in_memory=False\n",
    "        )\n",
    "        trainset.targets = torch.tensor(trainset.targets)\n",
    "        testset.targets = torch.tensor(testset.targets)\n",
    "\n",
    "    elif datasetname == \"Cifar10Aug\":\n",
    "        \"\"\"\n",
    "        On Bridging Generic and Personalized Federated Learning for Image Classification impl\n",
    "        \"\"\"\n",
    "        transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        transform_test = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        trainset = torchvision.datasets.CIFAR10(\n",
    "            root=\"~/data\", train=True, download=True, transform=transform_train\n",
    "        )\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root=\"~/data\", train=False, download=True, transform=transform_test\n",
    "        )\n",
    "        trainset.targets = torch.tensor(trainset.targets)\n",
    "        testset.targets = torch.tensor(testset.targets)\n",
    "    elif datasetname == \"GanEnhancedCifar10\":\n",
    "        trainset = GanEnhancedCifar10(\n",
    "            kwargs[\"generator_path\"], kwargs[\"dataset\"], kwargs[\"upsample\"]\n",
    "        )\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root=\"~/data\", train=False, download=True, transform=transform\n",
    "        )\n",
    "        testset.targets = torch.tensor(testset.targets)\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized dataset:{datasetname}\")\n",
    "\n",
    "    return trainset, testset, invTrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "客户端划分等工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_optimizer(model, config, round):\n",
    "    if config[\"client_lr_scheduler\"] == \"stepwise\":\n",
    "        if round < config[\"num_rounds\"] // 2:\n",
    "            lr = config[\"client_lr\"]\n",
    "        else:\n",
    "            lr = config[\"client_lr\"] * 0.1\n",
    "\n",
    "    elif config[\"client_lr_scheduler\"] == \"diminishing\":\n",
    "        lr = config[\"client_lr\"] * (config[\"lr_decay_per_round\"] ** (round - 1))\n",
    "    else:\n",
    "        raise ValueError(\"unknown client_lr_scheduler\")\n",
    "    if config[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr,\n",
    "            momentum=config[\"sgd_momentum\"],\n",
    "            weight_decay=config[\"sgd_weight_decay\"],\n",
    "        )\n",
    "        # print('line 34: weight_decay=1e-3')\n",
    "    elif config[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "    elif config[\"optimizer\"] == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr,\n",
    "            alpha=config[\"rmsprop_alpha\"],\n",
    "            eps=1e-08,\n",
    "            weight_decay=config[\"rmsprop_weight_decay\"],\n",
    "            momentum=config[\"rmsprop_momentum\"],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer{config['optimizer']}\")\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "client initialization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def setup_clients(\n",
    "    Client, trainset, testset, criterion, client_config_lst, client_pyus, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Client is a class constructor.\n",
    "    A deepcopy is invoked such that each client has a unique model.\n",
    "    **kwargs:\n",
    "        weight_init: {'init.normal'}\n",
    "        mean: params for init.normal\n",
    "        std: 0.1\n",
    "    \"\"\"\n",
    "    num_clients = kwargs[\"server_config\"][\"num_clients\"]\n",
    "    partition = kwargs[\"server_config\"][\"partition\"]\n",
    "    num_classes = kwargs[\"server_config\"][\"num_classes\"]\n",
    "    assert (\n",
    "        len(client_config_lst) == num_clients\n",
    "    ), \"Inconsistent num_clients and len(client_config_lst).\"\n",
    "    if \"noniid\" == partition[:6]:\n",
    "        trainset_per_client_dict, stats_dict = sampler(\n",
    "            trainset,\n",
    "            num_clients,\n",
    "            partition,\n",
    "            ylabels=trainset.targets,\n",
    "            num_classes=num_classes,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if testset is None:\n",
    "            testset_per_client_dict = {cid: None for cid in range(num_clients)}\n",
    "        else:\n",
    "            if kwargs[\"same_testset\"]:\n",
    "                testset_per_client_dict = {cid: testset for cid in range(num_clients)}\n",
    "            else:\n",
    "                testset_per_client_dict = sampler_reuse(\n",
    "                    testset,\n",
    "                    stats_dict,\n",
    "                    ylabels=testset.targets,\n",
    "                    num_classes=num_classes,\n",
    "                    **kwargs,\n",
    "                )\n",
    "    else:\n",
    "        trainset_per_client_dict, stats_dict = sampler(\n",
    "            trainset, num_clients, partition, num_classes=num_classes, **kwargs\n",
    "        )\n",
    "        if testset is None:\n",
    "            testset_per_client_dict = {cid: None for cid in range(num_clients)}\n",
    "        else:\n",
    "            if kwargs[\"same_testset\"]:\n",
    "                testset_per_client_dict = {cid: testset for cid in range(num_clients)}\n",
    "            else:\n",
    "                testset_per_client_dict = sampler_reuse(\n",
    "                    testset, stats_dict, num_classes=num_classes, **kwargs\n",
    "                )\n",
    "\n",
    "    n_samples = {cid: len(dataset) for cid, dataset in trainset_per_client_dict.items()}\n",
    "\n",
    "    all_clients_dict = {}\n",
    "    for cid in range(num_clients):\n",
    "        # same initial weight\n",
    "        setup_seed(2022)\n",
    "        all_clients_dict[cid] = Client(\n",
    "            device=client_pyus[cid],\n",
    "            criterion=criterion,\n",
    "            trainset=trainset_per_client_dict[cid],\n",
    "            testset=testset_per_client_dict[cid],\n",
    "            client_config=client_config_lst[cid],\n",
    "            cid=cid,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    return all_clients_dict, n_samples\n",
    "\n",
    "\n",
    "def create_clients_from_existing_ones(\n",
    "    Client, clients_dict, newtrainset, increment, criterion, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Create new clients. All clients will maintain the same data distribution as specified in clients_dict.\n",
    "    \"\"\"\n",
    "    num_clients = len(clients_dict)\n",
    "    all_clients_dict = {}\n",
    "    if \"same_pool\" not in kwargs:\n",
    "        same_pool = False\n",
    "    else:\n",
    "        same_pool = kwargs[\"same_pool\"]\n",
    "\n",
    "    if \"scale\" not in kwargs:\n",
    "        scale = len(newtrainset) // increment - 1\n",
    "    else:\n",
    "        scale = kwargs[\"scale\"]\n",
    "\n",
    "    for cid in range(num_clients):\n",
    "        client = clients_dict[cid]\n",
    "        data_idxs = client.trainset.idxs\n",
    "        add_idxs = []\n",
    "        if same_pool:\n",
    "            for cls in client.count_by_class.keys():\n",
    "                num_sample_cls = client.count_by_class[cls]\n",
    "                target_num_sample_cls = min(\n",
    "                    num_sample_cls * scale, len(newtrainset.get_fake_imgs_idxs(cls))\n",
    "                )\n",
    "                add_idxs += np.random.choice(\n",
    "                    newtrainset.get_fake_imgs_idxs(cls),\n",
    "                    target_num_sample_cls,\n",
    "                    replace=False,\n",
    "                ).tolist()\n",
    "        else:\n",
    "            for i in data_idxs:\n",
    "                for j in range(scale):\n",
    "                    add_idxs.append(i + increment * (j + 1))\n",
    "\n",
    "        full_idxs = data_idxs + add_idxs\n",
    "        client_newtrainset = DatasetSplit(newtrainset, full_idxs)\n",
    "        all_clients_dict[cid] = Client(\n",
    "            criterion,\n",
    "            client_newtrainset,\n",
    "            client.testset,\n",
    "            client.client_config,\n",
    "            client.cid,\n",
    "            client.group,\n",
    "            client.device,\n",
    "            **kwargs,\n",
    "        )\n",
    "    return all_clients_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "resume training\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def resume_training(server_config, checkpoint, model):\n",
    "    server = load_from_pkl(checkpoint)\n",
    "    server.server_config = server_config\n",
    "    for c in server.clients_dict.values():\n",
    "        c.model = deepcopy(model)\n",
    "        c.set_params(server.server_model_state_dict)\n",
    "        c.model.to(c.device)\n",
    "        c.model.init()\n",
    "    print(\"Resume Training\")\n",
    "    print(f\"Rounds performed:{server.rounds}\")\n",
    "    return server\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "state_dict operation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def scale_state_dict(this, scale, inplace=True, exclude=set()):\n",
    "    with torch.no_grad():\n",
    "        if not inplace:\n",
    "            ans = deepcopy(this)\n",
    "        else:\n",
    "            ans = this\n",
    "        for state_key in this.keys():\n",
    "            if state_key not in exclude:\n",
    "                ans[state_key] = this[state_key] * scale\n",
    "        return ans\n",
    "\n",
    "\n",
    "def linear_combination_state_dict(\n",
    "    this, other, this_weight=1.0, other_weight=1.0, exclude=set()\n",
    "):\n",
    "    \"\"\"\n",
    "    this, other: state_dict\n",
    "    this_weight * this + other_weight * other\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        ans = deepcopy(this)\n",
    "        for state_key in this.keys():\n",
    "            if state_key not in exclude:\n",
    "                # print('agg', state_key)\n",
    "                ans[state_key] = (\n",
    "                    this[state_key] * this_weight + other[state_key] * other_weight\n",
    "                )\n",
    "        return ans\n",
    "\n",
    "\n",
    "def average_list_of_state_dict(state_dict_lst, exclude=set()):\n",
    "    assert type(state_dict_lst) == list\n",
    "    num_participants = len(state_dict_lst)\n",
    "    keys = state_dict_lst[0].keys()\n",
    "    with torch.no_grad():\n",
    "        ans = OrderedDict()\n",
    "        for key in keys:\n",
    "            if state_key not in exclude:\n",
    "                for idx, client_state_dict in enumerate(state_dict_lst):\n",
    "                    if idx == 0:\n",
    "                        # must do deepcopy; otherwise subsequent operation overwrittes the first client_state_dict\n",
    "                        ans[key] = deepcopy(client_state_dict[key])\n",
    "                    else:\n",
    "                        ans[key] += client_state_dict[key]\n",
    "                ans[key] = ans[key] / num_participants\n",
    "    return ans\n",
    "\n",
    "\n",
    "def weight_sum_of_dict_of_state_dict(dict_state_dict, weight_dict):\n",
    "    layer_keys = next(iter(dict_state_dict.values())).keys()\n",
    "    with torch.no_grad():\n",
    "        ans = OrderedDict()\n",
    "        for layer in layer_keys:\n",
    "            count = 0\n",
    "            for cid in dict_state_dict.keys():\n",
    "                if count == 0:\n",
    "                    # must do deepcopy; otherwise subsequent operation overwrittes the first client_state_dict\n",
    "                    ans[layer] = (\n",
    "                        deepcopy(dict_state_dict[cid][layer]) * weight_dict[cid]\n",
    "                    )\n",
    "                else:\n",
    "                    ans[layer] += dict_state_dict[cid][layer] * weight_dict[cid]\n",
    "                count += 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义了联邦学习场景下的客户端 FedPCGClient 类和服务器 FedPCGServer类，实现了注入本地原型和全局原型的全局head更新的个性化联邦学习方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@proxy(PYUObject)\n",
    "class FedPCGClient(Client):\n",
    "    def __init__(self, criterion, trainset, testset, client_config, cid, **kwargs):\n",
    "        super().__init__(criterion, trainset, testset, client_config, cid, **kwargs)\n",
    "        self._initialize_model()\n",
    "        self.device = \"cpu\"\n",
    "        self.global_model = deepcopy(self.model)\n",
    "        self.client_config = client_config\n",
    "        self.beta = 1\n",
    "        self.tau = 0.5\n",
    "        self.num_classes = 10\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.KLDiv = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        temp = [\n",
    "            self.count_by_class[cls] if cls in self.count_by_class.keys() else 1e-12\n",
    "            for cls in range(client_config[\"num_classes\"])\n",
    "        ]\n",
    "        self.count_by_class_full = torch.tensor(temp).to(self.device)\n",
    "\n",
    "        self.global_model2 = deepcopy(self.model)\n",
    "\n",
    "    def get_model_named_parameters(self):\n",
    "        return list(self.model.named_parameters())\n",
    "\n",
    "    def _estimate_prototype(self, global_model2):\n",
    "        self.model.eval()\n",
    "        self.model.return_embedding = True\n",
    "        embedding_dim = self.model.prototype.shape[1]\n",
    "        prototype = torch.zeros_like(self.model.prototype)\n",
    "        self.set_gloabl_param(self.global_model2, global_model2)\n",
    "        self.global_model2.eval()\n",
    "        self.global_model2.return_embedding = True\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(self.trainloader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                feature_embedding, _ = self.model.forward(x)\n",
    "                feature_embedding_global, _ = self.global_model2.forward(x)\n",
    "                classes_shown_in_this_batch = torch.unique(y).cpu().numpy()\n",
    "                for cls in classes_shown_in_this_batch:\n",
    "                    mask = y == cls\n",
    "                    feature_embedding_in_cls = torch.sum(\n",
    "                        feature_embedding[mask, :], dim=0\n",
    "                    )\n",
    "                    feature_embedding_global_in_cls = torch.sum(\n",
    "                        feature_embedding_global[mask, :], dim=0\n",
    "                    )\n",
    "                    prototype[cls] += (\n",
    "                        0.7 * feature_embedding_in_cls\n",
    "                        + 0.3 * feature_embedding_global_in_cls\n",
    "                    )\n",
    "        for cls in self.count_by_class.keys():\n",
    "            prototype[cls] /= self.count_by_class[cls]\n",
    "            prototype_cls_norm = torch.norm(prototype[cls]).clamp(min=1e-12)\n",
    "            prototype[cls] = torch.div(prototype[cls], prototype_cls_norm)\n",
    "            prototype[cls] *= self.count_by_class[cls]\n",
    "\n",
    "        self.model.return_embedding = False\n",
    "\n",
    "        to_share = {\n",
    "            \"scaled_prototype\": prototype,\n",
    "            \"count_by_class_full\": self.count_by_class_full,\n",
    "        }\n",
    "        return to_share\n",
    "\n",
    "    def _estimate_prototype_adv(self):\n",
    "        self.model.eval()\n",
    "        self.model.return_embedding = True\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        weights = []\n",
    "        prototype = torch.zeros_like(self.model.prototype)\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(self.trainloader):\n",
    "                # forward pass\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                feature_embedding, logits = self.model.forward(x)\n",
    "                prob_ = F.softmax(logits, dim=1)\n",
    "                prob = torch.gather(prob_, dim=1, index=y.view(-1, 1))\n",
    "                labels.append(y)\n",
    "                weights.append(prob)\n",
    "                embeddings.append(feature_embedding)\n",
    "        self.model.return_embedding = False\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        weights = torch.cat(weights, dim=0).view(-1, 1)\n",
    "        for cls in self.count_by_class.keys():\n",
    "            mask = labels == cls\n",
    "            weights_in_cls = weights[mask, :]\n",
    "            feature_embedding_in_cls = embeddings[mask, :]\n",
    "            prototype[cls] = torch.sum(\n",
    "                feature_embedding_in_cls * weights_in_cls, dim=0\n",
    "            ) / torch.sum(weights_in_cls)\n",
    "            prototype_cls_norm = torch.norm(prototype[cls]).clamp(min=1e-12)\n",
    "            prototype[cls] = torch.div(prototype[cls], prototype_cls_norm)\n",
    "\n",
    "        # calculate predictive power\n",
    "        to_share = {\n",
    "            \"adv_agg_prototype\": prototype,\n",
    "            \"count_by_class_full\": self.count_by_class_full,\n",
    "        }\n",
    "        return to_share\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_orthonormal_basis(m, n):\n",
    "        \"\"\"\n",
    "        Each row of the the matrix is orthonormal\n",
    "        \"\"\"\n",
    "        W = torch.rand(m, n)\n",
    "        # gram schimdt\n",
    "        for i in range(m):\n",
    "            q = W[i, :]\n",
    "            for j in range(i):\n",
    "                q = q - torch.dot(W[j, :], W[i, :]) * W[j, :]\n",
    "            if torch.equal(q, torch.zeros_like(q)):\n",
    "                raise ValueError(\"The row vectors are not linearly independent!\")\n",
    "            q = q / torch.sqrt(torch.dot(q, q))\n",
    "            W[i, :] = q\n",
    "        return W\n",
    "\n",
    "    def setup_seed_local(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def _initialize_model(self):\n",
    "\n",
    "        self.model = Conv2CifarNH(self.client_config).to(self.device)\n",
    "        self.criterion = self.criterion.to(self.device)\n",
    "        try:\n",
    "            self.model.prototype.requires_grad_(False)\n",
    "            if self.client_config[\"FedNH_head_init\"] == \"orthogonal\":\n",
    "                m, n = self.model.prototype.shape\n",
    "                self.model.prototype.data = torch.nn.init.orthogonal_(\n",
    "                    torch.rand(m, n)\n",
    "                ).to(self.device)\n",
    "            elif (\n",
    "                self.client_config[\"FedNH_head_init\"] == \"uniform\"\n",
    "                and self.client_config[\"dim\"] == 2\n",
    "            ):\n",
    "                r = 1.0\n",
    "                num_cls = self.client_config[\"num_classes\"]\n",
    "                W = torch.zeros(num_cls, 2)\n",
    "                for i in range(num_cls):\n",
    "                    theta = i * 2 * torch.pi / num_cls\n",
    "                    W[i, :] = torch.tensor([r * math.cos(theta), r * math.sin(theta)])\n",
    "                self.model.prototype.copy_(W)\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"{self.client_config['FedNH_head_init']} + {self.client_config['num_classes']}d\"\n",
    "                )\n",
    "        except AttributeError:\n",
    "            raise NotImplementedError(\"Only support linear layers now.\")\n",
    "        if self.client_config[\"FedNH_fix_scaling\"] == True:\n",
    "            # 30.0 is a common choice in the paper\n",
    "            self.model.scaling.requires_grad_(False)\n",
    "            self.model.scaling.data = torch.tensor(30.0).to(self.device)\n",
    "            print(\"self.model.scaling.data:\", self.model.scaling.data)\n",
    "\n",
    "    def set_gloabl_param(self, g1, g2):\n",
    "        with torch.no_grad():\n",
    "            for key in g2.keys():\n",
    "                g1.state_dict()[key].copy_(g2[key])\n",
    "\n",
    "    def training(self, round, num_epochs, global_model):\n",
    "        \"\"\"\n",
    "        Note that in order to use the latest server side model the `set_params` method should be called before `training` method.\n",
    "        \"\"\"\n",
    "        print(\"Begin local training!\")\n",
    "        train_start = time.time()\n",
    "        self.setup_seed_local(round)\n",
    "        # train mode\n",
    "        self.model.train()\n",
    "        # tracking stats\n",
    "        self.set_gloabl_param(self.global_model, global_model)\n",
    "        self.global_model = self.global_model.eval().requires_grad_(False)\n",
    "        self.num_rounds_particiapted += 1\n",
    "        loss_seq = []\n",
    "        acc_seq = []\n",
    "        if self.trainloader is None:\n",
    "            raise ValueError(\"No trainloader is provided!\")\n",
    "        optimizer = setup_optimizer(self.model, self.client_config, round)\n",
    "        for i in range(num_epochs):\n",
    "            epoch_loss, correct = 0.0, 0\n",
    "            for _, (x, y) in enumerate(self.trainloader):\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                yhat = self.model.forward(x)\n",
    "                loss = self.criterion(yhat, y)\n",
    "                y_g = self.global_model.forward(x)\n",
    "                loss += self._ntd_loss(yhat, y_g, y) * self.beta\n",
    "                self.model.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    parameters=filter(\n",
    "                        lambda p: p.requires_grad, self.model.parameters()\n",
    "                    ),\n",
    "                    max_norm=10,\n",
    "                )\n",
    "                optimizer.step()\n",
    "                predicted = yhat.data.max(1)[1]\n",
    "                correct += predicted.eq(y.data).sum().item()\n",
    "                epoch_loss += loss.item() * x.shape[0]  # rescale to bacthsize\n",
    "\n",
    "            epoch_loss /= len(self.trainloader.dataset)\n",
    "            epoch_accuracy = correct / len(self.trainloader.dataset)\n",
    "            loss_seq.append(epoch_loss)\n",
    "            acc_seq.append(epoch_accuracy)\n",
    "        self.new_state_dict = self.model.state_dict()\n",
    "        self.train_loss_dict[round] = loss_seq\n",
    "        self.train_acc_dict[round] = acc_seq\n",
    "        print(\"Local training completed!\")\n",
    "        train_time = time.time() - train_start\n",
    "        print(f\"Local training time:{train_time:.3f} seconds\")\n",
    "\n",
    "    def get_train_loss_dict(self, r):\n",
    "        return self.train_loss_dict[r]\n",
    "\n",
    "    def get_train_acc_dict(self, r):\n",
    "        return self.train_acc_dict[r]\n",
    "\n",
    "    def get_test_loss_dict(self, r):\n",
    "        return self.test_loss_dict[r]\n",
    "\n",
    "    def get_test_acc_dict(self, r):\n",
    "        return self.test_acc_dict[r]\n",
    "\n",
    "    def get_num_train_samples(self):\n",
    "        return self.num_train_samples\n",
    "\n",
    "    def get_testloader(self):\n",
    "        return self.testloader\n",
    "\n",
    "    def _ntd_loss(self, logits, dg_logits, targets):\n",
    "        \"\"\"Not-tue Distillation Loss\"\"\"\n",
    "\n",
    "        logits = refine_as_not_true(logits, targets, self.num_classes)\n",
    "        pred_probs = F.log_softmax(logits / self.tau, dim=1)\n",
    "\n",
    "        # Get smoothed global model prediction\n",
    "        with torch.no_grad():\n",
    "            dg_logits = refine_as_not_true(dg_logits, targets, self.num_classes)\n",
    "            dg_probs = torch.softmax(dg_logits / self.tau, dim=1)\n",
    "\n",
    "        loss = (self.tau**2) * self.KLDiv(pred_probs, dg_probs)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def upload(self, global_model2):\n",
    "        if self.client_config[\"FedNH_client_adv_prototype_agg\"]:\n",
    "            return self.new_state_dict, self._estimate_prototype_adv()\n",
    "        else:\n",
    "            return self.new_state_dict, self._estimate_prototype(global_model2)\n",
    "\n",
    "    def testing(self, round, testloader=None):\n",
    "        self.model.eval()\n",
    "        if testloader is None:\n",
    "            testloader = self.testloader\n",
    "        test_count_per_class = Counter(testloader.dataset.targets.numpy())\n",
    "        num_classes = self.client_config[\"num_classes\"]\n",
    "        test_count_per_class = torch.tensor(\n",
    "            [test_count_per_class[cls] * 1.0 for cls in range(num_classes)]\n",
    "        )\n",
    "        test_correct_per_class = torch.tensor([0] * num_classes)\n",
    "\n",
    "        weight_per_class_dict = {\n",
    "            \"uniform\": torch.tensor([1.0] * num_classes),\n",
    "            \"validclass\": torch.tensor([0.0] * num_classes),\n",
    "            \"labeldist\": torch.tensor([0.0] * num_classes),\n",
    "        }\n",
    "        for cls in self.label_dist.keys():\n",
    "            weight_per_class_dict[\"labeldist\"][cls] = self.label_dist[cls]\n",
    "            weight_per_class_dict[\"validclass\"][cls] = 1.0\n",
    "        # start testing\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(testloader):\n",
    "                # forward pass\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                yhat = self.model.forward(x)\n",
    "                # stats\n",
    "                predicted = yhat.data.max(1)[1]\n",
    "                classes_shown_in_this_batch = torch.unique(y).cpu().numpy()\n",
    "                for cls in classes_shown_in_this_batch:\n",
    "                    test_correct_per_class[cls] += (\n",
    "                        ((predicted == y) * (y == cls)).sum().item()\n",
    "                    )\n",
    "        acc_by_critertia_dict = {}\n",
    "        for k in weight_per_class_dict.keys():\n",
    "            acc_by_critertia_dict[k] = (\n",
    "                ((weight_per_class_dict[k] * test_correct_per_class).sum())\n",
    "                / ((weight_per_class_dict[k] * test_count_per_class).sum())\n",
    "            ).item()\n",
    "\n",
    "        self.test_acc_dict[round] = {\n",
    "            \"acc_by_criteria\": acc_by_critertia_dict,\n",
    "            \"correct_per_class\": test_correct_per_class,\n",
    "            \"weight_per_class\": weight_per_class_dict,\n",
    "        }\n",
    "\n",
    "\n",
    "def refine_as_not_true(logits, targets, num_classes):\n",
    "    nt_positions = torch.arange(0, num_classes).to(logits.device)\n",
    "    nt_positions = nt_positions.repeat(logits.size(0), 1)\n",
    "    nt_positions = nt_positions[nt_positions[:, :] != targets.view(-1, 1)]\n",
    "    nt_positions = nt_positions.view(-1, num_classes - 1)\n",
    "\n",
    "    logits = torch.gather(logits, 1, nt_positions)\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "@proxy(PYUObject)\n",
    "class FedPCGServer(Server):\n",
    "    def __init__(self, n_samples, server_config, clients_dict, exclude, **kwargs):\n",
    "        super().__init__(server_config, clients_dict, **kwargs)\n",
    "\n",
    "        self.device = \"cpu\"\n",
    "        self.summary_setup()\n",
    "        self.server_model_state_dict = deepcopy(self.clients_dict[0].get_params())\n",
    "        self.server_side_client.set_params(\n",
    "            self.server_model_state_dict.to(self.server_side_client.device),\n",
    "            exclude_keys=set(),\n",
    "        )\n",
    "        self.exclude_layer_keys = set()\n",
    "        for key in sf.reveal(self.server_model_state_dict):\n",
    "            for ekey in exclude:\n",
    "                if ekey in key:\n",
    "                    self.exclude_layer_keys.add(key)\n",
    "        if len(self.exclude_layer_keys) > 0:\n",
    "            print(\n",
    "                f\"{self.server_config['strategy']}Server: the following keys will not be aggregated:\\n \",\n",
    "                self.exclude_layer_keys,\n",
    "            )\n",
    "        freeze_layers = []\n",
    "        for param in sf.reveal(self.server_side_client.get_model_named_parameters()):\n",
    "            if param[1].requires_grad == False:\n",
    "                freeze_layers.append(param[0])\n",
    "        if len(freeze_layers) > 0:\n",
    "            print(\"Server: the following layers will not be updated:\", freeze_layers)\n",
    "        self.selection = ClusteredSampling2(server_config[\"num_clients\"], \"cpu\", \"L1\")\n",
    "        self.nsamples = n_samples\n",
    "        self.selection.setup(self.nsamples)\n",
    "\n",
    "    def aggregate(self, client_uploads, round):\n",
    "\n",
    "        server_lr = self.server_config[\"learning_rate\"] * (\n",
    "            self.server_config[\"lr_decay_per_round\"] ** (round - 1)\n",
    "        )\n",
    "        num_participants = len(client_uploads)\n",
    "        update_direction_state_dict = None\n",
    "        cumsum_per_class = torch.zeros(server_config[\"num_classes\"])\n",
    "        agg_weights_vec_dict = {}\n",
    "        with torch.no_grad():\n",
    "            for idx, (client_state_dict, prototype_dict) in enumerate(\n",
    "                sf.reveal(client_uploads)\n",
    "            ):\n",
    "                if self.server_config[\"FedNH_server_adv_prototype_agg\"] == False:\n",
    "                    cumsum_per_class += prototype_dict[\"count_by_class_full\"]\n",
    "                else:\n",
    "                    mu = prototype_dict[\"adv_agg_prototype\"]\n",
    "                    W = self.server_model_state_dict[\"prototype\"]\n",
    "                    agg_weights_vec_dict[idx] = torch.exp(\n",
    "                        torch.sum(W * mu, dim=1, keepdim=True)\n",
    "                    )\n",
    "                client_update = linear_combination_state_dict(\n",
    "                    sf.reveal(client_state_dict),\n",
    "                    sf.reveal(self.server_model_state_dict),\n",
    "                    1.0,\n",
    "                    -1.0,\n",
    "                    exclude=self.exclude_layer_keys,\n",
    "                )\n",
    "                if idx == 0:\n",
    "                    update_direction_state_dict = client_update\n",
    "                else:\n",
    "                    update_direction_state_dict = linear_combination_state_dict(\n",
    "                        sf.reveal(update_direction_state_dict),\n",
    "                        sf.reveal(client_update),\n",
    "                        1.0,\n",
    "                        1.0,\n",
    "                        exclude=self.exclude_layer_keys,\n",
    "                    )\n",
    "            # new feature extractor\n",
    "            self.server_model_state_dict = linear_combination_state_dict(\n",
    "                sf.reveal(self.server_model_state_dict),\n",
    "                sf.reveal(update_direction_state_dict),\n",
    "                1.0,\n",
    "                server_lr / num_participants,\n",
    "                exclude=self.exclude_layer_keys,\n",
    "            )\n",
    "\n",
    "            avg_prototype = torch.zeros_like(self.server_model_state_dict[\"prototype\"])\n",
    "            if self.server_config[\"FedNH_server_adv_prototype_agg\"] == False:\n",
    "                for _, prototype_dict in sf.reveal(client_uploads):\n",
    "                    avg_prototype += prototype_dict[\n",
    "                        \"scaled_prototype\"\n",
    "                    ] / cumsum_per_class.view(-1, 1)\n",
    "\n",
    "            else:\n",
    "                m = self.server_model_state_dict[\"prototype\"].shape[0]\n",
    "                sum_of_weights = torch.zeros((m, 1)).to(avg_prototype.device)\n",
    "                for idx, (_, prototype_dict) in enumerate(client_uploads):\n",
    "                    sum_of_weights += agg_weights_vec_dict[idx]\n",
    "                    avg_prototype += (\n",
    "                        agg_weights_vec_dict[idx] * prototype_dict[\"adv_agg_prototype\"]\n",
    "                    )\n",
    "                avg_prototype /= sum_of_weights\n",
    "\n",
    "            avg_prototype = F.normalize(avg_prototype, dim=1)\n",
    "            weight = self.server_config[\"FedNH_smoothing\"]\n",
    "            temp = (\n",
    "                weight * self.server_model_state_dict[\"prototype\"]\n",
    "                + (1 - weight) * avg_prototype\n",
    "            )\n",
    "            self.server_model_state_dict[\"prototype\"].copy_(F.normalize(temp, dim=1))\n",
    "\n",
    "    def testing(self, round, active_only=True, **kwargs):\n",
    "        \"\"\"\n",
    "        active_only: only compute statiscs with to the active clients only\n",
    "        \"\"\"\n",
    "        self.server_side_client.set_params(\n",
    "            self.server_model_state_dict, self.exclude_layer_keys\n",
    "        )\n",
    "        self.server_side_client.testing(\n",
    "            round, testloader=None\n",
    "        )  # use global testdataset\n",
    "        print(\n",
    "            \" server global model correct\",\n",
    "            torch.sum(\n",
    "                sf.reveal(self.server_side_client.get_test_acc_dict(round))[\n",
    "                    \"correct_per_class\"\n",
    "                ]\n",
    "            ).item(),\n",
    "        )\n",
    "        client_indices = self.clients_dict.keys()\n",
    "        if active_only:\n",
    "            client_indices = self.active_clients_indicies\n",
    "        for cid in client_indices:\n",
    "            client = self.clients_dict[cid]\n",
    "            if self.server_config[\"split_testset\"] == True:\n",
    "                client.testing(round, None)\n",
    "            else:\n",
    "                client.testing(\n",
    "                    round, self.server_side_client.get_testloader().to(client.device)\n",
    "                )\n",
    "\n",
    "    def setup_seed_global(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def collect_stats(self, stage, round, active_only, **kwargs):\n",
    "        \"\"\"\n",
    "        No actual training and testing is performed. Just collect stats.\n",
    "        stage: str;\n",
    "            {\"train\", \"test\"}\n",
    "        active_only: bool;\n",
    "            True: compute stats on active clients only\n",
    "            False: compute stats on all clients\n",
    "        \"\"\"\n",
    "        client_indices = self.clients_dict.keys()\n",
    "        if active_only:\n",
    "            client_indices = self.active_clients_indicies\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        total_samples = 0\n",
    "        if stage == \"train\":\n",
    "            for cid in client_indices:\n",
    "                client = self.clients_dict[cid]\n",
    "                loss, acc, num_samples = (\n",
    "                    sf.reveal(client.get_train_loss_dict(round))[-1],\n",
    "                    sf.reveal(client.get_train_acc_dict(round))[-1],\n",
    "                    client.get_num_train_samples(),\n",
    "                )\n",
    "                total_loss += loss * sf.reveal(num_samples)\n",
    "                total_acc += acc * sf.reveal(num_samples)\n",
    "                total_samples += sf.reveal(num_samples)\n",
    "            average_loss, average_acc = (\n",
    "                total_loss / total_samples,\n",
    "                total_acc / total_samples,\n",
    "            )\n",
    "            self.average_train_loss_dict[round] = average_loss\n",
    "            self.average_train_acc_dict[round] = average_acc\n",
    "        else:\n",
    "            self.gfl_test_acc_dict[round] = self.server_side_client.get_test_acc_dict(\n",
    "                round\n",
    "            )\n",
    "            acc_criteria = sf.reveal(self.server_side_client.get_test_acc_dict(round))[\n",
    "                \"acc_by_criteria\"\n",
    "            ].keys()\n",
    "            self.average_pfl_test_acc_dict[round] = {key: 0.0 for key in acc_criteria}\n",
    "            for cid in client_indices:\n",
    "                client = self.clients_dict[cid]\n",
    "                acc_by_criteria_dict = sf.reveal(client.get_test_acc_dict(round))[\n",
    "                    \"acc_by_criteria\"\n",
    "                ]\n",
    "                for key in acc_criteria:\n",
    "                    self.average_pfl_test_acc_dict[round][key] += acc_by_criteria_dict[\n",
    "                        key\n",
    "                    ]\n",
    "\n",
    "            num_participants = len(client_indices)\n",
    "            for key in acc_criteria:\n",
    "                self.average_pfl_test_acc_dict[round][key] /= num_participants\n",
    "\n",
    "    def client_selection(self):\n",
    "        client_indices = [*range(self.server_config[\"num_clients\"])]\n",
    "        n = int(\n",
    "            self.server_config[\"num_clients\"] * self.server_config[\"participate_ratio\"]\n",
    "        )\n",
    "        selected_client_indices = self.selection.select(n, client_indices)\n",
    "        return selected_client_indices\n",
    "\n",
    "    def run(self, device, **kwargs):\n",
    "        if self.server_config[\"use_tqdm\"]:\n",
    "            round_iterator = tqdm(\n",
    "                range(self.rounds + 1, self.server_config[\"num_rounds\"] + 1),\n",
    "                desc=\"Round Progress\",\n",
    "            )\n",
    "        else:\n",
    "            round_iterator = range(\n",
    "                self.rounds + 1, self.server_config[\"num_rounds\"] + 1\n",
    "            )\n",
    "        for r in round_iterator:\n",
    "            self.setup_seed_global(r)\n",
    "            if r == 1:\n",
    "                selected_indices = self.select_clients(\n",
    "                    self.server_config[\"participate_ratio\"]\n",
    "                )\n",
    "            else:\n",
    "                selected_indices = self.client_selection()\n",
    "            if self.server_config[\"drop_ratio\"] > 0:\n",
    "                self.active_clients_indicies = np.random.choice(\n",
    "                    selected_indices,\n",
    "                    int(len(selected_indices) * (1 - self.server_config[\"drop_ratio\"])),\n",
    "                    replace=False,\n",
    "                )\n",
    "            else:\n",
    "                self.active_clients_indicies = selected_indices\n",
    "            tqdm.write(f\"Round:{r} - Active clients:{self.active_clients_indicies}:\")\n",
    "            for cid in self.active_clients_indicies:\n",
    "                client = self.clients_dict[cid]\n",
    "                client.set_params(\n",
    "                    sf.reveal(self.server_model_state_dict), self.exclude_layer_keys\n",
    "                )\n",
    "\n",
    "            client_uploads = []\n",
    "            for cid in self.active_clients_indicies:\n",
    "                client = self.clients_dict[cid]\n",
    "                client.training(\n",
    "                    r,\n",
    "                    client_config[\"num_epochs\"],\n",
    "                    sf.reveal(self.server_model_state_dict),\n",
    "                )\n",
    "                client_upload = client.upload(sf.reveal(self.server_model_state_dict))\n",
    "                client_uploads.append(client_upload.to(device))\n",
    "\n",
    "            local_models = [\n",
    "                self.clients_dict[cid].get_model().to(device)\n",
    "                for cid in range(self.server_config[\"num_clients\"])\n",
    "            ]\n",
    "            self.selection.init(self.server_model_state_dict, local_models)\n",
    "\n",
    "            self.collect_stats(stage=\"train\", round=r, active_only=True)\n",
    "\n",
    "            self.aggregate(client_uploads, round=r)\n",
    "\n",
    "            if (r - 1) % self.server_config[\"test_every\"] == 0:\n",
    "                test_start = time.time()\n",
    "                self.testing(round=r, active_only=True)\n",
    "                test_time = time.time() - test_start\n",
    "                print(f\" Testing time:{test_time:.3f} seconds\")\n",
    "                self.collect_stats(stage=\"test\", round=r, active_only=True)\n",
    "                print(\n",
    "                    \" avg_test_acc:\",\n",
    "                    sf.reveal(self.gfl_test_acc_dict[r])[\"acc_by_criteria\"],\n",
    "                )\n",
    "                print(\" pfl_avg_test_acc:\", self.average_pfl_test_acc_dict[r])\n",
    "                if len(self.gfl_test_acc_dict) >= 2:\n",
    "                    current_key = r\n",
    "                    if (\n",
    "                        sf.reveal(self.gfl_test_acc_dict[current_key])[\n",
    "                            \"acc_by_criteria\"\n",
    "                        ][\"uniform\"]\n",
    "                        > best_test_acc\n",
    "                    ):\n",
    "                        best_test_acc = sf.reveal(self.gfl_test_acc_dict[current_key])[\n",
    "                            \"acc_by_criteria\"\n",
    "                        ][\"uniform\"]\n",
    "                        self.server_model_state_dict_best_so_far = deepcopy(\n",
    "                            self.server_model_state_dict\n",
    "                        )\n",
    "                        tqdm.write(\n",
    "                            f\" Best test accuracy:{float(best_test_acc):5.3f}. Best server model is updatded and saved at {kwargs['filename']}!\"\n",
    "                        )\n",
    "                        if \"filename\" in kwargs:\n",
    "                            torch.save(\n",
    "                                sf.reveal(self.server_model_state_dict_best_so_far),\n",
    "                                kwargs[\"filename\"],\n",
    "                            )\n",
    "                else:\n",
    "                    best_test_acc = sf.reveal(self.gfl_test_acc_dict[r])[\n",
    "                        \"acc_by_criteria\"\n",
    "                    ][\"uniform\"]\n",
    "            if kwargs[\"use_wandb\"]:\n",
    "                stats = {\n",
    "                    \"avg_train_loss\": self.average_train_loss_dict[r],\n",
    "                    \"avg_train_acc\": self.average_train_acc_dict[r],\n",
    "                    \"gfl_test_acc_uniform\": self.gfl_test_acc_dict[r][\n",
    "                        \"acc_by_criteria\"\n",
    "                    ][\"uniform\"],\n",
    "                }\n",
    "\n",
    "                for criteria in self.average_pfl_test_acc_dict[r].keys():\n",
    "                    stats[f\"pfl_test_acc_{criteria}\"] = self.average_pfl_test_acc_dict[\n",
    "                        r\n",
    "                    ][criteria]\n",
    "\n",
    "                wandb.log(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐语环境下FedPCG的实现流程，这里只初始化5个客户端为例，可以修改config.ini中的客户端数量和采样率，提高模型的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secretflow as sf\n",
    "\n",
    "sf.shutdown()\n",
    "num_clients = server_config[\"num_clients\"]\n",
    "sf.init(\n",
    "    [f\"client_{i}\" for i in range(server_config[\"num_clients\"])]\n",
    "    + [\"server\"]\n",
    "    + [\"fake_client\"],\n",
    "    address=\"local\",\n",
    ")\n",
    "\n",
    "if args.strategy == \"FedAvg\":\n",
    "    ClientCstr, ServerCstr = FedAvgClient, FedAvgServer\n",
    "\n",
    "elif args.strategy == \"FedNH\":\n",
    "    ClientCstr, ServerCstr = FedNHClient, FedNHServer\n",
    "    server_config[\"FedNH_smoothing\"] = args.FedNH_smoothing\n",
    "    server_config[\"FedNH_server_adv_prototype_agg\"] = (\n",
    "        args.FedNH_server_adv_prototype_agg\n",
    "    )\n",
    "    client_config[\"FedNH_client_adv_prototype_agg\"] = (\n",
    "        args.FedNH_client_adv_prototype_agg\n",
    "    )\n",
    "\n",
    "elif args.strategy == \"FedPCG\":\n",
    "    ClientCstr, ServerCstr = FedPCGClient, FedPCGServer\n",
    "    server_config[\"FedNH_smoothing\"] = args.FedNH_smoothing\n",
    "    server_config[\"FedNH_server_adv_prototype_agg\"] = (\n",
    "        args.FedNH_server_adv_prototype_agg\n",
    "    )\n",
    "    client_config[\"FedNH_client_adv_prototype_agg\"] = (\n",
    "        args.FedNH_client_adv_prototype_agg\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Invalid strategy!\")\n",
    "\n",
    "directory = f\"./{args.purpose}_{server_config['strategy']}/\"\n",
    "mkdirs(directory)\n",
    "path = directory\n",
    "print(\"results are saved in: \", path)\n",
    "client_config_lst = [client_config for i in range(args.num_clients)]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainset, testset, _ = get_datasets(server_config[\"dataset\"])\n",
    "client_pyus = [sf.PYU(f\"client_{i}\") for i in range(num_clients)]\n",
    "server_pyu = sf.PYU(\"server\")\n",
    "# setup clients\n",
    "if server_config[\"split_testset\"] == False:\n",
    "    clients_dict, n_samples = setup_clients(\n",
    "        ClientCstr,\n",
    "        trainset,\n",
    "        None,\n",
    "        criterion,\n",
    "        client_config_lst,\n",
    "        client_pyus,\n",
    "        server_config=server_config,\n",
    "        beta=server_config[\"beta\"],\n",
    "        num_classes_per_client=server_config[\"num_classes_per_client\"],\n",
    "        num_shards_per_client=server_config[\"num_shards_per_client\"],\n",
    "    )\n",
    "else:\n",
    "    print(\"split test set!\")\n",
    "    clients_dict, n_samples = setup_clients(\n",
    "        ClientCstr,\n",
    "        trainset,\n",
    "        testset,\n",
    "        criterion,\n",
    "        client_config_lst,\n",
    "        client_pyus,\n",
    "        server_config=server_config,\n",
    "        beta=server_config[\"beta\"],\n",
    "        num_classes_per_client=server_config[\"num_classes_per_client\"],\n",
    "        num_shards_per_client=server_config[\"num_shards_per_client\"],\n",
    "        same_testset=False,\n",
    "    )\n",
    "\n",
    "if args.strategy != \"Local\":\n",
    "    print(\"ClientCstr\", ClientCstr)\n",
    "    server = FedPCGServer(\n",
    "        n_samples=n_samples,\n",
    "        device=server_pyu,\n",
    "        server_config=server_config,\n",
    "        clients_dict=clients_dict,\n",
    "        exclude=server_config[\"exclude\"],\n",
    "        server_side_criterion=criterion,\n",
    "        global_testset=testset,\n",
    "        global_trainset=trainset,\n",
    "        client_cstr=ClientCstr,\n",
    "        server_side_client_config=client_config,\n",
    "        server_side_client_device=args.device,\n",
    "    )\n",
    "    print(\"Strategy Related Hyper-parameters:\")\n",
    "    print(\"server side\")\n",
    "    for k in server_config.keys():\n",
    "        if args.strategy in k:\n",
    "            print(\" \", k, \":\", server_config[k])\n",
    "    print(\"client side\")\n",
    "    for k in client_config.keys():\n",
    "        if args.strategy in k:\n",
    "            print(\" \", k, \":\", client_config[k])\n",
    "    server.run(\n",
    "        device=server.device,\n",
    "        filename=path + \"_best_global_model.pkl\",\n",
    "        use_wandb=use_wandb,\n",
    "        global_seed=args.global_seed,\n",
    "    )\n",
    "else:\n",
    "    expected_num_rounds = int(\n",
    "        server_config[\"num_rounds\"] * server_config[\"participate_ratio\"]\n",
    "    )\n",
    "    init_weight = clients_dict[0].get_params()\n",
    "    global_testloader = DataLoader(testset, batch_size=128, shuffle=False)\n",
    "    for cid in clients_dict.keys():\n",
    "        print(f\"Progress:{cid}/{len(clients_dict)}\")\n",
    "        client = clients_dict[cid]\n",
    "        client.set_params(init_weight, exclude_keys=set())\n",
    "        for r in range(1, expected_num_rounds + 1):\n",
    "            setup_seed(r + args.global_seed)\n",
    "            client.training(r, client.client_config[\"num_epochs\"])\n",
    "            client.testing(r, global_testloader)\n",
    "            print(\n",
    "                f\"Round: {r}/{expected_num_rounds}\",\n",
    "                client.test_acc_dict[r][\"acc_by_criteria\"],\n",
    "            )\n",
    "        client.model = None\n",
    "        client.trainloader = None\n",
    "        client.trainset = None\n",
    "        client.new_state_dict = None\n",
    "    save_to_pkl(clients_dict, path + \"_final_clients_dict.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedProto",
   "language": "python",
   "name": "fedproto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
