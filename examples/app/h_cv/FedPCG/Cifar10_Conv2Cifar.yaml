server_config:
  # learning strategy for serve: str; currently supported: {FeaAvg, FedDiscrete}
  strategy: None
  # number of communication roundsï¼š int
  num_rounds: 200
  # number of total participating clients: int
  num_clients: 100
  # control numper of participating clients per round: float [0, 1.0];
  participate_ratio: 0.1
  # randomly remove `drop_ratio` fraction of total participating clients at the aggregation time of each round: float [0.0, 1.0)
  drop_ratio: 0.0
  test_every: 1
  # split testset
  split_testset: False
  use_tqdm: False
  # dataset used for training: str; currently supported: {Mnist, FashionMnist, Cifar10}
  dataset: Cifar10
  # dataset partition strategy: str; {'iid-equal-size', 'iid-diff-size', 'noniid-label-quantity', 'noniid-label-distribution', 'shards'}
  partition: None
  # float >0
  beta: None
  # int <= 10
  num_classes_per_client: None
  # int
  num_shards_per_client: None
  num_classes: 10
  learning_rate: 1.0
  lr_decay_per_round: 1.0
  # layers to be skipped in aggregation
  exclude: !!python/tuple []

  #################################################
  ##### algorithm specific settings go here ########
  #################################################
  # FedNH:
  FedNH_smoothing: 0.9
  FedNH_server_adv_prototype_agg: False

  # CReFF
  CReFF_num_of_fl_feature: 100
  CReFF_match_epoch: 100
  CReFF_lr_net: 0.01
  CReFF_lr_feature: 0.1
  CReFF_crt_epoch: 300
  CReFF_dis_metric: 'ours'


client_config:
  # network used for each client
  model: Conv2Cifar
  # network specific setting goes here

  # dataset setting
  # input size: size of a single input; 3 channel, 32*32
  input_size: !!python/tuple [3, 32, 32]
  num_classes: 10
  # number of local epochs: int
  num_epochs: 5
  # number of samples per batch: int
  batch_size: 64
  # {Adam, SGD}
  optimizer: SGD
  # initial learning rate for each round
  learning_rate: 0.1
  lr_scheduler: stepwise # {diminishing}
  lr_decay_per_round: 0.99
  num_rounds: 200
  # other settings
  use_tqdm: False

  #################################################
  ##### algorithm specific settings go here ########
  #################################################

  # FedROD
  FedROD_hyper_clf: True
  FedROD_phead_separate: False  # this is very expensive

  # FedNH
  FedNH_return_embedding: False
  FedNH_head_init: orthogonal
  FedNH_client_adv_prototype_agg: False
  FedNH_fix_scaling: False

  # FedProto
  FedProto_lambda: 0.1

  # FedRep
  FedRep_head_epochs: 10

  # FedBABU
  FedBABU_finetune_epoch: 5

  
  # Ditto
  Ditto_lambda: 0.75  # penalty parameter for Ditto follows the setting of FedRep

  # CReFF
  CReFF_batch_real: 64

  # # pFedMe:
  # pFedMe_lambda: 15
  # pFedMe_local_steps: 5
  # # # pmodel_lr and lgmodel_lr is the learning rate for the global model and personalized model on the client side
  # # # in the actual test I set both of them to be the same and derive them from the setup_optimizer function to make fair comparison
  # # pFedMe_pmodel_lr: 0.05
  # # pFedMe_lgmodel_lr: 0.05
  
