{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该Notebook在隐语环境中实现了论文FedProto: Federated Prototype Learning across Heterogeneous Clients的数据划分和联邦学习方法。reference:Tan, Y., Long, G., Liu, L., Zhou, T., Lu, Q., Jiang, J., & Zhang, C. (2022, June). Fedproto: Federated prototype learning across heterogeneous clients. In Proceedings of the AAAI conference on artificial intelligence (Vol. 36, No. 8, pp. 8432-8440).https://github.com/yuetan031/FedProto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from tqdm import tqdm\n",
    "import copy, sys\n",
    "import time\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "from secretflow import PYUObject, proxy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些超参数设置，可以通过同级目录下的config.ini设置对应不同数据集的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--rounds', type=int, default=100,\n",
    "                        help=\"number of rounds of training\")\n",
    "    parser.add_argument('--num_users', type=int, default=20,\n",
    "                        help=\"number of users: K\")\n",
    "    parser.add_argument('--frac', type=float, default=0.04,\n",
    "                        help='the fraction of clients: C')\n",
    "    parser.add_argument('--train_ep', type=int, default=1,\n",
    "                        help=\"the number of local episodes: E\")\n",
    "    parser.add_argument('--local_bs', type=int, default=4,\n",
    "                        help=\"local batch size: B\")\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5,\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "\n",
    "    parser.add_argument('--model', type=str, default='cnn', help='model name')\n",
    "    parser.add_argument('--alg', type=str, default='fedproto', help=\"algorithms\")\n",
    "    parser.add_argument('--mode', type=str, default='task_heter', help=\"mode\")\n",
    "    parser.add_argument('--num_channels', type=int, default=1, help=\"number \\\n",
    "                        of channels of imgs\")\n",
    "    parser.add_argument('--norm', type=str, default='batch_norm',\n",
    "                        help=\"batch_norm, layer_norm, or None\")\n",
    "    parser.add_argument('--num_filters', type=int, default=32,\n",
    "                        help=\"number of filters for conv nets -- 32 for \\\n",
    "                        mini-imagenet, 64 for omiglot.\")\n",
    "    parser.add_argument('--max_pool', type=str, default='True',\n",
    "                        help=\"Whether use max pooling rather than \\\n",
    "                        strided convolutions\")\n",
    "\n",
    "    parser.add_argument('--data_dir', type=str, default='../data/', help=\"directory of dataset\")\n",
    "    parser.add_argument('--dataset', type=str, default='mnist', help=\"name \\\n",
    "                        of dataset\")\n",
    "    parser.add_argument('--num_classes', type=int, default=10, help=\"number \\\n",
    "                        of classes\")\n",
    "    parser.add_argument('--device', default='cpu', help=\"To use cuda\")\n",
    "    parser.add_argument('--gpu', default=0, help=\"To use cuda, set \\\n",
    "                        to a specific GPU ID. Default set to use CPU.\")\n",
    "    parser.add_argument('--optimizer', type=str, default='sgd', help=\"type \\\n",
    "                        of optimizer\")\n",
    "    parser.add_argument('--iid', type=int, default=0,\n",
    "                        help='Default set to IID. Set to 0 for non-IID.')\n",
    "    parser.add_argument('--unequal', type=int, default=0,\n",
    "                        help='whether to use unequal data splits for  \\\n",
    "                        non-i.i.d setting (use 0 for equal splits)')\n",
    "    parser.add_argument('--stopping_rounds', type=int, default=10,\n",
    "                        help='rounds of early stopping')\n",
    "    parser.add_argument('--verbose', type=int, default=1, help='verbose')\n",
    "    parser.add_argument('--seed', type=int, default=1234, help='random seed')\n",
    "    parser.add_argument('--test_ep', type=int, default=10, help=\"num of test episodes for evaluation\")\n",
    "    parser.add_argument('--ways', type=int, default=3, help=\"num of classes\")\n",
    "    parser.add_argument('--shots', type=int, default=100, help=\"num of shots\")\n",
    "    parser.add_argument('--train_shots_max', type=int, default=110, help=\"num of shots\")\n",
    "    parser.add_argument('--test_shots', type=int, default=15, help=\"num of shots\")\n",
    "    parser.add_argument('--stdev', type=int, default=2, help=\"stdev of ways\")\n",
    "    parser.add_argument('--ld', type=float, default=1, help=\"weight of proto loss\")\n",
    "    parser.add_argument('--ft_round', type=int, default=10, help=\"round of fine tuning\")\n",
    "    arg_list = None\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "    arg_list = []\n",
    "    for k, v in config['train'].items():\n",
    "        arg_list.append(\"--\"+k)\n",
    "        arg_list.append(v)\n",
    "\n",
    "    args = parser.parse_args(arg_list)\n",
    "    return args\n",
    "args = args_parser()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_details(args):\n",
    "    print('\\nExperimental details:')\n",
    "    print(f'    Model     : {args.model}')\n",
    "    print(f'    Optimizer : {args.optimizer}')\n",
    "    print(f'    Learning  : {args.lr}')\n",
    "    print(f'    Global Rounds   : {args.rounds}\\n')\n",
    "\n",
    "    print('    Federated parameters:')\n",
    "    if args.iid:\n",
    "        print('    IID')\n",
    "    else:\n",
    "        print('    Non-IID')\n",
    "    print(f'    Fraction of users  : {args.frac}')\n",
    "    print(f'    Local Batch size   : {args.local_bs}')\n",
    "    print(f'    Local Epochs       : {args.train_ep}\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "客户端的本地模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(int(320/20*20), 50)\n",
    "        self.fc2 = nn.Linear(50, args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x1, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1), x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "客户端类，主要包含了初始化、数据集划分和本地更新等主要函数，本地更新的loss由交叉熵loss和本地原型、全局原型之间的mse_loss之和组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@proxy(PYUObject)\n",
    "class Client(object):\n",
    "    def __init__(self, args, dataset, idxs):\n",
    "        self.args = args\n",
    "        self.trainloader = self.train_val_test(dataset, list(idxs))\n",
    "        self.device = 'cpu'\n",
    "        self.criterion = nn.NLLLoss().to(self.device)\n",
    "        self.model_param = None\n",
    "        self.loss = None\n",
    "        self.acc = None\n",
    "        self.protos = None\n",
    "        self.local_loss = None\n",
    "        self.proto_loss = None\n",
    "        self.global_protos = []\n",
    "        self.model = CNNMnist(args=args).to(self.device)\n",
    "        \n",
    "    def train_val_test(self, dataset, idxs):\n",
    "        \"\"\"\n",
    "        Returns train, validation and test dataloaders for a given dataset\n",
    "        and user indexes.\n",
    "        \"\"\"\n",
    "        idxs_train = idxs[:int(1 * len(idxs))]\n",
    "        trainloader = DataLoader(DatasetSplit(dataset, idxs_train),\n",
    "                                 batch_size=self.args.local_bs, shuffle=True, drop_last=True)\n",
    "\n",
    "        return trainloader\n",
    "\n",
    "\n",
    "    def update_weights_het(self, args, idx, global_round=round):\n",
    "        self.model.train()\n",
    "        epoch_loss = {'total':[],'1':[], '2':[], '3':[]}\n",
    "\n",
    "        if self.args.optimizer == 'sgd':\n",
    "            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.args.lr,\n",
    "                                        momentum=0.5)\n",
    "        elif self.args.optimizer == 'adam':\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.lr,\n",
    "                                         weight_decay=1e-4)\n",
    "\n",
    "        for iter in range(self.args.train_ep):\n",
    "            batch_loss = {'total':[],'1':[], '2':[], '3':[]}\n",
    "            agg_protos_label = {}\n",
    "            for batch_idx, (images, label_g) in enumerate(self.trainloader):\n",
    "                images, labels = images.to(self.device), label_g.to(self.device)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "                log_probs, protos = self.model(images)\n",
    "                loss1 = self.criterion(log_probs, labels)\n",
    "\n",
    "                loss_mse = nn.MSELoss()\n",
    "                if len(self.global_protos) == 0:\n",
    "                    loss2 = 0*loss1\n",
    "                else:\n",
    "                    proto_new = copy.deepcopy(protos.data)\n",
    "                    i = 0\n",
    "                    for label in labels:\n",
    "                        if label.item() in self.global_protos.keys():\n",
    "                            proto_new[i, :] = self.global_protos[label.item()][0].data\n",
    "                        i += 1\n",
    "                    loss2 = loss_mse(proto_new, protos)\n",
    "\n",
    "                loss = loss1 + loss2 * args.ld\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                for i in range(len(labels)):\n",
    "                    if label_g[i].item() in agg_protos_label:\n",
    "                        agg_protos_label[label_g[i].item()].append(protos[i,:])\n",
    "                    else:\n",
    "                        agg_protos_label[label_g[i].item()] = [protos[i,:]]\n",
    "\n",
    "                log_probs = log_probs[:, 0:args.num_classes]\n",
    "                _, y_hat = log_probs.max(1)\n",
    "                acc_val = torch.eq(y_hat, labels.squeeze()).float().mean()\n",
    "\n",
    "                if self.args.verbose and (batch_idx % 10 == 0):\n",
    "                    print('| Global Round : {} | User: {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.3f} | Acc: {:.3f}'.format(\n",
    "                        global_round, idx, iter, batch_idx * len(images),\n",
    "                        len(self.trainloader.dataset),\n",
    "                        100. * batch_idx / len(self.trainloader),\n",
    "                        loss.item(),\n",
    "                        acc_val.item()))\n",
    "                batch_loss['total'].append(loss.item())\n",
    "                batch_loss['1'].append(loss1.item())\n",
    "                batch_loss['2'].append(loss2.item())\n",
    "            epoch_loss['total'].append(sum(batch_loss['total'])/len(batch_loss['total']))\n",
    "            epoch_loss['1'].append(sum(batch_loss['1']) / len(batch_loss['1']))\n",
    "            epoch_loss['2'].append(sum(batch_loss['2']) / len(batch_loss['2']))\n",
    "\n",
    "        epoch_loss['total'] = sum(epoch_loss['total']) / len(epoch_loss['total'])\n",
    "        epoch_loss['1'] = sum(epoch_loss['1']) / len(epoch_loss['1'])\n",
    "        epoch_loss['2'] = sum(epoch_loss['2']) / len(epoch_loss['2'])\n",
    "        \n",
    "        self.set_param_model(self.model.state_dict())\n",
    "        self.set_loss(epoch_loss)\n",
    "        self.set_acc(acc_val.item())\n",
    "        self.set_protos(agg_protos_label)\n",
    "    \n",
    "    def set_param_model(self,param):\n",
    "        self.model_param = param\n",
    "    def get_param_model(self):\n",
    "        return self.model_param\n",
    "\n",
    "    def set_loss(self,loss):\n",
    "        self.loss = loss\n",
    "        self.local_loss = loss['total']\n",
    "        self.proto_loss = loss['2']                      \n",
    "    def get_loss(self):  \n",
    "        return self.loss\n",
    "    def get_local_loss(self):\n",
    "        return self.local_loss\n",
    "    def get_proto_loss(self):\n",
    "        return self.proto_loss\n",
    "    def set_acc(self,acc):\n",
    "        self.acc = acc\n",
    "    def get_acc(self):\n",
    "        return self.acc\n",
    "    def set_protos(self,protos):\n",
    "        self.protos = protos\n",
    "    def get_protos(self):\n",
    "        return self.protos\n",
    "    \n",
    "    def set_global_protos(self,global_protos):\n",
    "        self.global_protos=global_protos\n",
    "       \n",
    "    def get_weights(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(weights,strict=True)\n",
    "    def agg_func(self,protos):\n",
    "        \"\"\"\n",
    "        Returns the average of the weights.\n",
    "        \"\"\"\n",
    "\n",
    "        for [label, proto_list] in protos.items():\n",
    "            if len(proto_list) > 1:\n",
    "                proto = 0 * proto_list[0].data\n",
    "                for i in proto_list:\n",
    "                    proto += i.data\n",
    "                protos[label] = proto / len(proto_list)\n",
    "            else:\n",
    "                protos[label] = proto_list[0]\n",
    "\n",
    "        return protos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "服务器类，主要包含了初始化、全局原型聚合和测试全局模型、保存原型等函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@proxy(PYUObject)\n",
    "class Server(object):\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "        self.device = 'cpu'\n",
    "        self.acc_list_l = []\n",
    "        self.acc_list_g = []\n",
    "        self.loss_list = []\n",
    "        self.model = CNNMnist(args=args).to(self.device)\n",
    "    def proto_aggregation(self,local_protos_list):\n",
    "        agg_protos_label = dict()\n",
    "        for idx in local_protos_list:\n",
    "            local_protos = local_protos_list[idx]\n",
    "            for label in local_protos.keys():\n",
    "                if label in agg_protos_label:\n",
    "                    agg_protos_label[label].append(local_protos[label])\n",
    "                else:\n",
    "                    agg_protos_label[label] = [local_protos[label]]\n",
    "\n",
    "        for [label, proto_list] in agg_protos_label.items():\n",
    "            if len(proto_list) > 1:\n",
    "                proto = 0 * proto_list[0].data\n",
    "                for i in proto_list:\n",
    "                    proto += i.data\n",
    "                agg_protos_label[label] = [proto / len(proto_list)]\n",
    "            else:\n",
    "                agg_protos_label[label] = [proto_list[0].data]\n",
    "\n",
    "        return agg_protos_label\n",
    "    \n",
    "    def test_inference_new_het_lt(self,args, local_weights_list_global,test_dataset, classes_list, user_groups_gt, global_protos=[]):\n",
    "        \"\"\" Returns the test accuracy and loss.\n",
    "        \"\"\"\n",
    "        loss, total, correct = 0.0, 0.0, 0.0\n",
    "        loss_mse = nn.MSELoss()\n",
    "        criterion = nn.NLLLoss().to(self.device)\n",
    "\n",
    "        for idx in range(args.num_users):\n",
    "            self.model.load_state_dict(local_weights_list_global[idx],strict=True)\n",
    "            testloader = DataLoader(DatasetSplit(test_dataset, user_groups_gt[idx]), batch_size=64, shuffle=True)\n",
    "            self.model.eval()\n",
    "            for batch_idx, (images, labels) in enumerate(testloader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                self.model.zero_grad()\n",
    "                outputs, protos = self.model(images)\n",
    "                batch_loss = criterion(outputs, labels)\n",
    "                loss += batch_loss.item()\n",
    "                _, pred_labels = torch.max(outputs, 1)\n",
    "                pred_labels = pred_labels.view(-1)\n",
    "                correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "                total += len(labels)\n",
    "\n",
    "            acc = correct / total\n",
    "            print('| User: {} | Global Test Acc w/o protos: {:.3f}'.format(idx, acc))\n",
    "            self.acc_list_l.append(acc)\n",
    "\n",
    "            if global_protos!=[]:\n",
    "                for batch_idx, (images, labels) in enumerate(testloader):\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    self.model.zero_grad()\n",
    "                    outputs, protos = self.model(images)\n",
    "                    a_large_num = 100\n",
    "                    dist = a_large_num * torch.ones(size=(images.shape[0], args.num_classes)).to(self.device)  # initialize a distance matrix\n",
    "                    for i in range(images.shape[0]):\n",
    "                        for j in range(args.num_classes):\n",
    "                            if j in global_protos.keys() and j in classes_list[idx]:\n",
    "                                d = loss_mse(protos[i, :], global_protos[j][0])\n",
    "                                dist[i, j] = d\n",
    "\n",
    "                    _, pred_labels = torch.min(dist, 1)\n",
    "                    pred_labels = pred_labels.view(-1)\n",
    "                    correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "                    total += len(labels)\n",
    "                    proto_new = copy.deepcopy(protos.data)\n",
    "                    i = 0\n",
    "                    for label in labels:\n",
    "                        if label.item() in global_protos.keys():\n",
    "                            proto_new[i, :] = global_protos[label.item()][0].data\n",
    "                        i += 1\n",
    "                    loss2 = loss_mse(proto_new, protos)\n",
    "                    if self.device == 'cuda':\n",
    "                        loss2 = loss2.cpu().detach().numpy()\n",
    "                    else:\n",
    "                        loss2 = loss2.detach().numpy()\n",
    "\n",
    "                acc = correct / total\n",
    "                print('| User: {} | Global Test Acc with protos: {:.5f}'.format(idx, acc))\n",
    "                self.acc_list_g.append(acc)\n",
    "                self.loss_list.append(loss2)\n",
    "\n",
    "    def get_acc_list_l(self):\n",
    "        return self.acc_list_l\n",
    "\n",
    "    def get_acc_list_g(self):\n",
    "        return self.acc_list_g\n",
    "\n",
    "    def get_loss_list(self):\n",
    "        return self.loss_list\n",
    "    def save_protos(self,args, test_dataset, user_groups_gt):\n",
    "        \"\"\" Returns the test accuracy and loss.\n",
    "        \"\"\"\n",
    "        loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "        device = self.args.device\n",
    "        criterion = nn.NLLLoss().to(device)\n",
    "\n",
    "        agg_protos_label = {}\n",
    "        for idx in range(self.args.num_users):\n",
    "            agg_protos_label[idx] = {}\n",
    "\n",
    "            testloader = DataLoader(DatasetSplit(test_dataset, user_groups_gt[idx]), batch_size=64, shuffle=True)\n",
    "\n",
    "            self.model.eval()\n",
    "            for batch_idx, (images, labels) in enumerate(testloader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "                outputs, protos = self.model(images)\n",
    "                batch_loss = criterion(outputs, labels)\n",
    "                loss += batch_loss.item()\n",
    "                _, pred_labels = torch.max(outputs, 1)\n",
    "                pred_labels = pred_labels.view(-1)\n",
    "                correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "                total += len(labels)\n",
    "\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i].item() in agg_protos_label[idx]:\n",
    "                        agg_protos_label[idx][labels[i].item()].append(protos[i, :])\n",
    "                    else:\n",
    "                        agg_protos_label[idx][labels[i].item()] = [protos[i, :]]\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        d = []\n",
    "        for i in range(self.args.num_users):\n",
    "            for label in agg_protos_label[i].keys():\n",
    "                for proto in agg_protos_label[i][label]:\n",
    "                    if args.device == 'cuda':\n",
    "                        tmp = proto.cpu().detach().numpy()\n",
    "                    else:\n",
    "                        tmp = proto.detach().numpy()\n",
    "                    x.append(tmp)\n",
    "                    y.append(label)\n",
    "                    d.append(i)\n",
    "\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        d = np.array(d)\n",
    "        np.save('./' + args.alg + '_protos.npy', x)\n",
    "        np.save('./' + args.alg + '_labels.npy', y)\n",
    "        np.save('./' + args.alg + '_idx.npy', d)\n",
    "\n",
    "        print(\"Save protos and labels successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多种数据集的划分方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_iid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample I.I.D. client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: dict of image index\n",
    "    \"\"\"\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
    "                                             replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n",
    "\n",
    "def mnist_noniid(args, dataset, num_users, n_list, k_list):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_shards, num_imgs = 10, 6000\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    labels = dataset.train_labels.numpy()\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "    label_begin = {}\n",
    "    cnt=0\n",
    "    for i in idxs_labels[1,:]:\n",
    "        if i not in label_begin:\n",
    "                label_begin[i] = cnt\n",
    "        cnt+=1\n",
    "\n",
    "    classes_list = []\n",
    "    for i in range(num_users):\n",
    "        n = n_list[i]\n",
    "        k = k_list[i]\n",
    "        k_len = args.train_shots_max\n",
    "        classes = random.sample(range(0,args.num_classes), n)\n",
    "        classes = np.sort(classes)\n",
    "        print(\"user {:d}: {:d}-way {:d}-shot\".format(i + 1, n, k))\n",
    "        print(\"classes:\", classes)\n",
    "        user_data = np.array([])\n",
    "        for each_class in classes:\n",
    "            begin = i * k_len + label_begin[each_class.item()]\n",
    "            user_data = np.concatenate((user_data, idxs[begin : begin+k]),axis=0)\n",
    "        dict_users[i] = user_data\n",
    "        classes_list.append(classes)\n",
    "\n",
    "    return dict_users, classes_list\n",
    "\n",
    "def mnist_noniid_lt(args, test_dataset, num_users, n_list, k_list, classes_list):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    num_shards, num_imgs = 10, 1000\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    labels = test_dataset.train_labels.numpy()\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "    label_begin = {}\n",
    "    cnt=0\n",
    "    for i in idxs_labels[1,:]:\n",
    "        if i not in label_begin:\n",
    "                label_begin[i] = cnt\n",
    "        cnt+=1\n",
    "\n",
    "    for i in range(num_users):\n",
    "        k = 40 # 每个类选多少张做测试\n",
    "        classes = classes_list[i]\n",
    "        print(\"local test classes:\", classes)\n",
    "        user_data = np.array([])\n",
    "        for each_class in classes:\n",
    "            begin = i*40 + label_begin[each_class.item()]\n",
    "            user_data = np.concatenate((user_data, idxs[begin : begin+k]),axis=0)\n",
    "        dict_users[i] = user_data\n",
    "\n",
    "\n",
    "    return dict_users\n",
    "\n",
    "def mnist_noniid_unequal(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset s.t clients\n",
    "    have unequal amount of data\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :returns a dict of clients with each clients assigned certain\n",
    "    number of training imgs\n",
    "    \"\"\"\n",
    "    num_shards, num_imgs = 1200, 50\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    labels = dataset.train_labels.numpy()\n",
    "\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    min_shard = 1\n",
    "    max_shard = 30\n",
    "\n",
    "    random_shard_size = np.random.randint(min_shard, max_shard+1,\n",
    "                                          size=num_users)\n",
    "    random_shard_size = np.around(random_shard_size /\n",
    "                                  sum(random_shard_size) * num_shards)\n",
    "    random_shard_size = random_shard_size.astype(int)\n",
    "\n",
    "    if sum(random_shard_size) > num_shards:\n",
    "\n",
    "        for i in range(num_users):\n",
    "            rand_set = set(np.random.choice(idx_shard, 1, replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "        random_shard_size = random_shard_size-1\n",
    "\n",
    "        for i in range(num_users):\n",
    "            if len(idx_shard) == 0:\n",
    "                continue\n",
    "            shard_size = random_shard_size[i]\n",
    "            if shard_size > len(idx_shard):\n",
    "                shard_size = len(idx_shard)\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "    else:\n",
    "\n",
    "        for i in range(num_users):\n",
    "            shard_size = random_shard_size[i]\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "        if len(idx_shard) > 0:\n",
    "            shard_size = len(idx_shard)\n",
    "            k = min(dict_users, key=lambda x: len(dict_users.get(x)))\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            for rand in rand_set:\n",
    "                dict_users[k] = np.concatenate(\n",
    "                    (dict_users[k], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(args, n_list, k_list):\n",
    "    \"\"\" Returns train and test datasets and a user group which is a dict where\n",
    "    the keys are the user index and the values are the corresponding data for\n",
    "    each of those users.\n",
    "    \"\"\"\n",
    "    data_dir = args.data_dir + args.dataset\n",
    "    if args.dataset == 'mnist':\n",
    "        apply_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "        train_dataset = datasets.MNIST(data_dir, train=True, download=True,\n",
    "                                       transform=apply_transform)\n",
    "\n",
    "        test_dataset = datasets.MNIST(data_dir, train=False, download=True,\n",
    "                                      transform=apply_transform)\n",
    "\n",
    "        if args.iid:\n",
    "            user_groups = mnist_iid(train_dataset, args.num_users)\n",
    "        else:\n",
    "            if args.unequal:\n",
    "                user_groups = mnist_noniid_unequal(args, train_dataset, args.num_users)\n",
    "            else:\n",
    "                user_groups, classes_list = mnist_noniid(args, train_dataset, args.num_users, n_list, k_list)\n",
    "                user_groups_lt = mnist_noniid_lt(args, test_dataset, args.num_users, n_list, k_list, classes_list)\n",
    "                classes_list_gt = classes_list\n",
    "\n",
    "    elif args.dataset == 'femnist':\n",
    "        apply_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "        train_dataset = femnist.FEMNIST(args, data_dir, train=True, download=True,\n",
    "                                        transform=apply_transform)\n",
    "        test_dataset = femnist.FEMNIST(args, data_dir, train=False, download=True,\n",
    "                                       transform=apply_transform)\n",
    "\n",
    "        if args.iid:\n",
    "            user_groups = femnist_iid(train_dataset, args.num_users)\n",
    "        else:\n",
    "            if args.unequal:\n",
    "                user_groups = femnist_noniid_unequal(args, train_dataset, args.num_users)\n",
    "            else:\n",
    "                user_groups, classes_list, classes_list_gt = femnist_noniid(args, args.num_users, n_list, k_list)\n",
    "                user_groups_lt = femnist_noniid_lt(args, args.num_users, classes_list)\n",
    "\n",
    "    elif args.dataset == 'cifar10':\n",
    "        train_dataset = datasets.CIFAR10(data_dir, train=True, download=True, transform=trans_cifar10_train)\n",
    "        test_dataset = datasets.CIFAR10(data_dir, train=False, download=True, transform=trans_cifar10_val)\n",
    "\n",
    "\n",
    "        if args.iid:\n",
    "            user_groups = cifar_iid(train_dataset, args.num_users)\n",
    "        else:\n",
    "            if args.unequal:\n",
    "                raise NotImplementedError()\n",
    "            else:\n",
    "                user_groups, classes_list, classes_list_gt = cifar10_noniid(args, train_dataset, args.num_users, n_list, k_list)\n",
    "                user_groups_lt = cifar10_noniid_lt(args, test_dataset, args.num_users, n_list, k_list, classes_list)\n",
    "\n",
    "\n",
    "    return train_dataset, test_dataset, user_groups, user_groups_lt, classes_list, classes_list_gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各个客户端完成本地更新，将更新后的本地原型上传服务器，服务器完成全局原型聚合，最后对全局模型进行测试。主要是需要注意server和clients之间的各数据交互过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg(total,length):\n",
    "    return total / length\n",
    "def add_and_div(loss,length):\n",
    "    return sum(loss) / length\n",
    "def mean(list_a):\n",
    "    return np.mean(list_a)\n",
    "def std(list_a):\n",
    "    return np.std(list_a)\n",
    "def FedProto_taskheter(args, train_dataset, test_dataset, user_groups, user_groups_lt, classes_list, clients, server,server_pyu):\n",
    "\n",
    "    global_protos = []\n",
    "    idxs_users = np.arange(args.num_users)\n",
    "\n",
    "    train_loss, train_accuracy = [], []\n",
    "    local_model_list = []\n",
    "    for round in tqdm(range(args.rounds)):\n",
    "        local_weights,local_weights_global, local_losses, local_protos = [], [],[], {}\n",
    "        print(f'\\n | Global Training Round : {round + 1} |\\n')\n",
    "        for idx,local_model in enumerate(clients):    \n",
    "            \n",
    "            local_model.update_weights_het(args, idx, global_round=round)\n",
    "            w = local_model.get_param_model()\n",
    "            loss = local_model.get_loss()\n",
    "            acc = local_model.get_acc()\n",
    "            protos = local_model.get_protos()\n",
    "            agg_protos = local_model.agg_func(protos).to(server.device)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_weights_global.append(copy.deepcopy(w.to(server.device)))\n",
    "            local_losses.append(copy.deepcopy(local_model.get_local_loss()).to(server.device))\n",
    "            local_protos[idx] = agg_protos          \n",
    "\n",
    "        local_weights_list = local_weights\n",
    "        local_weights_list_global =local_weights_global\n",
    "\n",
    "        for idx,local_model in enumerate(clients):\n",
    "            local_model.set_weights(local_weights_list[idx])\n",
    "        global_protos = server.proto_aggregation(local_protos)\n",
    "        for local_model in clients:\n",
    "            local_protos = global_protos.to(local_model.device)\n",
    "            local_model.set_global_protos(local_protos)\n",
    "            \n",
    "        loss_avg = server_pyu(add_and_div)(local_losses,len(local_losses))\n",
    "        train_loss.append(loss_avg)\n",
    "    global_protos = global_protos.to(server.device)\n",
    "    server.test_inference_new_het_lt(args, local_weights_list_global,test_dataset, classes_list, user_groups_lt, global_protos)\n",
    "    acc_list_l = server.get_acc_list_l()\n",
    "    acc_list_g = server.get_acc_list_g()\n",
    "    loss_list = server.get_loss_list()\n",
    "    print('For all users (with protos), mean of test acc is ',sf.reveal(server_pyu(mean)(acc_list_g)), 'std of test acc is ',sf.reveal(server_pyu(std)(acc_list_g)))\n",
    "    print('For all users (w/o protos), mean of test acc is ',sf.reveal(server_pyu(mean)(acc_list_l)), 'std of test acc is ', sf.reveal(server_pyu(std)(acc_list_l)))\n",
    "    print('For all users (with protos), mean of proto loss is ', sf.reveal(server_pyu(mean)(loss_list)),'std of test acc is ', sf.reveal(server_pyu(std)(loss_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在隐语平台实现FedProto，主要是一些初始化的内容，包含数据集和客户端、服务器的初始化，最后执行FedProto_taskheter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secretflow as sf\n",
    "\n",
    "start_time = time.time()\n",
    "args = args_parser()\n",
    "exp_details(args)\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if args.device == 'cuda':\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "else:\n",
    "    torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "print(args.device)\n",
    "print(torch.cuda.is_available())\n",
    "n_list = np.random.randint(max(2, args.ways - args.stdev), min(args.num_classes, args.ways + args.stdev + 1), args.num_users)\n",
    "if args.dataset == 'mnist':\n",
    "    k_list = np.random.randint(args.shots - args.stdev + 1 , args.shots + args.stdev - 1, args.num_users)\n",
    "train_dataset, test_dataset, user_groups, user_groups_lt, classes_list, classes_list_gt = get_dataset(args, n_list, k_list)\n",
    "\n",
    "sf.shutdown()\n",
    "sf.init([f\"client_{i}\" for i in range(1, args.num_users + 1)]+[\"server\"], address='local', num_gpus=1)\n",
    "# sf.init([\"client_1\", \"client_2\", \"client_3\", \"client_4\", \"client_5\", \"client_6\", \"client_7\", \"client_8\",\"server\"], address='local', num_gpus=1)\n",
    "clients = []\n",
    "for i in np.arange(args.num_users):\n",
    "    client_i = \"client_\"+str(i+1)\n",
    "    print(client_i)\n",
    "    client_i_pyu = sf.PYU(client_i)\n",
    "    client_i = Client(args=args, dataset=train_dataset, idxs=user_groups[i],device=client_i_pyu)\n",
    "    \n",
    "    clients.append(client_i)\n",
    "server_pyu = sf.PYU(\"server\")\n",
    "server = Server(args,device = server_pyu)\n",
    "print(\"clients\",clients)\n",
    "for idx,local_model in enumerate(clients):\n",
    "    print(idx,local_model)\n",
    "\n",
    "FedProto_taskheter(args, train_dataset, test_dataset, user_groups, user_groups_lt, classes_list,clients, server,server_pyu)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedProto",
   "language": "python",
   "name": "fedproto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
