{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该Notebook在隐语环境中实现了论文FedProto: Federated Prototype Learning across Heterogeneous Clients的数据划分和联邦学习方法。reference:Tan, Y., Long, G., Liu, L., Zhou, T., Lu, Q., Jiang, J., & Zhang, C. (2022, June). Fedproto: Federated prototype learning across heterogeneous clients. In Proceedings of the AAAI conference on artificial intelligence (Vol. 36, No. 8, pp. 8432-8440).https://github.com/yuetan031/FedProto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from tqdm import tqdm\n",
    "import copy, sys\n",
    "import time\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "from secretflow import PYUObject, proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些超参数设置，可以通过同级目录下的config.ini设置对应不同数据集的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--rounds\", type=int, default=100, help=\"number of rounds of training\"\n",
    "    )\n",
    "    parser.add_argument(\"--num_users\", type=int, default=20, help=\"number of users: K\")\n",
    "    parser.add_argument(\n",
    "        \"--frac\", type=float, default=0.04, help=\"the fraction of clients: C\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_ep\", type=int, default=1, help=\"the number of local episodes: E\"\n",
    "    )\n",
    "    parser.add_argument(\"--local_bs\", type=int, default=4, help=\"local batch size: B\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.01, help=\"learning rate\")\n",
    "    parser.add_argument(\n",
    "        \"--momentum\", type=float, default=0.5, help=\"SGD momentum (default: 0.5)\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--model\", type=str, default=\"cnn\", help=\"model name\")\n",
    "    parser.add_argument(\"--alg\", type=str, default=\"fedproto\", help=\"algorithms\")\n",
    "    parser.add_argument(\"--mode\", type=str, default=\"task_heter\", help=\"mode\")\n",
    "    parser.add_argument(\n",
    "        \"--num_channels\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"number \\\n",
    "                        of channels of imgs\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--norm\", type=str, default=\"batch_norm\", help=\"batch_norm, layer_norm, or None\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_filters\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"number of filters for conv nets -- 32 for \\\n",
    "                        mini-imagenet, 64 for omiglot.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_pool\",\n",
    "        type=str,\n",
    "        default=\"True\",\n",
    "        help=\"Whether use max pooling rather than \\\n",
    "                        strided convolutions\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\", type=str, default=\"../data/\", help=\"directory of dataset\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"mnist\",\n",
    "        help=\"name \\\n",
    "                        of dataset\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_classes\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"number \\\n",
    "                        of classes\",\n",
    "    )\n",
    "    parser.add_argument(\"--device\", default=\"cpu\", help=\"To use cuda\")\n",
    "    parser.add_argument(\n",
    "        \"--gpu\",\n",
    "        default=0,\n",
    "        help=\"To use cuda, set \\\n",
    "                        to a specific GPU ID. Default set to use CPU.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--optimizer\",\n",
    "        type=str,\n",
    "        default=\"sgd\",\n",
    "        help=\"type \\\n",
    "                        of optimizer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--iid\", type=int, default=0, help=\"Default set to IID. Set to 0 for non-IID.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--unequal\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"whether to use unequal data splits for  \\\n",
    "                        non-i.i.d setting (use 0 for equal splits)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--stopping_rounds\", type=int, default=10, help=\"rounds of early stopping\"\n",
    "    )\n",
    "    parser.add_argument(\"--verbose\", type=int, default=1, help=\"verbose\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1234, help=\"random seed\")\n",
    "    parser.add_argument(\n",
    "        \"--test_ep\", type=int, default=10, help=\"num of test episodes for evaluation\"\n",
    "    )\n",
    "    parser.add_argument(\"--ways\", type=int, default=3, help=\"num of classes\")\n",
    "    parser.add_argument(\"--shots\", type=int, default=100, help=\"num of shots\")\n",
    "    parser.add_argument(\"--train_shots_max\", type=int, default=110, help=\"num of shots\")\n",
    "    parser.add_argument(\"--test_shots\", type=int, default=15, help=\"num of shots\")\n",
    "    parser.add_argument(\"--stdev\", type=int, default=2, help=\"stdev of ways\")\n",
    "    parser.add_argument(\"--ld\", type=float, default=1, help=\"weight of proto loss\")\n",
    "    parser.add_argument(\"--ft_round\", type=int, default=10, help=\"round of fine tuning\")\n",
    "    arg_list = None\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "    arg_list = []\n",
    "    for k, v in config[\"train\"].items():\n",
    "        arg_list.append(\"--\" + k)\n",
    "        arg_list.append(v)\n",
    "\n",
    "    args = parser.parse_args(arg_list)\n",
    "    return args\n",
    "\n",
    "\n",
    "args = args_parser()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_details(args):\n",
    "    print(\"\\nExperimental details:\")\n",
    "    print(f\"    Model     : {args.model}\")\n",
    "    print(f\"    Optimizer : {args.optimizer}\")\n",
    "    print(f\"    Learning  : {args.lr}\")\n",
    "    print(f\"    Global Rounds   : {args.rounds}\\n\")\n",
    "\n",
    "    print(\"    Federated parameters:\")\n",
    "    if args.iid:\n",
    "        print(\"    IID\")\n",
    "    else:\n",
    "        print(\"    Non-IID\")\n",
    "    print(f\"    Fraction of users  : {args.frac}\")\n",
    "    print(f\"    Local Batch size   : {args.local_bs}\")\n",
    "    print(f\"    Local Epochs       : {args.train_ep}\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "客户端的本地模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(int(320 / 20 * 20), 50)\n",
    "        self.fc2 = nn.Linear(50, args.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x1, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1), x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "客户端类，主要包含了初始化、数据集划分和本地更新等主要函数，本地更新的loss由交叉熵loss和本地原型、全局原型之间的mse_loss之和组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@proxy(PYUObject)\n",
    "class Client(object):\n",
    "    def __init__(self, args, dataset, idxs):\n",
    "        self.args = args\n",
    "        self.trainloader = self.train_val_test(dataset, list(idxs))\n",
    "        self.device = \"cpu\"\n",
    "        self.criterion = nn.NLLLoss().to(self.device)\n",
    "        self.model_param = None\n",
    "        self.loss = None\n",
    "        self.acc = None\n",
    "        self.protos = None\n",
    "        self.local_loss = None\n",
    "        self.proto_loss = None\n",
    "        self.global_protos = []\n",
    "        self.model = CNNMnist(args=args).to(self.device)\n",
    "\n",
    "    def train_val_test(self, dataset, idxs):\n",
    "        \"\"\"\n",
    "        Returns train, validation and test dataloaders for a given dataset\n",
    "        and user indexes.\n",
    "        \"\"\"\n",
    "        idxs_train = idxs[: int(1 * len(idxs))]\n",
    "        trainloader = DataLoader(\n",
    "            DatasetSplit(dataset, idxs_train),\n",
    "            batch_size=self.args.local_bs,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        return trainloader\n",
    "\n",
    "    def update_weights_het(self, args, idx, global_round=round):\n",
    "        self.model.train()\n",
    "        epoch_loss = {\"total\": [], \"1\": [], \"2\": [], \"3\": []}\n",
    "\n",
    "        if self.args.optimizer == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(\n",
    "                self.model.parameters(), lr=self.args.lr, momentum=0.5\n",
    "            )\n",
    "        elif self.args.optimizer == \"adam\":\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.model.parameters(), lr=self.args.lr, weight_decay=1e-4\n",
    "            )\n",
    "\n",
    "        for iter in range(self.args.train_ep):\n",
    "            batch_loss = {\"total\": [], \"1\": [], \"2\": [], \"3\": []}\n",
    "            agg_protos_label = {}\n",
    "            for batch_idx, (images, label_g) in enumerate(self.trainloader):\n",
    "                images, labels = images.to(self.device), label_g.to(self.device)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "                log_probs, protos = self.model(images)\n",
    "                loss1 = self.criterion(log_probs, labels)\n",
    "\n",
    "                loss_mse = nn.MSELoss()\n",
    "                if len(self.global_protos) == 0:\n",
    "                    loss2 = 0 * loss1\n",
    "                else:\n",
    "                    proto_new = copy.deepcopy(protos.data)\n",
    "                    i = 0\n",
    "                    for label in labels:\n",
    "                        if label.item() in self.global_protos.keys():\n",
    "                            proto_new[i, :] = self.global_protos[label.item()][0].data\n",
    "                        i += 1\n",
    "                    loss2 = loss_mse(proto_new, protos)\n",
    "\n",
    "                loss = loss1 + loss2 * args.ld\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                for i in range(len(labels)):\n",
    "                    if label_g[i].item() in agg_protos_label:\n",
    "                        agg_protos_label[label_g[i].item()].append(protos[i, :])\n",
    "                    else:\n",
    "                        agg_protos_label[label_g[i].item()] = [protos[i, :]]\n",
    "\n",
    "                log_probs = log_probs[:, 0 : args.num_classes]\n",
    "                _, y_hat = log_probs.max(1)\n",
    "                acc_val = torch.eq(y_hat, labels.squeeze()).float().mean()\n",
    "\n",
    "                if self.args.verbose and (batch_idx % 10 == 0):\n",
    "                    print(\n",
    "                        \"| Global Round : {} | User: {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tLoss: {:.3f} | Acc: {:.3f}\".format(\n",
    "                            global_round,\n",
    "                            idx,\n",
    "                            iter,\n",
    "                            batch_idx * len(images),\n",
    "                            len(self.trainloader.dataset),\n",
    "                            100.0 * batch_idx / len(self.trainloader),\n",
    "                            loss.item(),\n",
    "                            acc_val.item(),\n",
    "                        )\n",
    "                    )\n",
    "                batch_loss[\"total\"].append(loss.item())\n",
    "                batch_loss[\"1\"].append(loss1.item())\n",
    "                batch_loss[\"2\"].append(loss2.item())\n",
    "            epoch_loss[\"total\"].append(\n",
    "                sum(batch_loss[\"total\"]) / len(batch_loss[\"total\"])\n",
    "            )\n",
    "            epoch_loss[\"1\"].append(sum(batch_loss[\"1\"]) / len(batch_loss[\"1\"]))\n",
    "            epoch_loss[\"2\"].append(sum(batch_loss[\"2\"]) / len(batch_loss[\"2\"]))\n",
    "\n",
    "        epoch_loss[\"total\"] = sum(epoch_loss[\"total\"]) / len(epoch_loss[\"total\"])\n",
    "        epoch_loss[\"1\"] = sum(epoch_loss[\"1\"]) / len(epoch_loss[\"1\"])\n",
    "        epoch_loss[\"2\"] = sum(epoch_loss[\"2\"]) / len(epoch_loss[\"2\"])\n",
    "\n",
    "        self.set_param_model(self.model.state_dict())\n",
    "        self.set_loss(epoch_loss)\n",
    "        self.set_acc(acc_val.item())\n",
    "        self.set_protos(agg_protos_label)\n",
    "\n",
    "    def set_param_model(self, param):\n",
    "        self.model_param = param\n",
    "\n",
    "    def get_param_model(self):\n",
    "        return self.model_param\n",
    "\n",
    "    def set_loss(self, loss):\n",
    "        self.loss = loss\n",
    "        self.local_loss = loss[\"total\"]\n",
    "        self.proto_loss = loss[\"2\"]\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.loss\n",
    "\n",
    "    def get_local_loss(self):\n",
    "        return self.local_loss\n",
    "\n",
    "    def get_proto_loss(self):\n",
    "        return self.proto_loss\n",
    "\n",
    "    def set_acc(self, acc):\n",
    "        self.acc = acc\n",
    "\n",
    "    def get_acc(self):\n",
    "        return self.acc\n",
    "\n",
    "    def set_protos(self, protos):\n",
    "        self.protos = protos\n",
    "\n",
    "    def get_protos(self):\n",
    "        return self.protos\n",
    "\n",
    "    def set_global_protos(self, global_protos):\n",
    "        self.global_protos = global_protos\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.model.load_state_dict(weights, strict=True)\n",
    "\n",
    "    def agg_func(self, protos):\n",
    "        \"\"\"\n",
    "        Returns the average of the weights.\n",
    "        \"\"\"\n",
    "\n",
    "        for [label, proto_list] in protos.items():\n",
    "            if len(proto_list) > 1:\n",
    "                proto = 0 * proto_list[0].data\n",
    "                for i in proto_list:\n",
    "                    proto += i.data\n",
    "                protos[label] = proto / len(proto_list)\n",
    "            else:\n",
    "                protos[label] = proto_list[0]\n",
    "\n",
    "        return protos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "服务器类，主要包含了初始化、全局原型聚合和测试全局模型、保存原型等函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@proxy(PYUObject)\n",
    "class Server(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.device = \"cpu\"\n",
    "        self.acc_list_l = []\n",
    "        self.acc_list_g = []\n",
    "        self.loss_list = []\n",
    "        self.model = CNNMnist(args=args).to(self.device)\n",
    "\n",
    "    def proto_aggregation(self, local_protos_list):\n",
    "        agg_protos_label = dict()\n",
    "        for idx in local_protos_list:\n",
    "            local_protos = local_protos_list[idx]\n",
    "            for label in local_protos.keys():\n",
    "                if label in agg_protos_label:\n",
    "                    agg_protos_label[label].append(local_protos[label])\n",
    "                else:\n",
    "                    agg_protos_label[label] = [local_protos[label]]\n",
    "\n",
    "        for [label, proto_list] in agg_protos_label.items():\n",
    "            if len(proto_list) > 1:\n",
    "                proto = 0 * proto_list[0].data\n",
    "                for i in proto_list:\n",
    "                    proto += i.data\n",
    "                agg_protos_label[label] = [proto / len(proto_list)]\n",
    "            else:\n",
    "                agg_protos_label[label] = [proto_list[0].data]\n",
    "\n",
    "        return agg_protos_label\n",
    "\n",
    "    def test_inference_new_het_lt(\n",
    "        self,\n",
    "        args,\n",
    "        local_weights_list_global,\n",
    "        test_dataset,\n",
    "        classes_list,\n",
    "        user_groups_gt,\n",
    "        global_protos=[],\n",
    "    ):\n",
    "        \"\"\"Returns the test accuracy and loss.\"\"\"\n",
    "        loss, total, correct = 0.0, 0.0, 0.0\n",
    "        loss_mse = nn.MSELoss()\n",
    "        criterion = nn.NLLLoss().to(self.device)\n",
    "\n",
    "        for idx in range(args.num_users):\n",
    "            self.model.load_state_dict(local_weights_list_global[idx], strict=True)\n",
    "            testloader = DataLoader(\n",
    "                DatasetSplit(test_dataset, user_groups_gt[idx]),\n",
    "                batch_size=64,\n",
    "                shuffle=True,\n",
    "            )\n",
    "            self.model.eval()\n",
    "            for batch_idx, (images, labels) in enumerate(testloader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                self.model.zero_grad()\n",
    "                outputs, protos = self.model(images)\n",
    "                batch_loss = criterion(outputs, labels)\n",
    "                loss += batch_loss.item()\n",
    "                _, pred_labels = torch.max(outputs, 1)\n",
    "                pred_labels = pred_labels.view(-1)\n",
    "                correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "                total += len(labels)\n",
    "\n",
    "            acc = correct / total\n",
    "            print(\"| User: {} | Global Test Acc w/o protos: {:.3f}\".format(idx, acc))\n",
    "            self.acc_list_l.append(acc)\n",
    "\n",
    "            if global_protos != []:\n",
    "                for batch_idx, (images, labels) in enumerate(testloader):\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    self.model.zero_grad()\n",
    "                    outputs, protos = self.model(images)\n",
    "                    a_large_num = 100\n",
    "                    dist = a_large_num * torch.ones(\n",
    "                        size=(images.shape[0], args.num_classes)\n",
    "                    ).to(\n",
    "                        self.device\n",
    "                    )  # initialize a distance matrix\n",
    "                    for i in range(images.shape[0]):\n",
    "                        for j in range(args.num_classes):\n",
    "                            if j in global_protos.keys() and j in classes_list[idx]:\n",
    "                                d = loss_mse(protos[i, :], global_protos[j][0])\n",
    "                                dist[i, j] = d\n",
    "\n",
    "                    _, pred_labels = torch.min(dist, 1)\n",
    "                    pred_labels = pred_labels.view(-1)\n",
    "                    correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "                    total += len(labels)\n",
    "                    proto_new = copy.deepcopy(protos.data)\n",
    "                    i = 0\n",
    "                    for label in labels:\n",
    "                        if label.item() in global_protos.keys():\n",
    "                            proto_new[i, :] = global_protos[label.item()][0].data\n",
    "                        i += 1\n",
    "                    loss2 = loss_mse(proto_new, protos)\n",
    "                    if self.device == \"cuda\":\n",
    "                        loss2 = loss2.cpu().detach().numpy()\n",
    "                    else:\n",
    "                        loss2 = loss2.detach().numpy()\n",
    "\n",
    "                acc = correct / total\n",
    "                print(\n",
    "                    \"| User: {} | Global Test Acc with protos: {:.5f}\".format(idx, acc)\n",
    "                )\n",
    "                self.acc_list_g.append(acc)\n",
    "                self.loss_list.append(loss2)\n",
    "\n",
    "    def get_acc_list_l(self):\n",
    "        return self.acc_list_l\n",
    "\n",
    "    def get_acc_list_g(self):\n",
    "        return self.acc_list_g\n",
    "\n",
    "    def get_loss_list(self):\n",
    "        return self.loss_list\n",
    "\n",
    "    def save_protos(self, args, test_dataset, user_groups_gt):\n",
    "        \"\"\"Returns the test accuracy and loss.\"\"\"\n",
    "        loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "        device = self.args.device\n",
    "        criterion = nn.NLLLoss().to(device)\n",
    "\n",
    "        agg_protos_label = {}\n",
    "        for idx in range(self.args.num_users):\n",
    "            agg_protos_label[idx] = {}\n",
    "\n",
    "            testloader = DataLoader(\n",
    "                DatasetSplit(test_dataset, user_groups_gt[idx]),\n",
    "                batch_size=64,\n",
    "                shuffle=True,\n",
    "            )\n",
    "\n",
    "            self.model.eval()\n",
    "            for batch_idx, (images, labels) in enumerate(testloader):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                self.model.zero_grad()\n",
    "                outputs, protos = self.model(images)\n",
    "                batch_loss = criterion(outputs, labels)\n",
    "                loss += batch_loss.item()\n",
    "                _, pred_labels = torch.max(outputs, 1)\n",
    "                pred_labels = pred_labels.view(-1)\n",
    "                correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "                total += len(labels)\n",
    "\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i].item() in agg_protos_label[idx]:\n",
    "                        agg_protos_label[idx][labels[i].item()].append(protos[i, :])\n",
    "                    else:\n",
    "                        agg_protos_label[idx][labels[i].item()] = [protos[i, :]]\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        d = []\n",
    "        for i in range(self.args.num_users):\n",
    "            for label in agg_protos_label[i].keys():\n",
    "                for proto in agg_protos_label[i][label]:\n",
    "                    if args.device == \"cuda\":\n",
    "                        tmp = proto.cpu().detach().numpy()\n",
    "                    else:\n",
    "                        tmp = proto.detach().numpy()\n",
    "                    x.append(tmp)\n",
    "                    y.append(label)\n",
    "                    d.append(i)\n",
    "\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        d = np.array(d)\n",
    "        np.save(\"./\" + args.alg + \"_protos.npy\", x)\n",
    "        np.save(\"./\" + args.alg + \"_labels.npy\", y)\n",
    "        np.save(\"./\" + args.alg + \"_idx.npy\", d)\n",
    "\n",
    "        print(\"Save protos and labels successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多种数据集的划分方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_iid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample I.I.D. client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: dict of image index\n",
    "    \"\"\"\n",
    "    num_items = int(len(dataset) / num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n",
    "\n",
    "\n",
    "def mnist_noniid(args, dataset, num_users, n_list, k_list):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_shards, num_imgs = 10, 6000\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {}\n",
    "    idxs = np.arange(num_shards * num_imgs)\n",
    "    labels = dataset.train_labels.numpy()\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "    label_begin = {}\n",
    "    cnt = 0\n",
    "    for i in idxs_labels[1, :]:\n",
    "        if i not in label_begin:\n",
    "            label_begin[i] = cnt\n",
    "        cnt += 1\n",
    "\n",
    "    classes_list = []\n",
    "    for i in range(num_users):\n",
    "        n = n_list[i]\n",
    "        k = k_list[i]\n",
    "        k_len = args.train_shots_max\n",
    "        classes = random.sample(range(0, args.num_classes), n)\n",
    "        classes = np.sort(classes)\n",
    "        print(\"user {:d}: {:d}-way {:d}-shot\".format(i + 1, n, k))\n",
    "        print(\"classes:\", classes)\n",
    "        user_data = np.array([])\n",
    "        for each_class in classes:\n",
    "            begin = i * k_len + label_begin[each_class.item()]\n",
    "            user_data = np.concatenate((user_data, idxs[begin : begin + k]), axis=0)\n",
    "        dict_users[i] = user_data\n",
    "        classes_list.append(classes)\n",
    "\n",
    "    return dict_users, classes_list\n",
    "\n",
    "\n",
    "def mnist_noniid_lt(args, test_dataset, num_users, n_list, k_list, classes_list):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    num_shards, num_imgs = 10, 1000\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {}\n",
    "    idxs = np.arange(num_shards * num_imgs)\n",
    "    labels = test_dataset.train_labels.numpy()\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "    label_begin = {}\n",
    "    cnt = 0\n",
    "    for i in idxs_labels[1, :]:\n",
    "        if i not in label_begin:\n",
    "            label_begin[i] = cnt\n",
    "        cnt += 1\n",
    "\n",
    "    for i in range(num_users):\n",
    "        k = 40  # 每个类选多少张做测试\n",
    "        classes = classes_list[i]\n",
    "        print(\"local test classes:\", classes)\n",
    "        user_data = np.array([])\n",
    "        for each_class in classes:\n",
    "            begin = i * 40 + label_begin[each_class.item()]\n",
    "            user_data = np.concatenate((user_data, idxs[begin : begin + k]), axis=0)\n",
    "        dict_users[i] = user_data\n",
    "\n",
    "    return dict_users\n",
    "\n",
    "\n",
    "def mnist_noniid_unequal(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset s.t clients\n",
    "    have unequal amount of data\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :returns a dict of clients with each clients assigned certain\n",
    "    number of training imgs\n",
    "    \"\"\"\n",
    "    num_shards, num_imgs = 1200, 50\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards * num_imgs)\n",
    "    labels = dataset.train_labels.numpy()\n",
    "\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    min_shard = 1\n",
    "    max_shard = 30\n",
    "\n",
    "    random_shard_size = np.random.randint(min_shard, max_shard + 1, size=num_users)\n",
    "    random_shard_size = np.around(\n",
    "        random_shard_size / sum(random_shard_size) * num_shards\n",
    "    )\n",
    "    random_shard_size = random_shard_size.astype(int)\n",
    "\n",
    "    if sum(random_shard_size) > num_shards:\n",
    "\n",
    "        for i in range(num_users):\n",
    "            rand_set = set(np.random.choice(idx_shard, 1, replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand * num_imgs : (rand + 1) * num_imgs]),\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "        random_shard_size = random_shard_size - 1\n",
    "\n",
    "        for i in range(num_users):\n",
    "            if len(idx_shard) == 0:\n",
    "                continue\n",
    "            shard_size = random_shard_size[i]\n",
    "            if shard_size > len(idx_shard):\n",
    "                shard_size = len(idx_shard)\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size, replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand * num_imgs : (rand + 1) * num_imgs]),\n",
    "                    axis=0,\n",
    "                )\n",
    "    else:\n",
    "\n",
    "        for i in range(num_users):\n",
    "            shard_size = random_shard_size[i]\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size, replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand * num_imgs : (rand + 1) * num_imgs]),\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "        if len(idx_shard) > 0:\n",
    "            shard_size = len(idx_shard)\n",
    "            k = min(dict_users, key=lambda x: len(dict_users.get(x)))\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size, replace=False))\n",
    "            for rand in rand_set:\n",
    "                dict_users[k] = np.concatenate(\n",
    "                    (dict_users[k], idxs[rand * num_imgs : (rand + 1) * num_imgs]),\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(args, n_list, k_list):\n",
    "    \"\"\"Returns train and test datasets and a user group which is a dict where\n",
    "    the keys are the user index and the values are the corresponding data for\n",
    "    each of those users.\n",
    "    \"\"\"\n",
    "    data_dir = args.data_dir + args.dataset\n",
    "    if args.dataset == \"mnist\":\n",
    "        apply_transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "        train_dataset = datasets.MNIST(\n",
    "            data_dir, train=True, download=True, transform=apply_transform\n",
    "        )\n",
    "\n",
    "        test_dataset = datasets.MNIST(\n",
    "            data_dir, train=False, download=True, transform=apply_transform\n",
    "        )\n",
    "\n",
    "        if args.iid:\n",
    "            user_groups = mnist_iid(train_dataset, args.num_users)\n",
    "        else:\n",
    "            if args.unequal:\n",
    "                user_groups = mnist_noniid_unequal(args, train_dataset, args.num_users)\n",
    "            else:\n",
    "                user_groups, classes_list = mnist_noniid(\n",
    "                    args, train_dataset, args.num_users, n_list, k_list\n",
    "                )\n",
    "                user_groups_lt = mnist_noniid_lt(\n",
    "                    args, test_dataset, args.num_users, n_list, k_list, classes_list\n",
    "                )\n",
    "                classes_list_gt = classes_list\n",
    "\n",
    "    elif args.dataset == \"femnist\":\n",
    "        apply_transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "        train_dataset = femnist.FEMNIST(\n",
    "            args, data_dir, train=True, download=True, transform=apply_transform\n",
    "        )\n",
    "        test_dataset = femnist.FEMNIST(\n",
    "            args, data_dir, train=False, download=True, transform=apply_transform\n",
    "        )\n",
    "\n",
    "        if args.iid:\n",
    "            user_groups = femnist_iid(train_dataset, args.num_users)\n",
    "        else:\n",
    "            if args.unequal:\n",
    "                user_groups = femnist_noniid_unequal(\n",
    "                    args, train_dataset, args.num_users\n",
    "                )\n",
    "            else:\n",
    "                user_groups, classes_list, classes_list_gt = femnist_noniid(\n",
    "                    args, args.num_users, n_list, k_list\n",
    "                )\n",
    "                user_groups_lt = femnist_noniid_lt(args, args.num_users, classes_list)\n",
    "\n",
    "    elif args.dataset == \"cifar10\":\n",
    "        train_dataset = datasets.CIFAR10(\n",
    "            data_dir, train=True, download=True, transform=trans_cifar10_train\n",
    "        )\n",
    "        test_dataset = datasets.CIFAR10(\n",
    "            data_dir, train=False, download=True, transform=trans_cifar10_val\n",
    "        )\n",
    "\n",
    "        if args.iid:\n",
    "            user_groups = cifar_iid(train_dataset, args.num_users)\n",
    "        else:\n",
    "            if args.unequal:\n",
    "                raise NotImplementedError()\n",
    "            else:\n",
    "                user_groups, classes_list, classes_list_gt = cifar10_noniid(\n",
    "                    args, train_dataset, args.num_users, n_list, k_list\n",
    "                )\n",
    "                user_groups_lt = cifar10_noniid_lt(\n",
    "                    args, test_dataset, args.num_users, n_list, k_list, classes_list\n",
    "                )\n",
    "\n",
    "    return (\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        user_groups,\n",
    "        user_groups_lt,\n",
    "        classes_list,\n",
    "        classes_list_gt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各个客户端完成本地更新，将更新后的本地原型上传服务器，服务器完成全局原型聚合，最后对全局模型进行测试。主要是需要注意server和clients之间的各数据交互过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg(total, length):\n",
    "    return total / length\n",
    "\n",
    "\n",
    "def add_and_div(loss, length):\n",
    "    return sum(loss) / length\n",
    "\n",
    "\n",
    "def mean(list_a):\n",
    "    return np.mean(list_a)\n",
    "\n",
    "\n",
    "def std(list_a):\n",
    "    return np.std(list_a)\n",
    "\n",
    "\n",
    "def FedProto_taskheter(\n",
    "    args,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    user_groups,\n",
    "    user_groups_lt,\n",
    "    classes_list,\n",
    "    clients,\n",
    "    server,\n",
    "    server_pyu,\n",
    "):\n",
    "\n",
    "    global_protos = []\n",
    "    idxs_users = np.arange(args.num_users)\n",
    "\n",
    "    train_loss, train_accuracy = [], []\n",
    "    local_model_list = []\n",
    "    for round in tqdm(range(args.rounds)):\n",
    "        local_weights, local_weights_global, local_losses, local_protos = [], [], [], {}\n",
    "        print(f\"\\n | Global Training Round : {round + 1} |\\n\")\n",
    "        for idx, local_model in enumerate(clients):\n",
    "\n",
    "            local_model.update_weights_het(args, idx, global_round=round)\n",
    "            w = local_model.get_param_model()\n",
    "            loss = local_model.get_loss()\n",
    "            acc = local_model.get_acc()\n",
    "            protos = local_model.get_protos()\n",
    "            agg_protos = local_model.agg_func(protos).to(server.device)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_weights_global.append(copy.deepcopy(w.to(server.device)))\n",
    "            local_losses.append(\n",
    "                copy.deepcopy(local_model.get_local_loss()).to(server.device)\n",
    "            )\n",
    "            local_protos[idx] = agg_protos\n",
    "\n",
    "        local_weights_list = local_weights\n",
    "        local_weights_list_global = local_weights_global\n",
    "\n",
    "        for idx, local_model in enumerate(clients):\n",
    "            local_model.set_weights(local_weights_list[idx])\n",
    "        global_protos = server.proto_aggregation(local_protos)\n",
    "        for local_model in clients:\n",
    "            local_protos = global_protos.to(local_model.device)\n",
    "            local_model.set_global_protos(local_protos)\n",
    "\n",
    "        loss_avg = server_pyu(add_and_div)(local_losses, len(local_losses))\n",
    "        train_loss.append(loss_avg)\n",
    "    global_protos = global_protos.to(server.device)\n",
    "    server.test_inference_new_het_lt(\n",
    "        args,\n",
    "        local_weights_list_global,\n",
    "        test_dataset,\n",
    "        classes_list,\n",
    "        user_groups_lt,\n",
    "        global_protos,\n",
    "    )\n",
    "    acc_list_l = server.get_acc_list_l()\n",
    "    acc_list_g = server.get_acc_list_g()\n",
    "    loss_list = server.get_loss_list()\n",
    "    print(\n",
    "        \"For all users (with protos), mean of test acc is \",\n",
    "        sf.reveal(server_pyu(mean)(acc_list_g)),\n",
    "        \"std of test acc is \",\n",
    "        sf.reveal(server_pyu(std)(acc_list_g)),\n",
    "    )\n",
    "    print(\n",
    "        \"For all users (w/o protos), mean of test acc is \",\n",
    "        sf.reveal(server_pyu(mean)(acc_list_l)),\n",
    "        \"std of test acc is \",\n",
    "        sf.reveal(server_pyu(std)(acc_list_l)),\n",
    "    )\n",
    "    print(\n",
    "        \"For all users (with protos), mean of proto loss is \",\n",
    "        sf.reveal(server_pyu(mean)(loss_list)),\n",
    "        \"std of test acc is \",\n",
    "        sf.reveal(server_pyu(std)(loss_list)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在隐语平台实现FedProto，主要是一些初始化的内容，包含数据集和客户端、服务器的初始化，最后执行FedProto_taskheter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secretflow as sf\n",
    "\n",
    "start_time = time.time()\n",
    "args = args_parser()\n",
    "exp_details(args)\n",
    "\n",
    "args.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if args.device == \"cuda\":\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "else:\n",
    "    torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "print(args.device)\n",
    "print(torch.cuda.is_available())\n",
    "n_list = np.random.randint(\n",
    "    max(2, args.ways - args.stdev),\n",
    "    min(args.num_classes, args.ways + args.stdev + 1),\n",
    "    args.num_users,\n",
    ")\n",
    "if args.dataset == \"mnist\":\n",
    "    k_list = np.random.randint(\n",
    "        args.shots - args.stdev + 1, args.shots + args.stdev - 1, args.num_users\n",
    "    )\n",
    "(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    user_groups,\n",
    "    user_groups_lt,\n",
    "    classes_list,\n",
    "    classes_list_gt,\n",
    ") = get_dataset(args, n_list, k_list)\n",
    "\n",
    "sf.shutdown()\n",
    "sf.init(\n",
    "    [f\"client_{i}\" for i in range(1, args.num_users + 1)] + [\"server\"],\n",
    "    address=\"local\",\n",
    "    num_gpus=1,\n",
    ")\n",
    "# sf.init([\"client_1\", \"client_2\", \"client_3\", \"client_4\", \"client_5\", \"client_6\", \"client_7\", \"client_8\",\"server\"], address='local', num_gpus=1)\n",
    "clients = []\n",
    "for i in np.arange(args.num_users):\n",
    "    client_i = \"client_\" + str(i + 1)\n",
    "    print(client_i)\n",
    "    client_i_pyu = sf.PYU(client_i)\n",
    "    client_i = Client(\n",
    "        args=args, dataset=train_dataset, idxs=user_groups[i], device=client_i_pyu\n",
    "    )\n",
    "\n",
    "    clients.append(client_i)\n",
    "server_pyu = sf.PYU(\"server\")\n",
    "server = Server(args, device=server_pyu)\n",
    "print(\"clients\", clients)\n",
    "for idx, local_model in enumerate(clients):\n",
    "    print(idx, local_model)\n",
    "\n",
    "FedProto_taskheter(\n",
    "    args,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    user_groups,\n",
    "    user_groups_lt,\n",
    "    classes_list,\n",
    "    clients,\n",
    "    server,\n",
    "    server_pyu,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedProto",
   "language": "python",
   "name": "fedproto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
