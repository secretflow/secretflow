{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c0c61b-3f26-4e33-a3cb-26b168c6e212",
   "metadata": {},
   "source": [
    "# PrivateFL Implementation in SecretFlow\n",
    "\n",
    "This notebook implements the PrivateFL method based on the paper [PRIVATEFL: Accurate, Differentially Private Federated Learningvia Personalized Data Transformation]\n",
    "by [Yuchen Yang∗, Bo Hui∗, Haolin Yuan∗, Neil Gong†, and Yinzhi Cao The Johns Hopkins University, †Duke University] ([https://www.usenix.org/system/files/sec23fall-prepub-427-yang-yuchen.pdf]).\n",
    "\n",
    "The implementation has been modified and adapted to work with the SecretFlow framework\n",
    "for demonstration and educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2fa499-5f1f-473c-bec6-f6cdaa24e458",
   "metadata": {},
   "source": [
    "# PrivateFL: 基于个性化数据转换的精确差分隐私联邦学习\n",
    "\n",
    "PrivateFL是一种新的差分隐私联邦学习方法，旨在通过个性化数据转换来提高模型精度。其核心思想包括：\n",
    "\n",
    "1. 观察到差分隐私（DP）会在联邦学习中引入额外的客户端异质性，从而降低模型精度。\n",
    "2. 为每个客户端学习一个差分隐私的个性化数据转换，以减少DP引入的异质性。\n",
    "3. 数据转换与本地模型同时学习，优化以最小化学习损失并最大化本地客户端模型效用。\n",
    "4. 可与现有的个性化联邦学习方法和DP效用改进方法结合，进一步提高精度。\n",
    "\n",
    "本实现基于SecretFlow框架，展示了PrivateFL在中央差分隐私（CDP）和本地差分隐私（LDP）设置下的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052473c-7b20-493b-bc98-a817e662d29d",
   "metadata": {},
   "source": [
    "## 1.modelUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e80cb2e-0bad-449a-add6-8183e66c36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import alexnet, resnet18\n",
    "from torch.nn.functional import relu, softmax, max_pool2d\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch import nn, tanh\n",
    "import copy\n",
    "from opacus.grad_sample import register_grad_sampler\n",
    "from typing import Dict\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "from numpy import median\n",
    "import numpy as np\n",
    "import torch.nn.functional as func\n",
    "\n",
    "\n",
    "def agg_weights(weights):\n",
    "    with torch.no_grad():\n",
    "        weights_avg = copy.deepcopy(weights[0])\n",
    "        for k in weights_avg.keys():\n",
    "            for i in range(1, len(weights)):\n",
    "                weights_avg[k] += weights[i][k]\n",
    "            weights_avg[k] = torch.div(weights_avg[k], len(weights))\n",
    "    return weights_avg\n",
    "\n",
    "\n",
    "def evaluate_global(users, test_dataloders, users_index):\n",
    "\n",
    "    testing_corrects = 0\n",
    "    testing_sum = 0\n",
    "    for index in users_index:\n",
    "        result = users[index].evaluate(test_dataloders[index])\n",
    "        corrects, total = sf.reveal(result)\n",
    "        testing_corrects += corrects\n",
    "        testing_sum += total\n",
    "    # 计算并返回全局准确率\n",
    "    if testing_sum > 0:\n",
    "        acc = testing_corrects / testing_sum\n",
    "        print(f\"全局准确率: {acc:.4f}\")\n",
    "        return acc\n",
    "    else:\n",
    "        print(\"没有评估任何样本\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "# 个性化数据转换类\n",
    "class InputNorm(nn.Module):\n",
    "    def __init__(self, num_channel, num_feature):\n",
    "        super().__init__()\n",
    "        self.num_channel = num_channel\n",
    "        self.gamma = nn.Parameter(torch.ones(num_channel))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_channel, num_feature, num_feature))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_channel == 1:\n",
    "            x = self.gamma * x\n",
    "            x = x + self.beta\n",
    "            return x\n",
    "        if self.num_channel == 3:\n",
    "            return torch.einsum(\"...ijk, i->...ijk\", x, self.gamma) + self.beta\n",
    "\n",
    "\n",
    "class resnet18(torch.nn.Module):\n",
    "    \"\"\"Constructs a ResNet-18 model.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.resnet18(pretrained=True)\n",
    "        n_ftrs = self.backbone.fc.in_features\n",
    "        self.backbone.fc = torch.nn.Linear(n_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.backbone(x)\n",
    "        return logits, softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "class resnet18_IN(torch.nn.Module):\n",
    "    \"\"\"Constructs a ResNet-18wIN model.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.resnet18(pretrained=True)\n",
    "        n_ftrs = self.backbone.fc.in_features\n",
    "        self.backbone.fc = torch.nn.Linear(n_ftrs, num_classes)\n",
    "        if num_classes == 8:\n",
    "            self.norm = InputNorm(3, 150)\n",
    "        else:\n",
    "            self.norm = InputNorm(3, 120)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        logits = self.backbone(x)\n",
    "        return logits, softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "class alexnet(torch.nn.Module):\n",
    "    \"\"\"Constructs a alexnet model.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.alexnet(pretrained=True)\n",
    "        n_ftrs = self.backbone.classifier[-1].out_features\n",
    "        self.fc = torch.nn.Linear(n_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.backbone(x)\n",
    "        logits = self.fc(logits)\n",
    "        return logits, softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "class alexnet_IN(torch.nn.Module):\n",
    "    \"\"\"Constructs a alexnet w IN model.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.alexnet(pretrained=True)\n",
    "        n_ftrs = self.backbone.classifier[-1].out_features\n",
    "        self.fc = torch.nn.Linear(n_ftrs, num_classes)\n",
    "        self.norm = InputNorm(3, 150)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        logits = self.backbone(x)\n",
    "        logits = self.fc(logits)\n",
    "        return logits, softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "class mnist_fully_connected_IN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(mnist_fully_connected_IN, self).__init__()\n",
    "        self.hidden1 = 600\n",
    "        self.hidden2 = 100\n",
    "        self.fc1 = nn.Linear(28 * 28, self.hidden1, bias=False)\n",
    "        self.fc2 = nn.Linear(self.hidden1, self.hidden2, bias=False)\n",
    "        self.fc3 = nn.Linear(self.hidden2, num_classes, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.norm = InputNorm(1, 28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = relu(self.fc1(x))\n",
    "        x = relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits, softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "class mnist_fully_connected(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(mnist_fully_connected, self).__init__()\n",
    "        self.hidden1 = 600\n",
    "        self.hidden2 = 100\n",
    "        self.fc1 = nn.Linear(28 * 28, self.hidden1, bias=False)\n",
    "        self.fc2 = nn.Linear(self.hidden1, self.hidden2, bias=False)\n",
    "        self.fc3 = nn.Linear(self.hidden2, num_classes, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = relu(self.fc1(x))\n",
    "        x = relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits, softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "class purchase_fully_connected(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(purchase_fully_connected, self).__init__()\n",
    "        self.fc1 = nn.Linear(600, 512, bias=False)\n",
    "        self.fc2 = nn.Linear(512, 256, bias=False)\n",
    "        self.fc3 = nn.Linear(256, 128, bias=False)\n",
    "        self.fc4 = nn.Linear(128, num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = tanh(self.fc1(x))\n",
    "        x = tanh(self.fc2(x))\n",
    "        x = tanh(self.fc3(x))\n",
    "        logits = self.fc4(x)\n",
    "        return logits, softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "class purchase_fully_connected_IN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(purchase_fully_connected_IN, self).__init__()\n",
    "        self.fc1 = nn.Linear(600, 512, bias=False)\n",
    "        self.fc2 = nn.Linear(512, 256, bias=False)\n",
    "        self.fc3 = nn.Linear(256, 128, bias=False)\n",
    "        self.fc4 = nn.Linear(128, num_classes, bias=False)\n",
    "        self.norm = FeatureNorm(600)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = tanh(self.fc1(x))\n",
    "        x = tanh(self.fc2(x))\n",
    "        x = tanh(self.fc3(x))\n",
    "        logits = self.fc4(x)\n",
    "        return logits, softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "class linear_model(nn.Module):\n",
    "    def __init__(self, num_classes, input_shape=512):\n",
    "        super(linear_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc1(x)\n",
    "        return logits, softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "def standardize(x, bn_stats):\n",
    "    if bn_stats is None:\n",
    "        return x\n",
    "\n",
    "    bn_mean, bn_var = bn_stats\n",
    "    bn_mean, bn_var = bn_mean.to(x.device), bn_var.to(x.device)\n",
    "    view = [1] * len(x.shape)\n",
    "    view[1] = -1\n",
    "    x = (x - bn_mean.reshape(view)) / torch.sqrt(bn_var.reshape(view) + 1e-5)\n",
    "\n",
    "    # if variance is too low, just ignore\n",
    "    x *= bn_var.reshape(view) != 0\n",
    "    return x\n",
    "\n",
    "\n",
    "class linear_model_DN(nn.Module):\n",
    "    def __init__(self, num_classes, input_shape=512, bn_stats=False):\n",
    "        super(linear_model_DN, self).__init__()\n",
    "        if not bn_stats:\n",
    "            self.bn_stats = (torch.zeros(input_shape), torch.ones(input_shape))\n",
    "        else:\n",
    "            mean = np.load(\"transfer/cifar100_resnext_mean.npy\")\n",
    "            var = np.load(\"transfer/cifar100_resnext_var.npy\")\n",
    "            self.bn_stats = (torch.from_numpy(mean), torch.from_numpy(var))\n",
    "        self.fc1 = nn.Linear(input_shape, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = standardize(x, self.bn_stats)\n",
    "        logits = self.fc1(x)\n",
    "        return logits, softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "class FeatureNorm(nn.Module):\n",
    "    def __init__(self, feature_shape):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, feature_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.einsum(\"ni, j->ni\", x, self.gamma)\n",
    "        x = x + self.beta\n",
    "        return x\n",
    "\n",
    "\n",
    "@register_grad_sampler(FeatureNorm)\n",
    "def compute_grad_sample(\n",
    "    layer: InputNorm, activations: torch.Tensor, backprops: torch.Tensor\n",
    ") -> Dict[nn.Parameter, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes per sample gradients for ``nn.Linear`` layer\n",
    "    Args:\n",
    "        layer: Layer\n",
    "        activations: Activations\n",
    "        backprops: Backpropagations\n",
    "    \"\"\"\n",
    "    gs = torch.einsum(\"nk,nk->n\", backprops, activations)\n",
    "    ret = {layer.gamma: gs}\n",
    "    if layer.beta is not None:\n",
    "        ret[layer.beta] = torch.einsum(\"n...i->ni\", backprops)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "class linear_model_DN_IN(nn.Module):\n",
    "    def __init__(self, num_classes, input_shape, bn_stats=False):\n",
    "        super(linear_model_DN_IN, self).__init__()\n",
    "        if not bn_stats:\n",
    "            self.bn_stats = (torch.zeros(input_shape), torch.ones(input_shape))\n",
    "        else:\n",
    "            mean = np.load(\"cifar100_resnext_mean.npy\")\n",
    "            var = np.load(\"cifar100_resnext_mean.npy\")\n",
    "            self.bn_stats = (torch.from_numpy(mean), torch.from_numpy(var))\n",
    "        self.backbone = nn.Linear(input_shape, num_classes, bias=True)\n",
    "        self.norm = FeatureNorm(input_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = standardize(x, self.bn_stats)\n",
    "        logits = self.backbone(x)\n",
    "        return logits, softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "@register_grad_sampler(InputNorm)\n",
    "def compute_grad_sample(\n",
    "    layer: InputNorm, activations: torch.Tensor, backprops: torch.Tensor\n",
    ") -> Dict[nn.Parameter, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes per sample gradients for ``nn.Linear`` layer\n",
    "    Args:\n",
    "        layer: Layer\n",
    "        activations: Activations\n",
    "        backprops: Backpropagations\n",
    "    \"\"\"\n",
    "    gs = torch.einsum(\"nk...,nk...->nk\", backprops, activations)\n",
    "    ret = {layer.gamma: gs}\n",
    "    if layer.beta is not None:\n",
    "        ret[layer.beta] = torch.einsum(\"nijk->nijk\", backprops)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda7e25-8988-4690-854b-13f3376ac3a7",
   "metadata": {},
   "source": [
    "## 2.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa5b7ff-8ee6-4fd5-837a-145becd06d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CHMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, root=\"data/CHMNIST\", train=True, download=True, transform=None):\n",
    "        self.images = []\n",
    "        self.root = root\n",
    "        self.targets = []\n",
    "        self.train = train\n",
    "        self.download = download\n",
    "        self.transform = transform\n",
    "\n",
    "        x_train, x_test, y_train, y_test = self._train_test_split()\n",
    "\n",
    "        if self.train:\n",
    "            self._setup_dataset(x_train, y_train)\n",
    "        else:\n",
    "            self._setup_dataset(x_test, y_test)\n",
    "\n",
    "    def _train_test_split(self):\n",
    "        img_names = []\n",
    "        img_label = []\n",
    "        for i, folder_name in enumerate(os.listdir(self.root)):\n",
    "\n",
    "            for j, img_name in enumerate(os.listdir(self.root + \"/\" + folder_name)):\n",
    "                img_names.append(os.path.join(self.root + \"/\", folder_name, img_name))\n",
    "                img_label.append(int(folder_name[0:2]) - 1)\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            img_names, img_label, train_size=0.9, random_state=1\n",
    "        )\n",
    "\n",
    "        return x_train, x_test, y_train, y_test\n",
    "\n",
    "    def _setup_dataset(self, x, y):\n",
    "        self.images = x\n",
    "        self.targets = y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_fn = self.images[item]\n",
    "        label = self.targets[item]\n",
    "        img = Image.open(img_fn)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "class Purchase(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root=\"data/purchase/dataset_purchase\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.images = []\n",
    "        self.root = root\n",
    "        self.targets = []\n",
    "        self.train = train\n",
    "        self.download = download\n",
    "        self.transform = transform\n",
    "\n",
    "        x_train, x_test, y_train, y_test = self._train_test_split()\n",
    "\n",
    "        if self.train:\n",
    "            self._setup_dataset(x_train, y_train)\n",
    "        else:\n",
    "            self._setup_dataset(x_test, y_test)\n",
    "\n",
    "    def _train_test_split(self):\n",
    "        df = pd.read_csv(self.root)\n",
    "\n",
    "        img_names = df.iloc[:, 1:].to_numpy(dtype=\"f\")\n",
    "        img_label = df.iloc[:, 0].to_numpy() - 1\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            img_names, img_label, train_size=0.8, random_state=1\n",
    "        )\n",
    "\n",
    "        return x_train, x_test, y_train, y_test\n",
    "\n",
    "    def _setup_dataset(self, x, y):\n",
    "        self.images = x\n",
    "        self.targets = y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = self.images[item]\n",
    "        label = self.targets[item]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b6cac-e3ec-4820-b178-24a3fc2a30c5",
   "metadata": {},
   "source": [
    "## 3.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06f1a5d-1cc7-4c38-8d24-451815105734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10, MNIST, FashionMNIST, CIFAR100, EMNIST\n",
    "\n",
    "np.random.seed(2022)\n",
    "\n",
    "\n",
    "def get_datasets(data_name, dataroot, preprocess=None):\n",
    "    \"\"\"\n",
    "    get_datasets returns train/val/test data splits of CIFAR10/100 datasets\n",
    "    :param data_name: name of dataset, choose from [cifar10, cifar100]\n",
    "    :param dataroot: root to data dir\n",
    "    :param normalize: True/False to normalize the data\n",
    "    :param val_size: validation split size (in #samples)\n",
    "    :return: train_set, val_set, test_set (tuple of pytorch dataset/subset)\n",
    "    \"\"\"\n",
    "\n",
    "    if data_name == \"cifar10\":\n",
    "        normalization = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        transform = (\n",
    "            transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Resize(120), normalization]\n",
    "            )\n",
    "            if preprocess == None\n",
    "            else preprocess\n",
    "        )\n",
    "\n",
    "        data_obj = CIFAR10\n",
    "    elif data_name == \"cifar100\":\n",
    "        normalization = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        transform = (\n",
    "            transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Resize(224), normalization]\n",
    "            )\n",
    "            if preprocess == None\n",
    "            else preprocess\n",
    "        )\n",
    "\n",
    "        data_obj = CIFAR100\n",
    "    elif data_name == \"mnist\":\n",
    "        normalization = transforms.Normalize((0.5,), (0.5,))\n",
    "        transform = transforms.Compose([transforms.ToTensor(), normalization])\n",
    "        data_obj = MNIST\n",
    "    elif data_name == \"fashionmnist\":\n",
    "        normalization = transforms.Normalize((0.5,), (0.5,))\n",
    "        transform = transforms.Compose([transforms.ToTensor(), normalization])\n",
    "        data_obj = FashionMNIST\n",
    "    elif data_name == \"emnist\":\n",
    "        normalization = transforms.Normalize((0.5,), (0.5,))\n",
    "        transform = transforms.Compose([transforms.ToTensor(), normalization])\n",
    "        data_obj = EMNIST\n",
    "    elif data_name == \"purchase\":\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        data_obj = Purchase\n",
    "    elif data_name == \"chmnist\":\n",
    "        normalization = transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "        )\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Resize((150, 150)), normalization]\n",
    "        )\n",
    "        data_obj = CHMNIST\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"choose data_name from ['mnist', 'cifar10', 'cifar100', 'fashionmnist', 'emnist, 'purchase', 'chmnist']\"\n",
    "        )\n",
    "\n",
    "    if data_name == \"emnist\":\n",
    "        train_set = data_obj(\n",
    "            dataroot, train=True, transform=transform, split=\"digits\", download=True\n",
    "        )\n",
    "\n",
    "        test_set = data_obj(dataroot, train=False, split=\"digits\", transform=transform)\n",
    "\n",
    "    else:\n",
    "        train_set = data_obj(dataroot, train=True, transform=transform, download=True)\n",
    "\n",
    "        test_set = data_obj(dataroot, train=False, transform=transform)\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def get_num_classes_samples(dataset):\n",
    "    \"\"\"\n",
    "    extracts info about certain dataset\n",
    "    :param dataset: pytorch dataset object\n",
    "    :return: dataset info number of classes, number of samples, list of labels\n",
    "    \"\"\"\n",
    "    # ---------------#\n",
    "    # Extract labels #\n",
    "    # ---------------#\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        if isinstance(dataset.dataset.targets, list):\n",
    "            data_labels_list = np.array(dataset.dataset.targets)[dataset.indices]\n",
    "        else:\n",
    "            data_labels_list = dataset.dataset.targets[dataset.indices]\n",
    "    else:\n",
    "        if isinstance(dataset.targets, list):\n",
    "            data_labels_list = np.array(dataset.targets)\n",
    "        else:\n",
    "            data_labels_list = dataset.targets\n",
    "    classes, num_samples = np.unique(data_labels_list, return_counts=True)\n",
    "    num_classes = len(classes)\n",
    "    return num_classes, num_samples, data_labels_list\n",
    "\n",
    "\n",
    "def gen_classes_per_node(\n",
    "    dataset, num_users, classes_per_user=2, high_prob=0.6, low_prob=0.4\n",
    "):\n",
    "    \"\"\"\n",
    "    creates the data distribution of each client\n",
    "    :param dataset: pytorch dataset object\n",
    "    :param num_users: number of clients\n",
    "    :param classes_per_user: number of classes assigned to each client\n",
    "    :param high_prob: highest prob sampled\n",
    "    :param low_prob: lowest prob sampled\n",
    "    :return: dictionary mapping between classes and proportions, each entry refers to other client\n",
    "    \"\"\"\n",
    "    num_classes, num_samples, _ = get_num_classes_samples(dataset)\n",
    "\n",
    "    # -------------------------------------------#\n",
    "    # Divide classes + num samples for each user #\n",
    "    # -------------------------------------------#\n",
    "    # print(num_classes)\n",
    "    assert (\n",
    "        classes_per_user * num_users\n",
    "    ) % num_classes == 0, \"equal classes appearance is needed\"\n",
    "    count_per_class = (classes_per_user * num_users) // num_classes\n",
    "    class_dict = {}\n",
    "    for i in range(num_classes):\n",
    "        probs = np.array([1] * count_per_class)\n",
    "        probs_norm = (probs / probs.sum()).tolist()\n",
    "        class_dict[i] = {\"count\": count_per_class, \"prob\": probs_norm}\n",
    "    # -------------------------------------#\n",
    "    # Assign each client with data indexes #\n",
    "    # -------------------------------------#\n",
    "    class_partitions = defaultdict(list)\n",
    "    for i in range(num_users):\n",
    "        c = []\n",
    "        for _ in range(classes_per_user):\n",
    "            class_counts = [class_dict[i][\"count\"] for i in range(num_classes)]\n",
    "            max_class_counts = np.where(np.array(class_counts) == max(class_counts))[0]\n",
    "            max_class_counts = np.setdiff1d(max_class_counts, np.array(c))\n",
    "            c.append(np.random.choice(max_class_counts))\n",
    "            class_dict[c[-1]][\"count\"] -= 1\n",
    "        class_partitions[\"class\"].append(c)\n",
    "        class_partitions[\"prob\"].append([class_dict[i][\"prob\"].pop() for i in c])\n",
    "    return class_partitions\n",
    "\n",
    "\n",
    "def gen_data_split(dataset, num_users, class_partitions):\n",
    "    \"\"\"\n",
    "    divide data indexes for each client based on class_partition\n",
    "    :param dataset: pytorch dataset object (train/val/test)\n",
    "    :param num_users: number of clients\n",
    "    :param class_partitions: proportion of classes per client\n",
    "    :return: dictionary mapping client to its indexes\n",
    "    \"\"\"\n",
    "    num_classes, num_samples, data_labels_list = get_num_classes_samples(dataset)\n",
    "\n",
    "    # -------------------------- #\n",
    "    # Create class index mapping #\n",
    "    # -------------------------- #\n",
    "    data_class_idx = {i: np.where(data_labels_list == i)[0] for i in range(num_classes)}\n",
    "\n",
    "    # --------- #\n",
    "    # Shuffling #\n",
    "    # --------- #\n",
    "    for data_idx in data_class_idx.values():\n",
    "        random.shuffle(data_idx)\n",
    "\n",
    "    # ------------------------------ #\n",
    "    # Assigning samples to each user #\n",
    "    # ------------------------------ #\n",
    "    user_data_idx = [[] for i in range(num_users)]\n",
    "    for usr_i in range(num_users):\n",
    "        for c, p in zip(\n",
    "            class_partitions[\"class\"][usr_i], class_partitions[\"prob\"][usr_i]\n",
    "        ):\n",
    "            end_idx = int(num_samples[c] * p)\n",
    "            user_data_idx[usr_i].extend(data_class_idx[c][:end_idx])\n",
    "            data_class_idx[c] = data_class_idx[c][end_idx:]\n",
    "        if len(user_data_idx[usr_i]) % 2 == 1:\n",
    "            user_data_idx[usr_i] = user_data_idx[usr_i][:-1]\n",
    "\n",
    "    return user_data_idx\n",
    "\n",
    "\n",
    "def gen_classes_id(num_users=10, num_classes_per_user=2, classes=10):\n",
    "    class_partitions = defaultdict(list)\n",
    "    class_counts = [list(range(classes)) for _ in range(num_classes_per_user)]\n",
    "    user_data_classes = []\n",
    "    for user in range(num_users):\n",
    "        classes_user = np.random.choice(class_counts[0], size=1)\n",
    "        class_counts[0].remove(classes_user[0])\n",
    "        tmp = class_counts[1].copy()\n",
    "        if classes_user[0] in tmp:\n",
    "            tmp.remove(classes_user[0])\n",
    "        if tmp is None:\n",
    "            tmp = [user_data_classes[-1][0]]\n",
    "            user_data_classes[-1][0] = classes_user[0]\n",
    "        classes_user = np.append(classes_user, np.random.choice(tmp, size=1))\n",
    "        class_counts[1].remove(classes_user[1])\n",
    "        user_data_classes.append(classes_user)\n",
    "    for c in user_data_classes:\n",
    "        class_partitions[\"class\"].append(c)\n",
    "        class_partitions[\"prob\"].append([0.5, 0.5])\n",
    "    return class_partitions\n",
    "\n",
    "\n",
    "def gen_classes(num_users=10, num_classes_per_user=6, classes=10):\n",
    "    class_partitions = defaultdict(list)\n",
    "    class_counts = [list(range(classes)) for _ in range(num_classes_per_user)]\n",
    "    user_data_classes = []\n",
    "    for user in range(num_users):\n",
    "        user_data_classes.append(\n",
    "            np.array([*range(user, user + num_classes_per_user)]) % 10\n",
    "        )\n",
    "    for c in user_data_classes:\n",
    "        class_partitions[\"class\"].append(c)\n",
    "        class_partitions[\"prob\"].append(\n",
    "            [1 / num_classes_per_user] * num_classes_per_user\n",
    "        )\n",
    "    return class_partitions\n",
    "\n",
    "\n",
    "def gen_random_loaders(\n",
    "    data_name,\n",
    "    data_path,\n",
    "    num_users,\n",
    "    bz,\n",
    "    num_classes_per_user,\n",
    "    num_classes,\n",
    "    preprocess=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    generates train/val/test loaders of each client\n",
    "    :param data_name: name of dataset, choose from [cifar10, cifar100]\n",
    "    :param data_path: root path for data dir\n",
    "    :param num_users: number of clients\n",
    "    :param bz: batch size\n",
    "    :param classes_per_user: number of classes assigned to each client\n",
    "    :return: train/val/test loaders of each client, list of pytorch dataloaders\n",
    "    \"\"\"\n",
    "    loader_params = {\n",
    "        \"batch_size\": bz,\n",
    "        \"shuffle\": False,\n",
    "        \"pin_memory\": True,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "    dataloaders = []\n",
    "    datasets = get_datasets(data_name, data_path, preprocess=preprocess)\n",
    "    cls_partitions = None\n",
    "    distribution = np.zeros((num_users, num_classes))\n",
    "    for i, d in enumerate(datasets):\n",
    "        if i == 0:\n",
    "            cls_partitions = gen_classes_per_node(d, num_users, num_classes_per_user)\n",
    "            print(\"\\n每个客户端的类别分布:\")\n",
    "            for index in range(num_users):\n",
    "                print(f\"客户端 {index + 1}:\")\n",
    "                for class_idx, prob in zip(\n",
    "                    cls_partitions[\"class\"][index], cls_partitions[\"prob\"][index]\n",
    "                ):\n",
    "                    print(f\"  类别 {class_idx}: 概率 {prob:.4f}\")\n",
    "                distribution[index][cls_partitions[\"class\"][index]] = cls_partitions[\n",
    "                    \"prob\"\n",
    "                ][index]\n",
    "\n",
    "            loader_params[\"shuffle\"] = True\n",
    "        usr_subset_idx = gen_data_split(d, num_users, cls_partitions)\n",
    "\n",
    "        subsets = list(map(lambda x: torch.utils.data.Subset(d, x), usr_subset_idx))\n",
    "        dataloaders.append(\n",
    "            list(\n",
    "                map(lambda x: torch.utils.data.DataLoader(x, **loader_params), subsets)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7d10d-8880-4058-8c2a-3282b6f8613b",
   "metadata": {},
   "source": [
    "## 4.FedUser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d497ad-044e-437a-b3e6-cf9b606f0126",
   "metadata": {},
   "source": [
    "CDPUser 和 LDPUser\n",
    "\n",
    "这两个类代表了PrivateFL中的客户端。它们的主要特点是：\n",
    "\n",
    "- 包含个性化数据转换层（InputNorm），这是PrivateFL的核心创新。\n",
    "- 在训练过程中同时优化数据转换和模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55291a69-49df-4a36-aab4-68816692a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from secretflow import PYUObject, proxy\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torchmetrics\n",
    "import opacus\n",
    "from opacus.validators import ModuleValidator\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# from modelUtil import *\n",
    "\n",
    "\n",
    "@proxy(PYUObject)\n",
    "class CDPUser:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index,\n",
    "        device,\n",
    "        model,\n",
    "        input_shape,\n",
    "        n_classes,\n",
    "        train_dataloader,\n",
    "        epochs,\n",
    "        max_norm=1.0,\n",
    "        disc_lr=5e-3,\n",
    "        flr=1e-1,\n",
    "    ):\n",
    "        print(f\"初始化 CDPUser 参数: index={index}, device={device}, model={model}\")\n",
    "\n",
    "        self.index = index\n",
    "        self.device = device\n",
    "\n",
    "        model_name = model.__name__ if isinstance(model, type) else model\n",
    "        if \"linear_model\" in model_name:\n",
    "            if input_shape == 1024:\n",
    "                self.model = model(\n",
    "                    num_classes=n_classes, input_shape=input_shape, bn_stats=True\n",
    "                )\n",
    "            else:\n",
    "                self.model = model(\n",
    "                    num_classes=n_classes, input_shape=input_shape, bn_stats=False\n",
    "                )\n",
    "        else:\n",
    "            self.model = model(num_classes=n_classes)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.disc_lr = disc_lr\n",
    "        self.acc_metric = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\", num_classes=n_classes\n",
    "        )  # ************to(self.device)\n",
    "        self.max_norm = max_norm\n",
    "        self.epochs = epochs\n",
    "        self.flr = flr\n",
    "        self.agg = True\n",
    "        if \"IN\" in model_name:\n",
    "            self.optim = torch.optim.SGD(\n",
    "                [  # 转换层（self.model.norm）的参数使用了不同的学习率（self.flr），这允许个性化转换层有不同于模型其他部分的优化策略。\n",
    "                    {\"params\": self.model.norm.parameters(), \"lr\": self.flr},\n",
    "                    {\n",
    "                        \"params\": [\n",
    "                            v\n",
    "                            for k, v in self.model.named_parameters()\n",
    "                            if \"norm\" not in k\n",
    "                        ]\n",
    "                    },\n",
    "                ],\n",
    "                lr=self.disc_lr,\n",
    "            )\n",
    "            self.agg = False\n",
    "        else:\n",
    "            self.optim = torch.optim.SGD(self.model.parameters(), self.disc_lr)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        loading = []\n",
    "        for epoch in range(self.epochs):\n",
    "            losses = []\n",
    "            for images, labels in self.train_dataloader:\n",
    "                images, labels = images, labels\n",
    "                loading.append(self.optim.zero_grad())\n",
    "                logits, preds = self.model(images)\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "                loading.append(loss.backward())\n",
    "                loading.append(self.optim.step())\n",
    "                loading.append(self.acc_metric(preds, labels))\n",
    "                losses.append(loss.item())\n",
    "            sf.wait(loading)\n",
    "            logging.info(\n",
    "                f\"Client: {self.index} ACC: {self.acc_metric.compute()}, Loss:{np.mean(losses)}\"\n",
    "            )\n",
    "            self.acc_metric.reset()\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        logging.warning(f\"Client {self.index} start evaluating\")\n",
    "        self.model.eval()\n",
    "        testing_corrects = 0\n",
    "        testing_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in dataloader:\n",
    "                _, preds = self.model(images)\n",
    "                testing_corrects += torch.sum(torch.argmax(preds, dim=1) == labels)\n",
    "                testing_sum += len(labels)\n",
    "        return testing_corrects.cpu().detach().numpy(), testing_sum\n",
    "\n",
    "    def get_model_state_dict(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def set_model_state_dict(self, weights):\n",
    "        if self.agg == False:\n",
    "            for key, value in self.model.state_dict().items():\n",
    "                if \"norm\" not in key and \"bn\" not in key and \"downsample.1\" not in key:\n",
    "                    self.model.state_dict()[key].data.copy_(weights[key])\n",
    "        else:\n",
    "            for key, value in self.model.state_dict().items():\n",
    "                if \"bn\" not in key:\n",
    "                    self.model.state_dict()[key].data.copy_(weights[key])\n",
    "\n",
    "\n",
    "@proxy(PYUObject)\n",
    "class LDPUser(CDPUser):\n",
    "    def __init__(\n",
    "        self,\n",
    "        index,\n",
    "        device,\n",
    "        model,\n",
    "        n_classes,\n",
    "        input_shape,\n",
    "        train_dataloader,\n",
    "        epochs,\n",
    "        rounds,\n",
    "        target_epsilon,\n",
    "        target_delta,\n",
    "        sr,\n",
    "        max_norm=2.0,\n",
    "        disc_lr=5e-1,\n",
    "        mp_bs=3,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            index,\n",
    "            device,\n",
    "            model,\n",
    "            n_classes,\n",
    "            input_shape,\n",
    "            train_dataloader,\n",
    "            epochs=epochs,\n",
    "            max_norm=max_norm,\n",
    "            disc_lr=disc_lr,\n",
    "        )\n",
    "        self.rounds = rounds\n",
    "        self.target_epsilon = target_epsilon\n",
    "        self.epsilon = 0\n",
    "        self.delta = target_delta\n",
    "        self.model = ModuleValidator.fix(self.model)\n",
    "        self.optim = torch.optim.SGD(self.model.parameters(), self.disc_lr)\n",
    "        self.sr = sr\n",
    "        self.make_local_private()\n",
    "        self.agg = True\n",
    "        self.mp_bs = mp_bs\n",
    "\n",
    "        model_name = model.__name__ if isinstance(model, type) else model\n",
    "        if \"IN\" in model_name:\n",
    "            self.agg = False\n",
    "\n",
    "    def make_local_private(self):\n",
    "        self.privacy_engine = opacus.PrivacyEngine()\n",
    "        self.model, self.optim, self.train_dataloader = (\n",
    "            self.privacy_engine.make_private_with_epsilon(\n",
    "                module=self.model,\n",
    "                optimizer=self.optim,\n",
    "                data_loader=self.train_dataloader,\n",
    "                epochs=self.epochs * self.rounds * self.sr,\n",
    "                target_epsilon=self.target_epsilon,\n",
    "                target_delta=self.delta,\n",
    "                max_grad_norm=self.max_norm,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        loading = []\n",
    "        for epoch in range(self.epochs):\n",
    "            with BatchMemoryManager(\n",
    "                data_loader=self.train_dataloader,\n",
    "                max_physical_batch_size=self.mp_bs,\n",
    "                optimizer=self.optim,\n",
    "            ) as batch_loader:\n",
    "                for images, labels in batch_loader:\n",
    "                    images, labels = images, labels\n",
    "                    loading.append(self.optim.zero_grad())\n",
    "                    logits, preds = self.model(images)\n",
    "                    loss = self.loss_fn(logits, labels)\n",
    "                    loading.append(loss.backward())\n",
    "                    loading.append(self.optim.step())\n",
    "                    loading.append(self.acc_metric(preds, labels))\n",
    "        sf.wait(loading)\n",
    "        self.epsilon = self.privacy_engine.get_epsilon(self.delta)\n",
    "        logging.info(\n",
    "            f\"Client: {self.index} ACC: {self.acc_metric.compute()}, episilon: {self.epsilon}\"\n",
    "        )\n",
    "        self.acc_metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f66b25-ce50-49f7-a683-8ba2044b4f0e",
   "metadata": {},
   "source": [
    "## 5.FedServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d791b40e-6304-4561-a5f2-a3d544ba0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import opacus\n",
    "from opacus.validators import ModuleValidator\n",
    "\n",
    "\n",
    "@proxy(PYUObject)\n",
    "class CDPServer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        model,\n",
    "        input_shape,\n",
    "        n_classes,\n",
    "        noise_multiplier=1,\n",
    "        sample_clients=10,\n",
    "        disc_lr=1,\n",
    "    ):\n",
    "        print(f\"初始化 CDPServer 参数: device={device}, model={model}\")\n",
    "        model_name = model.__name__ if isinstance(model, type) else model\n",
    "        if \"linear_model\" in model_name:\n",
    "            self.model = model(num_classes=n_classes, input_shape=input_shape)\n",
    "        else:\n",
    "            self.model = model(num_classes=n_classes)\n",
    "        self.disc_lr = disc_lr\n",
    "        self.device = device\n",
    "        self.sample_clients = sample_clients\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "        self.trainable_names = [k for k, _ in self.model.named_parameters()]\n",
    "        self.agg = True\n",
    "        if \"IN\" in model_name:\n",
    "            self.agg = False\n",
    "\n",
    "    def get_median_norm(self, weights):\n",
    "        logging.warning(\"Calculating median norm\")\n",
    "        median_norm = OrderedDict()\n",
    "        for k, v in self.model.named_parameters():\n",
    "            norms = []\n",
    "            for i in range(len(weights)):\n",
    "                grad = v.detach() - weights[i][k]\n",
    "                norms.append(grad.norm(2))\n",
    "            median_norm[k] = min(median(norms), 10)\n",
    "        return median_norm\n",
    "\n",
    "    def get_model_state_dict(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def agg_updates(self, weights):\n",
    "        logging.warning(\"CDP Server Aggregating updates\")\n",
    "        with torch.no_grad():\n",
    "            norms = self.get_median_norm(weights)\n",
    "            if self.agg == False:\n",
    "                for k, v in self.get_model_state_dict().items():\n",
    "                    if \"bn\" not in k and \"norm\" not in k and \"downsample.1\" not in k:\n",
    "                        sumed_grad = torch.zeros_like(v)\n",
    "                        for i in range(len(weights)):\n",
    "                            grad = weights[i][k] - v\n",
    "                            grad = grad * min(1, norms[k] / grad.norm(2))\n",
    "                            sumed_grad += grad\n",
    "                        sigma = norms[k] * self.noise_multiplier\n",
    "                        sumed_grad += torch.normal(0, sigma, v.shape)\n",
    "                        value = v + sumed_grad / self.sample_clients\n",
    "                        self.model.state_dict()[k].data.copy_(value.detach().clone())\n",
    "            else:\n",
    "                for k, v in self.get_model_state_dict().items():\n",
    "                    if \"bn\" not in k:\n",
    "                        sumed_grad = torch.zeros_like(v)\n",
    "                        for i in range(len(weights)):\n",
    "                            grad = weights[i][k] - v\n",
    "                            grad = grad * min(1, norms[k] / grad.norm(2))\n",
    "                            sumed_grad += grad\n",
    "                        sigma = norms[k] * self.noise_multiplier\n",
    "                        sumed_grad += torch.normal(0, sigma, v.shape)\n",
    "                        value = v + sumed_grad / self.sample_clients\n",
    "                        self.model.state_dict()[k].data.copy_(value.detach().clone())\n",
    "\n",
    "\n",
    "@proxy(PYUObject)\n",
    "class LDPServer(CDPServer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        model,\n",
    "        n_classes,\n",
    "        input_shape,\n",
    "        noise_multiplier=1,\n",
    "        sample_clients=10,\n",
    "        disc_lr=1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            device,\n",
    "            model,\n",
    "            n_classes,\n",
    "            input_shape,\n",
    "            noise_multiplier,\n",
    "            sample_clients,\n",
    "            disc_lr,\n",
    "        )\n",
    "        self.model = ModuleValidator.fix(self.model)\n",
    "        self.privacy_engine = opacus.PrivacyEngine()\n",
    "        self.model = self.privacy_engine._prepare_model(self.model)\n",
    "        model_name = model.__name__ if isinstance(model, type) else model\n",
    "        self.agg = True\n",
    "        if \"IN\" in model_name:\n",
    "            self.agg = False\n",
    "\n",
    "    def agg_updates(self, weights):\n",
    "        logging.warning(\"LDP Server aggregating updates\")\n",
    "        with torch.no_grad():\n",
    "            if self.agg == False:\n",
    "                for k, v in self.get_model_state_dict().items():\n",
    "                    if \"bn\" not in k and \"norm\" not in k and \"downsample.1\" not in k:\n",
    "                        sumed_grad = torch.zeros_like(v)\n",
    "                        for i in range(len(weights)):\n",
    "                            grad = weights[i][k] - v\n",
    "                            sumed_grad += grad\n",
    "                        value = v + sumed_grad / self.sample_clients\n",
    "                        self.model.state_dict()[k].data.copy_(value.detach().clone())\n",
    "            else:\n",
    "                for k, v in self.get_model_state_dict().items():\n",
    "                    if \"bn\" not in k:\n",
    "                        sumed_grad = torch.zeros_like(v)\n",
    "                        for i in range(len(weights)):\n",
    "                            grad = weights[i][k] - v\n",
    "                            sumed_grad += grad\n",
    "                        value = v + sumed_grad / self.sample_clients\n",
    "                        self.model.state_dict()[k].data.copy_(value.detach().clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1bf751-0db2-4db0-87ca-0d2a2ac1193a",
   "metadata": {},
   "source": [
    "## 6.FedAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5234e90a-683a-4777-9e59-046c14e0962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 直接设置参数值\n",
    "args = type(\n",
    "    \"Args\",\n",
    "    (),\n",
    "    {\n",
    "        \"data\": \"mnist\",\n",
    "        \"nclient\": 50,\n",
    "        \"nclass\": 10,\n",
    "        \"ncpc\": 2,\n",
    "        \"model\": \"mnist_fully_connected_IN\",\n",
    "        \"mode\": \"CDP\",\n",
    "        \"round\": 60,\n",
    "        \"epsilon\": 2,\n",
    "        \"physical_bs\": 64,\n",
    "        \"sr\": 1.0,\n",
    "        \"lr\": 5e-3,\n",
    "        \"flr\": 1e-2,\n",
    "        \"E\": 1,\n",
    "    },\n",
    ")()\n",
    "\n",
    "today = date.today().isoformat()\n",
    "DATA_NAME = args.data\n",
    "NUM_CLIENTS = args.nclient\n",
    "NUM_CLASSES = args.nclass\n",
    "NUM_CLASES_PER_CLIENT = args.ncpc\n",
    "MODEL = args.model\n",
    "MODE = args.mode\n",
    "EPOCHS = 1\n",
    "ROUNDS = args.round\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE_DIS = args.lr\n",
    "LEARNING_RATE_F = args.flr\n",
    "mp_bs = args.physical_bs\n",
    "target_epsilon = args.epsilon\n",
    "target_delta = 1e-3\n",
    "sample_rate = args.sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845553f-cfa4-43d7-bde9-58968f1a21e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import secretflow as sf\n",
    "\n",
    "# 初始化 secretflow\n",
    "sf.shutdown()\n",
    "sf.init(\n",
    "    [\"server\"] + [f\"client_{i}\" for i in range(args.nclient)],\n",
    "    address=\"local\",\n",
    "    num_gpus=1,\n",
    ")\n",
    "# 为服务器和每个客户端创建PYU（Party Unit）\n",
    "server_pyu = sf.PYU(\"server\")\n",
    "client_pyus = [sf.PYU(f\"client_{i}\") for i in range(args.nclient)]\n",
    "\n",
    "os.makedirs(f\"log/E{args.E}\", exist_ok=True)\n",
    "user_param = {\"disc_lr\": LEARNING_RATE_DIS, \"epochs\": EPOCHS}\n",
    "server_param = {}\n",
    "if MODE == \"LDP\":\n",
    "    user_obj = LDPUser\n",
    "    server_obj = LDPServer\n",
    "    user_param[\"rounds\"] = ROUNDS\n",
    "    user_param[\"target_epsilon\"] = target_epsilon\n",
    "    user_param[\"target_delta\"] = target_delta\n",
    "    user_param[\"sr\"] = sample_rate\n",
    "    user_param[\"mp_bs\"] = mp_bs\n",
    "elif MODE == \"CDP\":\n",
    "    user_obj = CDPUser\n",
    "    server_obj = CDPServer\n",
    "    user_param[\"flr\"] = LEARNING_RATE_F\n",
    "    server_param[\"noise_multiplier\"] = opacus.accountants.utils.get_noise_multiplier(\n",
    "        target_epsilon=target_epsilon,\n",
    "        target_delta=target_delta,\n",
    "        sample_rate=sample_rate,\n",
    "        steps=ROUNDS,\n",
    "    )\n",
    "    # print(f\"noise_multipier: {server_param['noise_multiplier']}\")\n",
    "    server_param[\"sample_clients\"] = sample_rate * NUM_CLIENTS\n",
    "else:\n",
    "    raise ValueError(\"Choose mode from [CDP, LDP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd875a9e-a01c-4c34-bfda-a24fbbea9df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "每个客户端的类别分布:\n",
      "客户端 1:\n",
      "  类别 0: 概率 0.1000\n",
      "  类别 2: 概率 0.1000\n",
      "客户端 2:\n",
      "  类别 3: 概率 0.1000\n",
      "  类别 1: 概率 0.1000\n",
      "客户端 3:\n",
      "  类别 4: 概率 0.1000\n",
      "  类别 7: 概率 0.1000\n",
      "客户端 4:\n",
      "  类别 5: 概率 0.1000\n",
      "  类别 6: 概率 0.1000\n",
      "客户端 5:\n",
      "  类别 9: 概率 0.1000\n",
      "  类别 8: 概率 0.1000\n",
      "客户端 6:\n",
      "  类别 9: 概率 0.1000\n",
      "  类别 1: 概率 0.1000\n",
      "客户端 7:\n",
      "  类别 4: 概率 0.1000\n",
      "  类别 5: 概率 0.1000\n",
      "客户端 8:\n",
      "  类别 6: 概率 0.1000\n",
      "  类别 0: 概率 0.1000\n",
      "客户端 9:\n",
      "  类别 8: 概率 0.1000\n",
      "  类别 2: 概率 0.1000\n",
      "客户端 10:\n",
      "  类别 7: 概率 0.1000\n",
      "  类别 3: 概率 0.1000\n",
      "客户端 11:\n",
      "  类别 6: 概率 0.1000\n",
      "  类别 9: 概率 0.1000\n",
      "客户端 12:\n",
      "  类别 7: 概率 0.1000\n",
      "  类别 5: 概率 0.1000\n",
      "客户端 13:\n",
      "  类别 0: 概率 0.1000\n",
      "  类别 3: 概率 0.1000\n",
      "客户端 14:\n",
      "  类别 4: 概率 0.1000\n",
      "  类别 8: 概率 0.1000\n",
      "客户端 15:\n",
      "  类别 2: 概率 0.1000\n",
      "  类别 1: 概率 0.1000\n",
      "客户端 16:\n",
      "  类别 2: 概率 0.1000\n",
      "  类别 0: 概率 0.1000\n",
      "客户端 17:\n",
      "  类别 7: 概率 0.1000\n",
      "  类别 8: 概率 0.1000\n",
      "客户端 18:\n",
      "  类别 3: 概率 0.1000\n",
      "  类别 9: 概率 0.1000\n",
      "客户端 19:\n",
      "  类别 1: 概率 0.1000\n",
      "  类别 6: 概率 0.1000\n",
      "客户端 20:\n",
      "  类别 4: 概率 0.1000\n",
      "  类别 5: 概率 0.1000\n",
      "客户端 21:\n",
      "  类别 3: 概率 0.1000\n",
      "  类别 1: 概率 0.1000\n",
      "客户端 22:\n",
      "  类别 4: 概率 0.1000\n",
      "  类别 7: 概率 0.1000\n",
      "客户端 23:\n",
      "  类别 0: 概率 0.1000\n",
      "  类别 9: 概率 0.1000\n",
      "客户端 24:\n",
      "  类别 6: 概率 0.1000\n",
      "  类别 5: 概率 0.1000\n",
      "客户端 25:\n",
      "  类别 2: 概率 0.1000\n",
      "  类别 8: 概率 0.1000\n",
      "客户端 26:\n",
      "  类别 7: 概率 0.1000\n",
      "  类别 5: 概率 0.1000\n",
      "客户端 27:\n",
      "  类别 6: 概率 0.1000\n",
      "  类别 8: 概率 0.1000\n",
      "客户端 28:\n",
      "  类别 0: 概率 0.1000\n",
      "  类别 4: 概率 0.1000\n",
      "客户端 29:\n",
      "  类别 9: 概率 0.1000\n",
      "  类别 3: 概率 0.1000\n",
      "客户端 30:\n",
      "  类别 1: 概率 0.1000\n",
      "  类别 2: 概率 0.1000\n",
      "客户端 31:\n",
      "  类别 4: 概率 0.1000\n",
      "  类别 0: 概率 0.1000\n",
      "客户端 32:\n",
      "  类别 2: 概率 0.1000\n",
      "  类别 7: 概率 0.1000\n",
      "客户端 33:\n",
      "  类别 3: 概率 0.1000\n",
      "  类别 1: 概率 0.1000\n",
      "客户端 34:\n",
      "  类别 6: 概率 0.1000\n",
      "  类别 8: 概率 0.1000\n",
      "客户端 35:\n",
      "  类别 5: 概率 0.1000\n",
      "  类别 9: 概率 0.1000\n",
      "客户端 36:\n",
      "  类别 0: 概率 0.1000\n",
      "  类别 4: 概率 0.1000\n",
      "客户端 37:\n",
      "  类别 2: 概率 0.1000\n",
      "  类别 8: 概率 0.1000\n",
      "客户端 38:\n",
      "  类别 6: 概率 0.1000\n",
      "  类别 5: 概率 0.1000\n",
      "客户端 39:\n",
      "  类别 3: 概率 0.1000\n",
      "  类别 7: 概率 0.1000\n",
      "客户端 40:\n",
      "  类别 9: 概率 0.1000\n",
      "  类别 1: 概率 0.1000\n",
      "客户端 41:\n",
      "  类别 2: 概率 0.1000\n",
      "  类别 1: 概率 0.1000\n",
      "客户端 42:\n",
      "  类别 9: 概率 0.1000\n",
      "  类别 6: 概率 0.1000\n",
      "客户端 43:\n",
      "  类别 3: 概率 0.1000\n",
      "  类别 5: 概率 0.1000\n",
      "客户端 44:\n",
      "  类别 8: 概率 0.1000\n",
      "  类别 7: 概率 0.1000\n",
      "客户端 45:\n",
      "  类别 4: 概率 0.1000\n",
      "  类别 0: 概率 0.1000\n",
      "客户端 46:\n",
      "  类别 1: 概率 0.1000\n",
      "  类别 5: 概率 0.1000\n",
      "客户端 47:\n",
      "  类别 4: 概率 0.1000\n",
      "  类别 0: 概率 0.1000\n",
      "客户端 48:\n",
      "  类别 8: 概率 0.1000\n",
      "  类别 9: 概率 0.1000\n",
      "客户端 49:\n",
      "  类别 2: 概率 0.1000\n",
      "  类别 3: 概率 0.1000\n",
      "客户端 50:\n",
      "  类别 7: 概率 0.1000\n",
      "  类别 6: 概率 0.1000\n"
     ]
    }
   ],
   "source": [
    "if DATA_NAME == \"purchase\":\n",
    "    root = \"data/purchase/dataset_purchase\"\n",
    "elif DATA_NAME == \"chmnist\":\n",
    "    root = \"data/CHMNIST\"\n",
    "else:\n",
    "    root = \"~/torch_data\"\n",
    "\n",
    "train_dataloaders, test_dataloaders = gen_random_loaders(\n",
    "    DATA_NAME, root, NUM_CLIENTS, BATCH_SIZE, NUM_CLASES_PER_CLIENT, NUM_CLASSES\n",
    ")\n",
    "\n",
    "# print(user_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b804f0ba-f644-411a-bba2-92f18ec2c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改：将device参数替换为相应的PYU,在secretflow中，计算设备由PYU表示\n",
    "users = [\n",
    "    user_obj(\n",
    "        i,\n",
    "        client_pyus[i],\n",
    "        device=client_pyus[i],\n",
    "        model=globals()[MODEL],\n",
    "        input_shape=None,\n",
    "        n_classes=NUM_CLASSES,\n",
    "        train_dataloader=train_dataloaders[i],\n",
    "        **user_param\n",
    "    )\n",
    "    for i in range(NUM_CLIENTS)\n",
    "]\n",
    "server = server_obj(\n",
    "    server_pyu,\n",
    "    device=server_pyu,\n",
    "    model=globals()[MODEL],\n",
    "    input_shape=None,\n",
    "    n_classes=NUM_CLASSES,\n",
    "    **server_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4b5ca1f-d9dc-4a41-b470-6c33db0c2851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1\n",
      "全局准确率: 0.0846\n",
      "Round: 2\n",
      "全局准确率: 0.1086\n",
      "Round: 3\n",
      "全局准确率: 0.1934\n",
      "Round: 4\n",
      "全局准确率: 0.1451\n",
      "Round: 5\n",
      "全局准确率: 0.1716\n",
      "Round: 6\n",
      "全局准确率: 0.1297\n",
      "Round: 7\n",
      "全局准确率: 0.1223\n",
      "Round: 8\n",
      "全局准确率: 0.2045\n",
      "Round: 9\n",
      "全局准确率: 0.2635\n",
      "Round: 10\n",
      "全局准确率: 0.3911\n",
      "Round: 11\n",
      "全局准确率: 0.5992\n",
      "Round: 12\n",
      "全局准确率: 0.6683\n",
      "Round: 13\n",
      "全局准确率: 0.8053\n",
      "Round: 14\n",
      "全局准确率: 0.8786\n",
      "Round: 15\n",
      "全局准确率: 0.9037\n",
      "Round: 16\n",
      "全局准确率: 0.9173\n",
      "Round: 17\n",
      "全局准确率: 0.9308\n",
      "Round: 18\n",
      "全局准确率: 0.9382\n",
      "Round: 19\n",
      "全局准确率: 0.9411\n",
      "Round: 20\n",
      "全局准确率: 0.9459\n",
      "Round: 21\n",
      "全局准确率: 0.9485\n",
      "Round: 22\n",
      "全局准确率: 0.9516\n",
      "Round: 23\n",
      "全局准确率: 0.9520\n",
      "Round: 24\n",
      "全局准确率: 0.9550\n",
      "Round: 25\n",
      "全局准确率: 0.9520\n",
      "Round: 26\n",
      "全局准确率: 0.9588\n",
      "Round: 27\n",
      "全局准确率: 0.9607\n",
      "Round: 28\n",
      "全局准确率: 0.9584\n",
      "Round: 29\n",
      "全局准确率: 0.9598\n",
      "Round: 30\n",
      "全局准确率: 0.9646\n",
      "Round: 31\n",
      "全局准确率: 0.9599\n",
      "Round: 32\n",
      "全局准确率: 0.9641\n",
      "Round: 33\n",
      "全局准确率: 0.9648\n",
      "Round: 34\n",
      "全局准确率: 0.9640\n",
      "Round: 35\n",
      "全局准确率: 0.9582\n",
      "Round: 36\n",
      "全局准确率: 0.9650\n",
      "Round: 37\n",
      "全局准确率: 0.9655\n",
      "Round: 38\n",
      "全局准确率: 0.9654\n",
      "Round: 39\n",
      "全局准确率: 0.9649\n",
      "Round: 40\n",
      "全局准确率: 0.9665\n",
      "Round: 41\n",
      "全局准确率: 0.9675\n",
      "Round: 42\n",
      "全局准确率: 0.9645\n",
      "Round: 43\n",
      "全局准确率: 0.9672\n",
      "Round: 44\n",
      "全局准确率: 0.9668\n",
      "Round: 45\n",
      "全局准确率: 0.9693\n",
      "Round: 46\n",
      "全局准确率: 0.9666\n",
      "Round: 47\n",
      "全局准确率: 0.9688\n",
      "Round: 48\n",
      "全局准确率: 0.9685\n",
      "Round: 49\n",
      "全局准确率: 0.9698\n",
      "Round: 50\n",
      "全局准确率: 0.9686\n",
      "Round: 51\n",
      "全局准确率: 0.9683\n",
      "Round: 52\n",
      "全局准确率: 0.9686\n",
      "Round: 53\n",
      "全局准确率: 0.9704\n",
      "Round: 54\n",
      "全局准确率: 0.9700\n",
      "Round: 55\n",
      "全局准确率: 0.9693\n",
      "Round: 56\n",
      "全局准确率: 0.9703\n",
      "Round: 57\n",
      "全局准确率: 0.9709\n",
      "Round: 58\n",
      "全局准确率: 0.9713\n",
      "Round: 59\n",
      "全局准确率: 0.9716\n",
      "Round: 60\n",
      "全局准确率: 0.9701\n",
      "Use time: 0.14h\n",
      "Best accuracy: 0.9716183574879227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ActorCDPUser pid=3029667)\u001b[0m WARNING:root:Client 49 start evaluating\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# def sf_train(clients, server, rounds):\n",
    "# 原有代码: 为所有客户端设置初始模型\n",
    "server_state_dict = server.get_model_state_dict()\n",
    "for i in range(NUM_CLIENTS):\n",
    "    # 使用 SecretFlow 的方法将服务器的状态字典传输到客户端\n",
    "    client_state_dict = server_state_dict.to(users[i].device)\n",
    "    users[i].set_model_state_dict(client_state_dict)\n",
    "best_acc = 0\n",
    "for round in range(ROUNDS):\n",
    "    random_index = np.random.choice(\n",
    "        NUM_CLIENTS, int(sample_rate * NUM_CLIENTS), replace=False\n",
    "    )\n",
    "    for index in random_index:\n",
    "        users[index].train()\n",
    "    if MODE == \"LDP\":\n",
    "        weights_agg = agg_weights(\n",
    "            [users[index].get_model_state_dict() for index in random_index]\n",
    "        )\n",
    "        for i in range(NUM_CLIENTS):\n",
    "            users[i].set_model_state_dict(weights_agg)\n",
    "    else:\n",
    "        server.agg_updates(\n",
    "            [\n",
    "                users[index].get_model_state_dict().to(server.device)\n",
    "                for index in random_index\n",
    "            ]\n",
    "        )\n",
    "        server_state_dict = server.get_model_state_dict()\n",
    "        for i in range(NUM_CLIENTS):\n",
    "            client_state_dict = server_state_dict.to(users[i].device)\n",
    "            users[i].set_model_state_dict(client_state_dict)\n",
    "    print(f\"Round: {round+1}\")\n",
    "    acc = evaluate_global(users, test_dataloaders, range(NUM_CLIENTS))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "    if MODE == \"LDP\":\n",
    "        eps = max([user.epsilon for user in users])\n",
    "        print(f\"Epsilon: {eps}\")\n",
    "        if eps > target_epsilon:\n",
    "            break\n",
    "    # return best_acc\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Use time: {:.2f}h\".format((end_time - start_time) / 3600.0))\n",
    "print(f\"Best accuracy: {best_acc}\")\n",
    "results_df = pd.DataFrame(\n",
    "    columns=[\"data\", \"num_client\", \"ncpc\", \"mode\", \"model\", \"epsilon\", \"accuracy\"]\n",
    ")\n",
    "results_df = results_df._append(\n",
    "    {\n",
    "        \"data\": DATA_NAME,\n",
    "        \"num_client\": NUM_CLIENTS,\n",
    "        \"ncpc\": NUM_CLASES_PER_CLIENT,\n",
    "        \"mode\": MODE,\n",
    "        \"model\": MODEL,\n",
    "        \"epsilon\": target_epsilon,\n",
    "        \"accuracy\": best_acc,\n",
    "    },\n",
    "    ignore_index=True,\n",
    ")\n",
    "results_df.to_csv(\n",
    "    f\"log/E{args.E}/{DATA_NAME}_{NUM_CLIENTS}_{NUM_CLASES_PER_CLIENT}_{MODE}_{MODEL}_{target_epsilon}.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "sf.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privatefl_sf",
   "language": "python",
   "name": "privatefl_sf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
