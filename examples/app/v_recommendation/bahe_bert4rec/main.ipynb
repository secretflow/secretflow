{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433e03fc",
   "metadata": {},
   "source": [
    "#  BAHE+BERT4Rec代码详解\n",
    "1. 数据预处理模块（ dataset.py ）：负责构建双域行为序列数据集\n",
    "2. 模型架构模块（ model.py ）：包含BERT4Rec基础模型和BAHE跨域融合模型\n",
    "3. 训练评估模块（ train.py ）：实现模型训练、验证和评测流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import DualDomainSeqDataset, collate_fn_enhance\n",
    "from model import BERT4Rec, BAHE  # 导入 BAHE 模块\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# 初始化日志\n",
    "def init_logger(log_dir, log_file):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "    file_handler = logging.FileHandler(Path(log_dir) / log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def test(model, bahe_model, val_loader):\n",
    "    model.eval()\n",
    "    bahe_model.eval()\n",
    "    total_loss = 0\n",
    "    preds, labels = [], []\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Testing\"):\n",
    "            # 获取数据\n",
    "            user_node = batch['user_node'].long()\n",
    "            item_node = batch['i_node'].long()\n",
    "            seq_d1 = batch['seq_d1'].long()\n",
    "            seq_d2 = batch['seq_d2'].long()\n",
    "            domain_id = batch['domain_id'].long()\n",
    "            label = batch['label'].float()  # [batch_size]\n",
    "            behavior_texts = batch['behavior_texts']\n",
    "\n",
    "            # 获取用户嵌入\n",
    "            user_embedding = bahe_model(behavior_texts)\n",
    "\n",
    "            # 模型输出\n",
    "            # output_d1, output_d2 = model(user_embedding, item_node, seq_d1, seq_d2, domain_id)\n",
    "\n",
    "            # # 分别根据 domain_id 决定用哪个输出\n",
    "            # # domain_id == 0 → 用 output_d1\n",
    "            # # domain_id == 1 → 用 output_d2\n",
    "            # mask_d1 = (domain_id == 0).float()\n",
    "            # mask_d2 = (domain_id == 1).float()\n",
    "\n",
    "            # # 输出 reshape，确保和 label 对齐\n",
    "            # output_d1 = output_d1.view(-1)\n",
    "            # output_d2 = output_d2.view(-1)\n",
    "\n",
    "            # # 按域选择正确输出参与损失\n",
    "            # selected_output = output_d1 * mask_d1 + output_d2 * mask_d2\n",
    "            output = model(user_embedding, item_node, seq_d1, seq_d2, domain_id)\n",
    "            output = output.view(-1)  # 进行必要的形状调整\n",
    "            loss = criterion(output, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 收集预测结果用于 AUC\n",
    "            preds.extend(output.cpu().numpy())\n",
    "            labels.extend(label.cpu().numpy())\n",
    "\n",
    "    # 计算平均损失和 AUC\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "\n",
    "    return avg_loss, auc\n",
    "\n",
    "\n",
    "# 训练函数\n",
    "def train(model, bahe_model, train_loader, val_loader, optimizer, args):\n",
    "    logger = init_logger(args.model_dir, args.log_file)\n",
    "    best_auc = 0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        bahe_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(\n",
    "            tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{args.epochs}\")\n",
    "        ):\n",
    "            # 获取数据\n",
    "            # 获取数据并确保类型正确\n",
    "            user_node = batch['user_node'].long()\n",
    "            item_node = batch['i_node'].long()\n",
    "            seq_d1 = batch['seq_d1'].long()\n",
    "            seq_d2 = batch['seq_d2'].long()\n",
    "            domain_id = batch['domain_id'].long()\n",
    "            label = batch['label'].float()\n",
    "            behavior_texts = batch['behavior_texts']\n",
    "            # 打印维度信息（调试用）\n",
    "            print(f\"Sequence shapes - seq_d1: {seq_d1.shape}, seq_d2: {seq_d2.shape}\")\n",
    "            print(f\"Item node shape: {item_node.shape}\")\n",
    "            print(f\"Domain ID shape: {domain_id.shape}\")\n",
    "            # 使用较小的批次处理行为文本\n",
    "            user_embedding = bahe_model(behavior_texts)\n",
    "\n",
    "            # 模型预测\n",
    "            optimizer.zero_grad()\n",
    "            # output_d1, output_d2 = model(user_embedding, item_node, seq_d1, seq_d2, domain_id)\n",
    "            output = model(user_embedding, item_node, seq_d1, seq_d2, domain_id)\n",
    "\n",
    "            output = output.view(-1)  # 进行必要的形状调整\n",
    "            # output_d1 = output_d1.view(-1)\n",
    "            # output_d2 = output_d2.view(-1)\n",
    "            # mask_d1 = (domain_id == 0).float()\n",
    "            # mask_d2 = (domain_id == 1).float()\n",
    "            # selected_output = output_d1 * mask_d1 + output_d2 * mask_d2\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 定期清理内存\n",
    "            if batch_idx % 10 == 0:\n",
    "                torch.cuda.empty_cache()  # 即使在CPU上运行也是安全的\n",
    "\n",
    "        # 每个epoch结束后验证\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        val_loss, val_auc = test(model, bahe_model, val_loader)\n",
    "\n",
    "        # 记录日志\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch + 1}/{args.epochs} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            torch.save(\n",
    "                {\n",
    "                    'bert4rec_state_dict': model.state_dict(),\n",
    "                    'bahe_state_dict': bahe_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'best_auc': best_auc,\n",
    "                },\n",
    "                Path(args.model_dir) / \"best_model.pt\",\n",
    "            )\n",
    "\n",
    "    logger.info(f\"Training finished. Best AUC: {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"BERT4Rec Training\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=256, help=\"Batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=5e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--emb_dim\", type=int, default=128, help=\"Embedding dimension\")\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=20, help=\"Sequence length\")\n",
    "    parser.add_argument(\n",
    "        \"--model_dir\", type=str, default=\"model\", help=\"Directory to save models\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_file\", type=str, default=\"train.log\", help=\"Log file name\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset_path\", type=str, default=\"amazon_dataset\", help=\"Path to dataset\"\n",
    "    )\n",
    "    parser.add_argument('-ds', '--dataset_type', type=str, default='amazon')\n",
    "    parser.add_argument('-dm', '--domain_type', type=str, default='cloth_sport')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--long_length',\n",
    "        type=int,\n",
    "        default=7,\n",
    "        help='the length for setting long-tail node',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--neg_nums', type=int, default=199, help='sample negative numbers'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--overlap_ratio',\n",
    "        type=float,\n",
    "        default=0.25,\n",
    "        help='overlap ratio for choose dataset ',\n",
    "    )\n",
    "    # overlap_ratio = 0.25 表示25%的用户是跨域用户（即在两个域都有行为的用户）\n",
    "    parser.add_argument('--epoch', type=int, default=50, help='# of epoch')\n",
    "    parser.add_argument('--bs', type=int, default=256, help='# images in batch')\n",
    "    parser.add_argument('--hid_dim', type=int, default=32, help='hidden layer dim')\n",
    "    parser.add_argument('--isInC', type=bool, default=False, help='add inc ')\n",
    "    parser.add_argument('--isItC', type=bool, default=False, help='add itc')\n",
    "    parser.add_argument('--ts1', type=float, default=0.5, help='mask rate for encoder')\n",
    "    parser.add_argument('--ts2', type=float, default=0.5, help='mask rate for decoder')\n",
    "    args = parser.parse_args()\n",
    "    user_length = 895510  # 63275#6814 cdr23 #63275 cdr12\n",
    "    item_length_d1 = 8240\n",
    "    item_length_d2 = 26272\n",
    "    item_length = 447410  # item_length_d1 + item_length_d2 + 1 + 20000#1739+2 #13713 cdr23 #1739 + 2#item_length_d1 + item_length_d2 + 1 + 20000#1739 + 1 +200 # 1 = pad item #item_length_d1 + item_length_d2 + 1 + 20000\n",
    "\n",
    "    # 设备设置\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    # 加载数据集\n",
    "\n",
    "    datasetTrain = DualDomainSeqDataset(\n",
    "        seq_len=args.seq_len,\n",
    "        isTrain=True,\n",
    "        neg_nums=args.neg_nums,\n",
    "        long_length=args.long_length,\n",
    "        pad_id=item_length - 1,\n",
    "        csv_path=f\"{args.dataset_type}_dataset/{args.domain_type}_train{int(args.overlap_ratio*100)}.csv\",\n",
    "    )\n",
    "    trainLoader = DataLoader(\n",
    "        datasetTrain,\n",
    "        batch_size=32,  # 使用更小的批次大小\n",
    "        shuffle=True,\n",
    "        num_workers=4,  # 减少工作进程数\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn_enhance,\n",
    "    )\n",
    "\n",
    "    datasetVal = DualDomainSeqDataset(\n",
    "        seq_len=args.seq_len,\n",
    "        isTrain=False,\n",
    "        neg_nums=args.neg_nums,\n",
    "        long_length=args.long_length,\n",
    "        pad_id=item_length - 1,\n",
    "        csv_path=f\"{args.dataset_type}_dataset/{args.domain_type}_test.csv\",\n",
    "    )\n",
    "    valLoader = DataLoader(\n",
    "        datasetVal,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn_enhance,\n",
    "    )\n",
    "\n",
    "    # 初始化模型\n",
    "    bahe_model = BAHE(\n",
    "        albert_model_name='albert-base-v2',\n",
    "        embed_dim=args.emb_dim,\n",
    "        num_heads=4,\n",
    "        ff_dim=512,\n",
    "        num_layers=2,\n",
    "    )\n",
    "\n",
    "    model = BERT4Rec(\n",
    "        user_length=user_length,\n",
    "        user_emb_dim=args.emb_dim,\n",
    "        item_length=item_length,\n",
    "        item_emb_dim=args.emb_dim,\n",
    "        seq_len=args.seq_len,\n",
    "        hid_dim=args.hid_dim,\n",
    "        bs=32,  # 更新批次大小\n",
    "        isInC=args.isInC,\n",
    "        isItC=args.isItC,\n",
    "        threshold1=args.ts1,\n",
    "        threshold2=args.ts2,\n",
    "    )\n",
    "\n",
    "    # 优化器\n",
    "    optimizer = optim.Adam(\n",
    "        list(model.parameters()) + list(bahe_model.parameters()), lr=args.lr\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    train(model, bahe_model, trainLoader, valLoader, optimizer, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739e932",
   "metadata": {},
   "source": [
    "# secretflow框架下：BAHE+BERT4Rec代码详解\n",
    "1. 数据预处理模块（ dataset.py ）：负责构建双域行为序列数据集\n",
    "2. 模型架构模块（ model.py ）：包含BERT4Rec基础模型（拆分BERT4Rec为编码器和融合器）和\n",
    "BAHE跨域融合模型\n",
    "3. 训练评估模块（ train.py ）：实现模型训练、验证和评测流程\n",
    "\n",
    "数据分布:\n",
    "Bob端：持有域1数据（如电子产品行为序列）。\n",
    "Alice端：持有域2数据（如服装行为序列）。\n",
    "Server端：协调训练，聚合模型参数。\n",
    "主要拆分思路：将BERT4Rec拆分为编码器和融合器，在客户端处理本地数据和行为序列，在服务\n",
    "器端进行模型融合和预测，使用SecretFlow的PYU设备包装模型和数据，最终实现分布式训练流\n",
    "程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sf_dataset import DualDomainSeqDataset, collate_fn_enhance\n",
    "from sf_model import BERT4Rec, BAHE, BERT4RecEncoder, BERT4RecFusion\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import argparse\n",
    "import secretflow as sf\n",
    "from secretflow import PYUObject, proxy\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@proxy(PYUObject)\n",
    "class Client:\n",
    "    def __init__(\n",
    "        self, bahe_model, bert4rec_model, config, client_id, num_clients, device\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.bahe_model = device(bahe_model)\n",
    "        self.bert4rec_model = device(bert4rec_model)\n",
    "        self.client_id = device(client_id)\n",
    "        self.config = config\n",
    "\n",
    "    def _train_single_batch(self, batch):\n",
    "        # 处理本地数据\n",
    "        behavior_texts = batch['behavior_texts']\n",
    "        seq = batch['seq_d1'] if self.client_id == 0 else batch['seq_d2']\n",
    "        domain_id = batch['domain_id']\n",
    "\n",
    "        # 使用BAHE生成用户嵌入\n",
    "        user_embedding = self.bahe_model(behavior_texts)\n",
    "\n",
    "        # 直接使用forward方法而不是process_sequence\n",
    "        seq_embedding = self.bert4rec_model(\n",
    "            batch['i_node'], seq, domain_id, user_embedding\n",
    "        )\n",
    "\n",
    "        return {'user_embedding': user_embedding, 'seq_embedding': seq_embedding}\n",
    "\n",
    "    def update_gradients(self, loss):\n",
    "        loss = self.device(loss)\n",
    "        self.bahe_model.zero_grad()\n",
    "        self.bert4rec_model.zero_grad()\n",
    "        loss.backward()\n",
    "        return True\n",
    "\n",
    "\n",
    "@proxy(PYUObject)\n",
    "class Server:\n",
    "    def __init__(self, model, config, device):\n",
    "        self.device = device\n",
    "        self._model = device(model)\n",
    "        self.config = config\n",
    "\n",
    "    def _train_single_batch(self, client_outputs, batch):\n",
    "        # 获取客户端输出\n",
    "        user_embedding_d1 = client_outputs[0]['user_embedding']\n",
    "        user_embedding_d2 = client_outputs[1]['user_embedding']\n",
    "        seq_embedding_d1 = client_outputs[0]['seq_embedding']\n",
    "        seq_embedding_d2 = client_outputs[1]['seq_embedding']\n",
    "\n",
    "        # 使用forward方法而不是fusion_layer\n",
    "        output = self._model(\n",
    "            user_embedding_d1,\n",
    "            user_embedding_d2,\n",
    "            seq_embedding_d1,\n",
    "            seq_embedding_d2,\n",
    "            batch['domain_id'],\n",
    "        )\n",
    "\n",
    "        # 计算损失\n",
    "        loss = nn.BCEWithLogitsLoss()(output, batch['label'])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83933957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sf_train(clients, server, epochs, train_dataset_params, batch_size):\n",
    "    # 为alice和bob创建数据加载器\n",
    "    alice_data = DualDomainSeqDataset(\n",
    "        seq_len=train_dataset_params['seq_len'],\n",
    "        isTrain=True,\n",
    "        neg_nums=train_dataset_params['neg_nums'],\n",
    "        long_length=train_dataset_params['long_length'],\n",
    "        pad_id=train_dataset_params['pad_id'],\n",
    "        csv_path=f\"{train_dataset_params['dataset_type']}_dataset/{train_dataset_params['domain_type']}_train{int(train_dataset_params['overlap_ratio']*100)}.csv\",\n",
    "        domain_id=1,  # Alice负责域2\n",
    "    )\n",
    "\n",
    "    bob_data = DualDomainSeqDataset(\n",
    "        seq_len=train_dataset_params['seq_len'],\n",
    "        isTrain=True,\n",
    "        neg_nums=train_dataset_params['neg_nums'],\n",
    "        long_length=train_dataset_params['long_length'],\n",
    "        pad_id=train_dataset_params['pad_id'],\n",
    "        csv_path=f\"{train_dataset_params['dataset_type']}_dataset/{train_dataset_params['domain_type']}_train{int(train_dataset_params['overlap_ratio']*100)}.csv\",\n",
    "        domain_id=0,  # Bob负责域1\n",
    "    )\n",
    "\n",
    "    alice_loader = DataLoader(\n",
    "        alice_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn_enhance,\n",
    "    )\n",
    "\n",
    "    bob_loader = DataLoader(\n",
    "        bob_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn_enhance,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_id, (alice_batch, bob_batch) in enumerate(\n",
    "            zip(alice_loader, bob_loader)\n",
    "        ):\n",
    "            # 获取客户端outputs\n",
    "            client_outputs = []\n",
    "            for client_id, (client, batch) in enumerate(\n",
    "                zip(clients, [bob_batch, alice_batch])\n",
    "            ):\n",
    "                output = client._train_single_batch(batch)\n",
    "                client_outputs.append(output.to(server.device))\n",
    "\n",
    "            # 等待客户端计算完成\n",
    "            sf.wait(client_outputs)\n",
    "\n",
    "            # 准备服务器端需要的数据\n",
    "            labels = {\n",
    "                'label': server.device(bob_batch['label'].float()),\n",
    "                'domain_id': server.device(bob_batch['domain_id']),\n",
    "            }\n",
    "\n",
    "            # 服务器端计算损失\n",
    "            loss_pyu = server._train_single_batch(client_outputs, labels)\n",
    "            loss = sf.reveal(loss_pyu)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_id % 10 == 0:\n",
    "                logging.warning(\n",
    "                    f'[Training Epoch: {epoch}] Batch: {batch_id}, Loss: {loss}'\n",
    "                )\n",
    "\n",
    "            # 更新客户端梯度\n",
    "            updates = []\n",
    "            for client in clients:\n",
    "                ret = client.update_gradients(loss)  # 注意这里使用 loss_pyu\n",
    "                updates.append(ret)\n",
    "\n",
    "            sf.wait(updates)\n",
    "\n",
    "        logging.warning(f\"Training Epoch: {epoch}, total loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c07d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    sf.init([\"alice\", \"bob\", \"server\"], address='local', debug_mode=False)\n",
    "    alice_pyu = sf.PYU(\"alice\")\n",
    "    bob_pyu = sf.PYU(\"bob\")\n",
    "    server_pyu = sf.PYU(\"server\")\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"BERT4Rec Training\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=256, help=\"Batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=5e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--emb_dim\", type=int, default=128, help=\"Embedding dimension\")\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=20, help=\"Sequence length\")\n",
    "    parser.add_argument(\n",
    "        \"--model_dir\", type=str, default=\"model\", help=\"Directory to save models\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_file\", type=str, default=\"train.log\", help=\"Log file name\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset_path\", type=str, default=\"amazon_dataset\", help=\"Path to dataset\"\n",
    "    )\n",
    "    parser.add_argument('-ds', '--dataset_type', type=str, default='amazon')\n",
    "    parser.add_argument('-dm', '--domain_type', type=str, default='cloth_sport')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--long_length',\n",
    "        type=int,\n",
    "        default=7,\n",
    "        help='the length for setting long-tail node',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--neg_nums', type=int, default=199, help='sample negative numbers'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--overlap_ratio',\n",
    "        type=float,\n",
    "        default=0.25,\n",
    "        help='overlap ratio for choose dataset ',\n",
    "    )\n",
    "    # overlap_ratio = 0.25 表示25%的用户是跨域用户（即在两个域都有行为的用户）\n",
    "    parser.add_argument('--epoch', type=int, default=50, help='# of epoch')\n",
    "    parser.add_argument('--bs', type=int, default=256, help='# images in batch')\n",
    "    parser.add_argument('--hid_dim', type=int, default=32, help='hidden layer dim')\n",
    "    parser.add_argument('--isInC', type=bool, default=False, help='add inc ')\n",
    "    parser.add_argument('--isItC', type=bool, default=False, help='add itc')\n",
    "    parser.add_argument('--ts1', type=float, default=0.5, help='mask rate for encoder')\n",
    "    parser.add_argument('--ts2', type=float, default=0.5, help='mask rate for decoder')\n",
    "    args = parser.parse_args()\n",
    "    user_length = 895510  # 63275#6814 cdr23 #63275 cdr12\n",
    "    item_length_d1 = 8240\n",
    "    item_length_d2 = 26272\n",
    "    item_length = 447410  # item_length_d1 + item_length_d2 + 1 + 20000#1739+2 #13713 cdr23 #1739 + 2#item_length_d1 + item_length_d2 + 1 + 20000#1739 + 1 +200 # 1 = pad item #item_length_d1 + item_length_d2 + 1 + 20000\n",
    "\n",
    "    # 创建客户端模型\n",
    "    bahe_model = BAHE(\n",
    "        albert_model_name='albert/albert-base-v2',\n",
    "        embed_dim=args.emb_dim,\n",
    "        num_heads=4,\n",
    "        ff_dim=512,\n",
    "        num_layers=2,\n",
    "    )\n",
    "\n",
    "    bert4rec_encoder = BERT4RecEncoder(  # 需要新建一个BERT4RecEncoder类\n",
    "        user_length=user_length,\n",
    "        user_emb_dim=args.emb_dim,\n",
    "        item_length=item_length,\n",
    "        item_emb_dim=args.emb_dim,\n",
    "        seq_len=args.seq_len,\n",
    "        hid_dim=args.hid_dim,\n",
    "    )\n",
    "\n",
    "    # 创建服务器模型\n",
    "    fusion_model = BERT4RecFusion(  # 需要新建一个BERT4RecFusion类\n",
    "        embed_dim=args.emb_dim, num_heads=4, ff_dim=512\n",
    "    )\n",
    "\n",
    "    # 创建客户端和服务器\n",
    "    clients = [\n",
    "        Client(bahe_model, bert4rec_encoder, args, 0, 2, device=bob_pyu),  # Bob处理域1\n",
    "        Client(\n",
    "            bahe_model, bert4rec_encoder, args, 1, 2, device=alice_pyu\n",
    "        ),  # Alice处理域2\n",
    "    ]\n",
    "\n",
    "    server = Server(fusion_model, args, device=server_pyu)\n",
    "\n",
    "    # 训练参数\n",
    "    train_dataset_params = {\n",
    "        'seq_len': args.seq_len,\n",
    "        'neg_nums': args.neg_nums,\n",
    "        'long_length': args.long_length,\n",
    "        'pad_id': item_length - 1,\n",
    "        'dataset_type': args.dataset_type,\n",
    "        'domain_type': args.domain_type,\n",
    "        'overlap_ratio': args.overlap_ratio,\n",
    "    }\n",
    "\n",
    "    # 开始训练\n",
    "    sf_train(\n",
    "        clients,\n",
    "        server,\n",
    "        epochs=50,\n",
    "        train_dataset_params=train_dataset_params,\n",
    "        batch_size=32,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
