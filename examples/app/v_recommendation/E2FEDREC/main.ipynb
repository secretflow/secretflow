{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce72a7ae",
   "metadata": {},
   "source": [
    "# 项目概览：基于双重增强的跨域推荐优化方法（E2FedRec）\n",
    "\n",
    "本项目针对分布式搜广推模型中由于训练数据受限导致性能受限的问题，提出了一种创新的跨域推荐优化方法——E2FedRec。方法以垂直联邦学习为框架，结合域内与域间对比学习，提升推荐模型的准确性与泛化能力。\n",
    "\n",
    "## 核心创新点\n",
    "\n",
    "- **双重增强设计**：通过域内增强（Intra-domain Contrastive Learning）与域间增强（Inter-domain Contrastive Learning）联合优化用户与物品嵌入表示。\n",
    "- **高效利用非交集数据**：通过辅助源域数据扩充有效样本，提高目标域推荐效果。\n",
    "- **差异性建模与迁移**：从跨域数据分布差异出发，利用对比学习优势，实现精准的表征优化和知识迁移。\n",
    "\n",
    "## 技术框架\n",
    "\n",
    "1. **域内对比学习模块**：\n",
    "   - 对用户和物品嵌入进行聚类，生成原型表示。\n",
    "   - 采用正负样本对比，强化类内紧密性与类间差异性。\n",
    "   - 引入温度参数与轮廓系数辅助稳定训练。\n",
    "\n",
    "2. **域间对比学习模块**：\n",
    "   - 基于重叠用户构建跨域原型。\n",
    "   - 通过正负样本精细构造，提升域间嵌入一致性，减少数据分布差异的影响。\n",
    "\n",
    "3. **多任务联合优化**：\n",
    "   - 主监督损失（推荐目标）与辅助对比损失（嵌入增强）联合训练。\n",
    "   - 调整各损失权重，确保训练稳定且高效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f413562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import secretflow as sf\n",
    "\n",
    "from E2FEDREC import Model\n",
    "from trainer import Trainer\n",
    "from server import Server\n",
    "from secretflow import PYUObject, proxy\n",
    "\n",
    "\n",
    "def get_args(K_size):\n",
    "    \"\"\"\n",
    "    Parse and return command-line arguments for training configuration.\n",
    "\n",
    "    Args:\n",
    "        K_size (int): Initial embedding dimension size\n",
    "\n",
    "    Returns:\n",
    "        argparse.Namespace: Parsed training arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Cross-Domain Recommendation System Options\"\n",
    "    )\n",
    "\n",
    "    # Training hyperparameters\n",
    "    parser.add_argument(\n",
    "        \"-negNum\",\n",
    "        default=7,\n",
    "        type=int,\n",
    "        help=\"Number of negative samples per positive sample\",\n",
    "    )\n",
    "    parser.add_argument(\"-lr\", default=0.001, type=float, help=\"Learning rate\")\n",
    "    parser.add_argument(\n",
    "        \"-maxEpochs\", default=5, type=int, help=\"Maximum number of training epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-batchSize\", default=512, type=int, help=\"Mini-batch size for training\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-earlyStop\", default=5, type=int, help=\"Early stopping patience\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-checkPoint\", default=\"./checkPoint/\", help=\"Directory to save checkpoints\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-topK\", default=10, type=int, help=\"Top-K value for evaluation metrics\"\n",
    "    )\n",
    "\n",
    "    # Model architecture configuration\n",
    "    parser.add_argument(\n",
    "        \"-userLayer\",\n",
    "        default=[K_size, 2 * K_size, K_size],\n",
    "        help=\"Sizes of user encoder layers\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-itemLayer\",\n",
    "        default=[K_size, 2 * K_size, K_size],\n",
    "        help=\"Sizes of item encoder layers\",\n",
    "    )\n",
    "    parser.add_argument(\"-KSize\", default=K_size, help=\"Embedding dimension size\")\n",
    "\n",
    "    # Regularization settings\n",
    "    parser.add_argument(\n",
    "        \"-reg\", default=1e-3, type=float, help=\"L2 regularization coefficient\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-lambdad\", default=0.001, type=float, help=\"Weight for domain alignment loss\"\n",
    "    )\n",
    "\n",
    "    # Contrastive learning settings\n",
    "    parser.add_argument(\n",
    "        \"-ssl_temp\", default=1, type=float, help=\"Temperature for contrastive loss\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-ssl_reg_intra\",\n",
    "        default=0.3,\n",
    "        type=float,\n",
    "        help=\"Weight for intra-domain contrastive loss\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-ssl_reg_inter\",\n",
    "        default=0.2,\n",
    "        type=float,\n",
    "        help=\"Weight for inter-domain contrastive loss\",\n",
    "    )\n",
    "\n",
    "    # Empty list to prevent CLI parsing conflict when called from code\n",
    "    return parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bfc889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(records, server, max_epochs):\n",
    "    \"\"\"\n",
    "    Train models across domains for a given number of epochs.\n",
    "\n",
    "    Args:\n",
    "        records (dict): Dictionary storing trainer instances and metrics\n",
    "        server (Server): Server instance for aggregation\n",
    "        max_epochs (int): Maximum number of training epochs\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Starting epoch training...\")\n",
    "\n",
    "    dataset_names = list(records.keys())\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\n[INFO] === Epoch {epoch} ===\")\n",
    "\n",
    "        # Step 1: Collect representations from each trainer\n",
    "        trainer_weights = []\n",
    "        for dataset_name in dataset_names:\n",
    "            trainer = records[dataset_name][\"trainer\"]\n",
    "            weights = trainer.get_reps_shared()\n",
    "            if weights is None:\n",
    "                raise ValueError(\n",
    "                    f\"[ERROR] {dataset_name}: get_reps_shared() returned None.\"\n",
    "                )\n",
    "            trainer_weights.append(weights.to(server.device))\n",
    "\n",
    "        # Step 2: Aggregate representations at the server\n",
    "        global_weights = server.aggregate_reps(trainer_weights)\n",
    "\n",
    "        # Step 3: Distribute global representations back to trainers\n",
    "        setting = []\n",
    "        for dataset_name in dataset_names:\n",
    "            trainer = records[dataset_name][\"trainer\"]\n",
    "            ret = trainer.set_global_reps(global_weights.to(trainer.device))\n",
    "            setting.append(ret)\n",
    "\n",
    "        sf.wait(setting)  # Ensure all trainers have synchronized\n",
    "\n",
    "        # Step 4: Train and evaluate\n",
    "        for dataset_name in dataset_names:\n",
    "            trainer = records[dataset_name][\"trainer\"]\n",
    "            print(f\"[INFO] Training for {dataset_name}...\")\n",
    "            loss, hr, ndcg = sf.reveal(trainer.run_one_epoch(epoch))\n",
    "\n",
    "            records[dataset_name][\"loss_list\"].append(loss)\n",
    "            records[dataset_name][\"hr_list\"].append(hr)\n",
    "            records[dataset_name][\"NDCG_list\"].append(ndcg)\n",
    "\n",
    "            if hr > records[dataset_name][\"best_hr\"]:\n",
    "                records[dataset_name][\"best_hr\"] = hr\n",
    "                records[dataset_name][\"best_NDCG\"] = ndcg\n",
    "\n",
    "            print(\n",
    "                f\"[RESULT] {dataset_name} | Epoch {epoch} | Loss: {loss:.4f}, HR@10: {hr:.4f}, NDCG@10: {ndcg:.4f}\"\n",
    "            )\n",
    "\n",
    "    print(\"[INFO] Epoch training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "849946ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataName_A, dataName_B, K_size):\n",
    "    \"\"\"\n",
    "    Main entry point for running cross-domain federated training.\n",
    "\n",
    "    Args:\n",
    "        dataName_A (str): Name of source domain dataset\n",
    "        dataName_B (str): Name of target domain dataset\n",
    "        K_size (int): Embedding dimension size\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"[INFO] Starting training with:\\n  - Domain A: {dataName_A}\\n  - Domain B: {dataName_B}\\n  - K size: {K_size}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # === Setup section ===\n",
    "        args = get_args(K_size)\n",
    "        np.random.seed(42)\n",
    "\n",
    "        sf.shutdown()  # Clean up any previous session (optional)\n",
    "        sf.init([dataName_A, dataName_B, \"server\"], address=\"local\", num_gpus=1)\n",
    "        print(\"[INFO] SecretFlow initialized successfully.\")\n",
    "\n",
    "        dataName_A_pyu = sf.PYU(dataName_A)\n",
    "        dataName_B_pyu = sf.PYU(dataName_B)\n",
    "        server_pyu = sf.PYU(\"server\")\n",
    "\n",
    "        # Create trainers and server\n",
    "        trainer_A = Trainer(args, dataName_A, dataName_B, 0, device=dataName_A_pyu)\n",
    "        trainer_B = Trainer(args, dataName_B, dataName_A, 0, device=dataName_B_pyu)\n",
    "        server = Server(args, [dataName_A, dataName_B], device=server_pyu)\n",
    "\n",
    "        # Initialize metrics recorder\n",
    "        records = {\n",
    "            dataName_A: {\n",
    "                \"trainer\": trainer_A,\n",
    "                \"loss_list\": [],\n",
    "                \"hr_list\": [],\n",
    "                \"NDCG_list\": [],\n",
    "                \"best_hr\": -1,\n",
    "                \"best_NDCG\": -1,\n",
    "            },\n",
    "            dataName_B: {\n",
    "                \"trainer\": trainer_B,\n",
    "                \"loss_list\": [],\n",
    "                \"hr_list\": [],\n",
    "                \"NDCG_list\": [],\n",
    "                \"best_hr\": -1,\n",
    "                \"best_NDCG\": -1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # === Training section ===\n",
    "        max_epochs = args.maxEpochs\n",
    "        train_epochs(records, server, max_epochs)\n",
    "\n",
    "        # Save results\n",
    "        for dataset_name in [dataName_A, dataName_B]:\n",
    "            trainer = records[dataset_name][\"trainer\"]\n",
    "            matname = f\"E2FEDREC_{dataset_name}_KSize_{K_size}_Result.pt\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"loss_list\": records[dataset_name][\"loss_list\"],\n",
    "                    \"hr_list\": records[dataset_name][\"hr_list\"],\n",
    "                    \"NDCG_list\": records[dataset_name][\"NDCG_list\"],\n",
    "                    \"bestPerformance\": [\n",
    "                        records[dataset_name][\"best_hr\"],\n",
    "                        records[dataset_name][\"best_NDCG\"],\n",
    "                    ],\n",
    "                },\n",
    "                matname,\n",
    "            )\n",
    "            print(f\"[INFO] Results for {dataset_name} saved to {matname}\")\n",
    "\n",
    "        print(\"[INFO] Training completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Exception occurred during training:\")\n",
    "        print(e)\n",
    "\n",
    "    print(\"[INFO] Program execution finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b6f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting training with:\n",
      "  - Domain A: book\n",
      "  - Domain B: movie\n",
      "  - K size: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 19:12:47,854\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "INFO:root:Create proxy actor <class 'trainer.Trainer'> with party book.\n",
      "INFO:root:Create proxy actor <class 'trainer.Trainer'> with party movie.\n",
      "INFO:root:Create proxy actor <class 'server.Server'> with party server.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SecretFlow initialized successfully.\n",
      "[INFO] Starting epoch training...\n",
      "\n",
      "[INFO] === Epoch 0 ===\n",
      "[INFO] Training for book...\n",
      "[RESULT] book | Epoch 0 | Loss: 1344.8946, HR@10: 0.2602, NDCG@10: 0.1322\n",
      "[INFO] Training for movie...\n",
      "[RESULT] movie | Epoch 0 | Loss: 1322.5166, HR@10: 0.3758, NDCG@10: 0.2146\n",
      "\n",
      "[INFO] === Epoch 1 ===\n",
      "[INFO] Training for book...\n",
      "[RESULT] book | Epoch 1 | Loss: 1323.7429, HR@10: 0.2614, NDCG@10: 0.1314\n",
      "[INFO] Training for movie...\n",
      "[RESULT] movie | Epoch 1 | Loss: 1305.1205, HR@10: 0.4261, NDCG@10: 0.2489\n",
      "\n",
      "[INFO] === Epoch 2 ===\n",
      "[INFO] Training for book...\n",
      "[RESULT] book | Epoch 2 | Loss: 1321.3599, HR@10: 0.2871, NDCG@10: 0.1456\n",
      "[INFO] Training for movie...\n",
      "[RESULT] movie | Epoch 2 | Loss: 1303.3365, HR@10: 0.4270, NDCG@10: 0.2553\n",
      "\n",
      "[INFO] === Epoch 3 ===\n",
      "[INFO] Training for book...\n",
      "[RESULT] book | Epoch 3 | Loss: 1318.9921, HR@10: 0.3038, NDCG@10: 0.1576\n",
      "[INFO] Training for movie...\n",
      "[RESULT] movie | Epoch 3 | Loss: 1303.1806, HR@10: 0.4332, NDCG@10: 0.2593\n",
      "\n",
      "[INFO] === Epoch 4 ===\n",
      "[INFO] Training for book...\n",
      "[RESULT] book | Epoch 4 | Loss: 1317.4908, HR@10: 0.3260, NDCG@10: 0.1746\n",
      "[INFO] Training for movie...\n",
      "[RESULT] movie | Epoch 4 | Loss: 1301.6248, HR@10: 0.4299, NDCG@10: 0.2581\n",
      "[INFO] Epoch training completed.\n",
      "[INFO] Results for book saved to E2FEDREC_book_KSize_8_Result.pt\n",
      "[INFO] Results for movie saved to E2FEDREC_movie_KSize_8_Result.pt\n",
      "[INFO] Training completed successfully.\n",
      "[INFO] Program execution finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tasks = [[\"book\", \"movie\"]]\n",
    "    KList = [8]\n",
    "\n",
    "    for K_size in KList:\n",
    "        for domain_A, domain_B in tasks:\n",
    "            main(domain_A, domain_B, K_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
