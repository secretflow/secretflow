{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b40b9f1",
   "metadata": {},
   "source": [
    "# SplitGLORY：在隐语中使用基于全局图拆分的推荐算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf27a2",
   "metadata": {},
   "source": [
    "2023年发表在RecSys上新闻推荐的论文 **《Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations》**，提出了GLORY模型。通过利用所有用户点击累计频次构成的全局点击图中使用图神经网络提取隐含在不同的新闻邻居之中的潜在高阶特征增强新闻推荐的效果，但是在隐私保护的多联邦场景下，难以利用全局图进行协同的新闻推荐，因此针对这种场景下，设计了基于全局图共享的联邦拆分学习。本文重点介绍如何在隐语中使用拆分GLORY的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97042f5",
   "metadata": {},
   "source": [
    "## GLORY模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234e866",
   "metadata": {},
   "source": [
    "GLORY的全局图是通过所有用户来构建的，使用GGNN网络捕捉用户行为序列中的信息，整体结构如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95294537",
   "metadata": {},
   "source": "![整体结构](graph/11.png)"
  },
  {
   "cell_type": "markdown",
   "id": "9ad3d818",
   "metadata": {},
   "source": [
    "但是上述的模型结构在**多联邦推荐场景**下面临着很多问题，比如说：\n",
    "* 本地用户的存储和共享的通信开销大，难以构建和存储基于本地的用户全局点击图\n",
    "\n",
    "* 新闻的全局点击图的表征学习主要依赖的是GGNN网络，计算开销大，难以在本地用户场景下完全学习到用户的表征\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534069a",
   "metadata": {},
   "source": "![问题](graph/22.png)"
  },
  {
   "cell_type": "markdown",
   "id": "36bce458",
   "metadata": {},
   "source": [
    "首先需要对隐私保护的问题进行定义，在当前的推荐系统中，数据持有者（客户端）和服务器通常被认为是诚实的，但在某些场景下，他们可能会因好奇而尝试推断尽可能多的隐私信息。所谓“诚实但好奇”（半诚实）模型，意味着客户端和服务器虽然严格遵循协议，不进行恶意行为，但他们可能会利用协议中产生的所有中间计算结果，试图推测更多的用户数据，尤其是敏感的标签（label）和点击历史（history）。在这种情况下，服务器虽然不与任何数据持有者串通，但却可能通过对交互数据的深度分析，窥探到用户的隐私信息，进而影响系统的安全性和隐私性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc96988b",
   "metadata": {},
   "source": [
    "因此，我们将拆分学习方法应用到这种全局图的隐私推荐场景中，利用拆分学习的U-shape方法，在服务器端进行全局图的计算，通过把高负荷的存储量和计算量迁移到服务器端，在所有用户都不知道其他用户隐私数据的场景下，进行新闻推荐。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![拆分学习](graph/33.png)",
   "id": "eacc6ac8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据集介绍",
   "id": "f994666c"
  },
  {
   "cell_type": "markdown",
   "id": "15f9edfe",
   "metadata": {},
   "source": [
    "包含约16万篇新闻文章、100万匿名用户产生的1500万次新闻曝光记录及2400万次点击行为。数据采集自Microsoft News用户6周的匿名行为日志，覆盖新闻标题、摘要、正文等丰富文本信息。\n",
    "\n",
    "数据划分：\n",
    "分为训练集、验证集和测试集，每个子集包含以下4类文件：\n",
    "* news.tsv：新闻元数据（标题、摘要、类别、实体等）\n",
    "* behaviors.tsv：用户曝光日志与点击历史\n",
    "* entity_embedding.vec：新闻标题/摘要中实体的知识图谱嵌入\n",
    "* relation_embedding.vec：实体间关系的嵌入表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fa646",
   "metadata": {},
   "source": [
    "下载链接：https://msnews.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec015696",
   "metadata": {},
   "source": [
    "## 处理数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d25bf0",
   "metadata": {},
   "source": [
    "在data文件下有三个数据处理的文件，首先利用data_preprocess.py预先将用户行为数据分布式分片、提取新闻特征构建结构化数据、根据浏览记录生成新闻与实体的关联图谱，最终输出包含语义关系和图结构的标准化持久化数据。再利用dataset.py对数据进行封装成dataset类，最后通过下面的data_load.py加载数据进行后续的训练操作"
   ]
  },
  {
   "cell_type": "code",
   "id": "48739b5f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.utils import to_undirected\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from dataload.dataset import *  # 导入自定义数据集模块\n",
    "\n",
    "\n",
    "def load_data(cfg, mode='train', model=None, local_rank=0):\n",
    "    \"\"\"\n",
    "    加载训练、验证或测试数据。\n",
    "\n",
    "    :param cfg: 配置文件\n",
    "    :param mode: 数据模式，可选 'train', 'val', 'test'\n",
    "    :param model: 模型对象，只有在验证或测试时需要\n",
    "    :param local_rank: 当前进程的GPU编号\n",
    "    :return: 返回DataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    # 根据模式选择数据路径\n",
    "    data_dir = {\n",
    "        \"train\": cfg.dataset.train_dir,\n",
    "        \"val\": cfg.dataset.val_dir,\n",
    "        \"test\": cfg.dataset.test_dir,\n",
    "    }\n",
    "\n",
    "    # ------------- 加载新闻数据 -------------\n",
    "    # 载入新闻索引和新闻内容\n",
    "    news_index = pickle.load(open(Path(data_dir[mode]) / \"news_dict.bin\", \"rb\"))\n",
    "    news_input = pickle.load(open(Path(data_dir[mode]) / \"nltk_token_news.bin\", \"rb\"))\n",
    "\n",
    "    # ------------- 加载行为数据 -------------\n",
    "    if mode == 'train':\n",
    "        # 训练模式下，加载行为数据\n",
    "        target_file = (\n",
    "            Path(data_dir[mode]) / f\"behaviors_np{cfg.npratio}_{local_rank}.tsv\"\n",
    "        )\n",
    "\n",
    "        if cfg.model.use_graph:\n",
    "            # 如果使用图结构模型，加载新闻图数据\n",
    "            news_graph = torch.load(Path(data_dir[mode]) / \"nltk_news_graph.pt\")\n",
    "\n",
    "            # 如果是无向图，转换成无向图\n",
    "            if cfg.model.directed is False:\n",
    "                news_graph.edge_index, news_graph.edge_attr = to_undirected(\n",
    "                    news_graph.edge_index, news_graph.edge_attr\n",
    "                )\n",
    "            print(f\"[{mode}] News Graph Info: {news_graph}\")\n",
    "\n",
    "            # 加载新闻邻居字典\n",
    "            news_neighbors_dict = pickle.load(\n",
    "                open(Path(data_dir[mode]) / \"news_neighbor_dict.bin\", \"rb\")\n",
    "            )\n",
    "\n",
    "            # 如果模型使用实体图，还需要加载实体邻居数据\n",
    "            if cfg.model.use_entity:\n",
    "                entity_neighbors = pickle.load(\n",
    "                    open(Path(data_dir[mode]) / \"entity_neighbor_dict.bin\", \"rb\")\n",
    "                )\n",
    "                total_length = sum(len(lst) for lst in entity_neighbors.values())\n",
    "                print(f\"[{mode}] entity_neighbor list Length: {total_length}\")\n",
    "            else:\n",
    "                entity_neighbors = None\n",
    "\n",
    "            # 创建训练图数据集\n",
    "            dataset = TrainGraphDataset(\n",
    "                filename=target_file,\n",
    "                news_index=news_index,\n",
    "                news_input=news_input,\n",
    "                local_rank=local_rank,\n",
    "                cfg=cfg,\n",
    "                neighbor_dict=news_neighbors_dict,\n",
    "                news_graph=news_graph,\n",
    "                entity_neighbors=entity_neighbors,\n",
    "            )\n",
    "            dataloader = DataLoader(dataset, batch_size=None)  # 创建DataLoader\n",
    "\n",
    "        else:\n",
    "            # 不使用图结构模型时，直接加载普通的训练数据集\n",
    "            dataset = TrainDataset(\n",
    "                filename=target_file,\n",
    "                news_index=news_index,\n",
    "                news_input=news_input,\n",
    "                local_rank=local_rank,\n",
    "                cfg=cfg,\n",
    "            )\n",
    "\n",
    "            # 创建DataLoader，按GPU数进行批量划分\n",
    "            dataloader = DataLoader(\n",
    "                dataset, batch_size=int(cfg.batch_size / cfg.gpu_num), pin_memory=True\n",
    "            )\n",
    "        return dataloader\n",
    "    elif mode in ['val', 'test']:\n",
    "        # 转换新闻数据为嵌入\n",
    "        news_dataset = NewsDataset(news_input)\n",
    "        news_dataloader = DataLoader(\n",
    "            news_dataset,\n",
    "            batch_size=int(cfg.batch_size * cfg.gpu_num),\n",
    "            num_workers=cfg.num_workers,\n",
    "        )\n",
    "\n",
    "        stacked_news = []\n",
    "        with torch.no_grad():\n",
    "            # 在验证和测试模式下，计算新闻的嵌入表示\n",
    "            for news_batch in tqdm(\n",
    "                news_dataloader,\n",
    "                desc=f\"[{local_rank}] Processing validation News Embedding\",\n",
    "            ):\n",
    "                # 如果使用图模型，计算嵌入\n",
    "                if cfg.model.use_graph:\n",
    "                    batch_emb = (\n",
    "                        model.module.client.local_news_encoder(\n",
    "                            news_batch.long().unsqueeze(0).to(local_rank)\n",
    "                        )\n",
    "                        .squeeze(0)\n",
    "                        .detach()\n",
    "                    )\n",
    "                else:\n",
    "                    batch_emb = (\n",
    "                        model.module.client.local_news_encoder(\n",
    "                            news_batch.long().unsqueeze(0).to(local_rank)\n",
    "                        )\n",
    "                        .squeeze(0)\n",
    "                        .detach()\n",
    "                    )\n",
    "                stacked_news.append(batch_emb)\n",
    "\n",
    "        # 拼接所有新闻的嵌入表示\n",
    "        news_emb = torch.cat(stacked_news, dim=0).cpu().numpy()\n",
    "\n",
    "        if cfg.model.use_graph:\n",
    "            # 如果使用图结构模型，加载图数据\n",
    "            news_graph = torch.load(Path(data_dir[mode]) / \"nltk_news_graph.pt\")\n",
    "            news_neighbors_dict = pickle.load(\n",
    "                open(Path(data_dir[mode]) / \"news_neighbor_dict.bin\", \"rb\")\n",
    "            )\n",
    "\n",
    "            # 如果是无向图，转换成无向图\n",
    "            if cfg.model.directed is False:\n",
    "                news_graph.edge_index, news_graph.edge_attr = to_undirected(\n",
    "                    news_graph.edge_index, news_graph.edge_attr\n",
    "                )\n",
    "            print(f\"[{mode}] News Graph Info: {news_graph}\")\n",
    "\n",
    "            # 如果使用实体图，加载实体邻居数据\n",
    "            if cfg.model.use_entity:\n",
    "                # entity_graph = torch.load(Path(data_dir[mode]) / \"entity_graph.pt\")\n",
    "                entity_neighbors = pickle.load(\n",
    "                    open(Path(data_dir[mode]) / \"entity_neighbor_dict.bin\", \"rb\")\n",
    "                )\n",
    "                total_length = sum(len(lst) for lst in entity_neighbors.values())\n",
    "                print(f\"[{mode}] entity_neighbor list Length: {total_length}\")\n",
    "            else:\n",
    "                entity_neighbors = None\n",
    "\n",
    "            # 验证模式下，使用图数据集\n",
    "            if mode == 'val':\n",
    "                dataset = ValidGraphDataset(\n",
    "                    filename=Path(data_dir[mode])\n",
    "                    / f\"behaviors_np{cfg.npratio}_{local_rank}.tsv\",\n",
    "                    news_index=news_index,\n",
    "                    news_input=news_emb,\n",
    "                    local_rank=local_rank,\n",
    "                    cfg=cfg,\n",
    "                    neighbor_dict=news_neighbors_dict,\n",
    "                    news_graph=news_graph,\n",
    "                    news_entity=news_input[:, -8:-3],\n",
    "                    entity_neighbors=entity_neighbors,\n",
    "                )\n",
    "\n",
    "            # 创建DataLoader\n",
    "            dataloader = DataLoader(dataset, batch_size=None)\n",
    "\n",
    "        else:\n",
    "            # 不使用图结构时，使用普通的验证数据集\n",
    "            if mode == 'val':\n",
    "                dataset = ValidDataset(\n",
    "                    filename=Path(data_dir[mode]) / f\"behaviors_{local_rank}.tsv\",\n",
    "                    news_index=news_index,\n",
    "                    news_emb=news_emb,\n",
    "                    local_rank=local_rank,\n",
    "                    cfg=cfg,\n",
    "                )\n",
    "            else:\n",
    "                dataset = ValidDataset(\n",
    "                    filename=Path(data_dir[mode]) / f\"behaviors.tsv\",\n",
    "                    news_index=news_index,\n",
    "                    news_emb=news_emb,\n",
    "                    local_rank=local_rank,\n",
    "                    cfg=cfg,\n",
    "                )\n",
    "\n",
    "            # 创建DataLoader\n",
    "            dataloader = DataLoader(\n",
    "                dataset, batch_size=1, collate_fn=lambda b: collate_fn(b, local_rank)\n",
    "            )\n",
    "        return dataloader\n",
    "\n",
    "\n",
    "def collate_fn(tuple_list, local_rank):\n",
    "    \"\"\"\n",
    "    定义如何将批次中的样本合并为一个batch。\n",
    "\n",
    "    :param tuple_list: 一个包含样本的列表，每个样本是一个元组\n",
    "    :param local_rank: 当前进程的GPU编号\n",
    "    :return: 合并后的batch数据\n",
    "    \"\"\"\n",
    "    # 分别提取clicked_news、clicked_mask、candidate_news等数据\n",
    "    clicked_news = [x[0] for x in tuple_list]\n",
    "    clicked_mask = [x[1] for x in tuple_list]\n",
    "    candidate_news = [x[2] for x in tuple_list]\n",
    "    clicked_index = [x[3] for x in tuple_list]\n",
    "    candidate_index = [x[4] for x in tuple_list]\n",
    "\n",
    "    # 如果样本包含标签，则一并返回\n",
    "    if len(tuple_list[0]) == 6:\n",
    "        labels = [x[5] for x in tuple_list]\n",
    "        return (\n",
    "            clicked_news,\n",
    "            clicked_mask,\n",
    "            candidate_news,\n",
    "            clicked_index,\n",
    "            candidate_index,\n",
    "            labels,\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            clicked_news,\n",
    "            clicked_mask,\n",
    "            candidate_news,\n",
    "            clicked_index,\n",
    "            candidate_index,\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1943fb9",
   "metadata": {},
   "source": [
    "## 定义模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cc9c4",
   "metadata": {},
   "source": [
    "将模型拆分成GLORYServer和GLORYClient两个部分，最后通过GLORYSplit将完整的流程串起来，实现的是拆分学习中的U-shape策略,通过这个GLORYSplit模型实现了基于全局点击图的拆分学习的推荐，保护了用户的隐私数据。\n",
    "\n",
    "### 1. GLORYClient\n",
    "客户端侧部署轻量化模型，基于本地新闻数据执行细粒度语义特征提取，同时接收服务端下发的全局图向量，通过门控注意力网络将其与本地特征融合，实现隐私保护的混合表征计算。此过程严格遵循数据不动模型动原则，原始新闻内容始终驻留本地设备。\n",
    "\n",
    "### 2.GLORYServer\n",
    "作为全局知识中枢，GLORYServer通过联邦聚合机制构建并持续优化全局点击图（Global Click Graph），利用图卷积网络（GCN）学习新闻节点的联合嵌入表征。完成训练后，通过分层加密通道将新闻语义向量安全分发至各客户端，为本地模型提供跨用户兴趣模式的先验知识支撑。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import Sequential, GatedGraphConv\n",
    "from secretflow import PYUObject, proxy\n",
    "\n",
    "from models.base.layers import *\n",
    "from models.component.candidate_encoder import *\n",
    "from models.component.click_encoder import ClickEncoder\n",
    "from models.component.entity_encoder import EntityEncoder, GlobalEntityEncoder\n",
    "from models.component.nce_loss import NCELoss\n",
    "from models.component.news_encoder import *\n",
    "from models.component.user_encoder import *\n",
    "from secretflow import PYUObject, proxy\n",
    "import secretflow as sf\n",
    "\n",
    "\n",
    "@proxy(PYUObject)\n",
    "class GLORYServer(nn.Module):\n",
    "    '''\n",
    "    服务器只计算global gnn\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        glove_emb=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.news_dim = cfg.model.head_num * cfg.model.head_dim  # 新闻的维度\n",
    "\n",
    "        # GCN（图卷积网络）\n",
    "        self.global_news_encoder = Sequential(\n",
    "            'x, index',\n",
    "            [\n",
    "                (\n",
    "                    GatedGraphConv(self.news_dim, num_layers=3, aggr='add'),\n",
    "                    'x, index -> x',\n",
    "                ),  # 采用 GatedGraphConv 进行图卷积\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def forward(self, x_encoded, edge_index, mapping_idx):\n",
    "        # 服务器端计算图卷积特征\n",
    "        graph_emb = self.global_news_encoder(x_encoded, edge_index)\n",
    "        # clicked_graph_emb = graph_emb[mapping_idx, :]\n",
    "\n",
    "        return graph_emb\n",
    "\n",
    "\n",
    "@proxy(PYUObject)\n",
    "class GLORYClient(nn.Module):\n",
    "    '''\n",
    "    客户端负责本地新闻的语义计算和后续全局图向量传入的计算\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        entity_emb,\n",
    "        glove_emb=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.news_dim = cfg.model.head_num * cfg.model.head_dim  # 新闻的维度\n",
    "        self.local_news_encoder = NewsEncoder(cfg, glove_emb)\n",
    "        self.click_encoder = ClickEncoder(cfg)\n",
    "        self.user_encoder = UserEncoder(cfg)\n",
    "        self.candidate_encoder = CandidateEncoder(cfg)\n",
    "        self.click_predictor = DotProduct()\n",
    "        self.use_entity = cfg.model.use_entity\n",
    "        self.entity_dim = cfg.model.entity_emb_dim  # 实体的维度\n",
    "        self.loss_fn = NCELoss()\n",
    "        # 实体编码器\n",
    "        if self.use_entity:\n",
    "            pretrain = torch.from_numpy(entity_emb).float()  # 使用预训练的实体嵌入\n",
    "            self.entity_embedding_layer = nn.Embedding.from_pretrained(\n",
    "                pretrain, freeze=False, padding_idx=0\n",
    "            )  # 实体嵌入层\n",
    "\n",
    "            # 局部实体编码器\n",
    "            self.local_entity_encoder = Sequential(\n",
    "                'x, mask',\n",
    "                [\n",
    "                    (self.entity_embedding_layer, 'x -> x'),\n",
    "                    (\n",
    "                        EntityEncoder(cfg),\n",
    "                        'x, mask -> x',\n",
    "                    ),  # 使用 EntityEncoder 进行实体编码\n",
    "                ],\n",
    "            )\n",
    "            self.global_entity_encoder = Sequential(\n",
    "                'x, mask',\n",
    "                [\n",
    "                    (self.entity_embedding_layer, 'x -> x'),\n",
    "                    (\n",
    "                        GlobalEntityEncoder(cfg),\n",
    "                        'x, mask -> x',\n",
    "                    ),  # 使用 GlobalEntityEncoder 进行全局实体编码\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        subgraph,\n",
    "        mapping_idx,\n",
    "        candidate_news,\n",
    "        candidate_entity,\n",
    "        entity_mask,\n",
    "        label=None,\n",
    "    ):\n",
    "        # ----------------- 计算客户端的 x_encoded -----------------\n",
    "        x_encoded, mask, mapping_idx, batch_size, num_clicked, clicked_entity = (\n",
    "            self.process_news(\n",
    "                subgraph, mapping_idx, candidate_news, candidate_entity, entity_mask\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 服务器端返回 clicked_graph_emb\n",
    "        # 假设服务器端已经计算好了 clicked_graph_emb\n",
    "        clicked_graph_emb = self.get_clicked_graph_emb_from_server(\n",
    "            x_encoded, subgraph.edge_index, mapping_idx\n",
    "        )\n",
    "\n",
    "        # ----------------- 合并特征 -----------------\n",
    "        clicked_origin_emb = (\n",
    "            x_encoded[mapping_idx, :]\n",
    "            .masked_fill(~mask.unsqueeze(-1), 0)\n",
    "            .view(batch_size, num_clicked, self.news_dim)\n",
    "        )\n",
    "        user_emb = self.combine_embeddings(\n",
    "            clicked_origin_emb,\n",
    "            clicked_graph_emb,\n",
    "            clicked_entity,\n",
    "            mask,\n",
    "            batch_size,\n",
    "            num_clicked,\n",
    "        )\n",
    "\n",
    "        # ------------计算候选新闻向量并且预测点击 -----------------\n",
    "        score = self.predict_click(\n",
    "            user_emb, candidate_news, candidate_entity, entity_mask\n",
    "        )\n",
    "\n",
    "        # ----------------- 计算损失 -----------------\n",
    "        loss = self.loss_fn(score, label)\n",
    "\n",
    "        return loss, score\n",
    "\n",
    "    def combine_embeddings(\n",
    "        self,\n",
    "        clicked_origin_emb,\n",
    "        clicked_graph_emb,\n",
    "        clicked_entity,\n",
    "        mask,\n",
    "        batch_size,\n",
    "        num_clicked,\n",
    "    ):\n",
    "        # 合并原始特征和图卷积特征\n",
    "        clicked_total_emb = self.click_encoder(\n",
    "            clicked_origin_emb, clicked_graph_emb, clicked_entity\n",
    "        )\n",
    "        user_emb = self.user_encoder(clicked_total_emb, mask)  # 用户嵌入\n",
    "        return user_emb\n",
    "\n",
    "    def predict_click(self, user_emb, candidate_news, candidate_entity, entity_mask):\n",
    "        # 计算点击预测分数\n",
    "        # --------------------------------------------1-----------------------------------------------------------\n",
    "        # ----------------------------------------- 处理候选新闻 ------------------------------------\n",
    "        # 对候选新闻进行编码\n",
    "        cand_title_emb = self.local_news_encoder(candidate_news)  # 编码候选新闻标题\n",
    "        if self.use_entity:\n",
    "            # 分离候选新闻的原始实体和邻居实体\n",
    "            origin_entity, neighbor_entity = candidate_entity.split(\n",
    "                [\n",
    "                    self.cfg.model.entity_size,\n",
    "                    self.cfg.model.entity_size * self.cfg.model.entity_neighbors,\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "\n",
    "            # 对候选新闻的实体进行编码\n",
    "            cand_origin_entity_emb = self.local_entity_encoder(origin_entity, None)\n",
    "            cand_neighbor_entity_emb = self.global_entity_encoder(\n",
    "                neighbor_entity, entity_mask\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            cand_origin_entity_emb, cand_neighbor_entity_emb = None, None\n",
    "\n",
    "        # 将候选新闻的嵌入信息通过候选新闻编码器合并\n",
    "        cand_final_emb = self.candidate_encoder(\n",
    "            cand_title_emb, cand_origin_entity_emb, cand_neighbor_entity_emb\n",
    "        )\n",
    "        score = self.click_predictor(cand_final_emb, user_emb)\n",
    "        return score\n",
    "\n",
    "    def process_news(\n",
    "        self, subgraph, mapping_idx, candidate_news, candidate_entity, entity_mask\n",
    "    ):\n",
    "        # 处理新闻的编码过程\n",
    "        mask = mapping_idx != -1\n",
    "        mapping_idx[mapping_idx == -1] = 0\n",
    "        batch_size, num_clicked, token_dim = (\n",
    "            mapping_idx.shape[0],\n",
    "            mapping_idx.shape[1],\n",
    "            candidate_news.shape[-1],\n",
    "        )\n",
    "        clicked_entity = subgraph.x[mapping_idx, -8:-3]  # 获取点击新闻的实体信息\n",
    "        x_flatten = subgraph.x.view(1, -1, token_dim)  # 展平\n",
    "        x_encoded = self.local_news_encoder(x_flatten).view(-1, self.news_dim)  # 编码\n",
    "        if self.use_entity:\n",
    "            clicked_entity = self.local_entity_encoder(\n",
    "                clicked_entity, None\n",
    "            )  # 对点击新闻的实体进行编码\n",
    "        else:\n",
    "            clicked_entity = None\n",
    "        return x_encoded, mask, mapping_idx, batch_size, num_clicked, clicked_entity"
   ],
   "id": "8819f19e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练过程",
   "id": "b07e57a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.cuda import amp\n",
    "from tqdm import tqdm\n",
    "from dataload.data_load import load_data\n",
    "from dataload.data_preprocess import prepare_preprocessed_data\n",
    "from utils.metrics import *\n",
    "from utils.common import *\n",
    "import secretflow as sf\n",
    "\n",
    "\n",
    "def sf_train(\n",
    "    clients,\n",
    "    server,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    scheduler,\n",
    "    dataloader,\n",
    "    local_rank,\n",
    "    cfg,\n",
    "    early_stopping,\n",
    "    epochs,\n",
    "):\n",
    "    def distributed_train(epoch):\n",
    "        client_ops = []\n",
    "        for client in clients:\n",
    "\n",
    "            def client_train(client):\n",
    "                client.model.train()\n",
    "                sum_loss = torch.zeros(1).to(client.device)\n",
    "                sum_auc = torch.zeros(1).to(client.device)\n",
    "                for cnt, data in enumerate(dataloader, start=1):\n",
    "                    (\n",
    "                        subgraph,\n",
    "                        mapping_idx,\n",
    "                        candidate_news,\n",
    "                        candidate_entity,\n",
    "                        entity_mask,\n",
    "                        labels,\n",
    "                    ) = data\n",
    "                    subgraph = subgraph.to(client.device, non_blocking=True)\n",
    "                    mapping_idx = mapping_idx.to(client.device, non_blocking=True)\n",
    "                    candidate_news = candidate_news.to(client.device, non_blocking=True)\n",
    "                    labels = labels.to(client.device, non_blocking=True)\n",
    "                    candidate_entity = candidate_entity.to(\n",
    "                        client.device, non_blocking=True\n",
    "                    )\n",
    "                    entity_mask = entity_mask.to(client.device, non_blocking=True)\n",
    "\n",
    "                    with amp.autocast():\n",
    "                        (\n",
    "                            x_encoded,\n",
    "                            mask,\n",
    "                            mapping_idx,\n",
    "                            batch_size,\n",
    "                            num_clicked,\n",
    "                            clicked_entity,\n",
    "                        ) = client.process_news(\n",
    "                            subgraph,\n",
    "                            mapping_idx,\n",
    "                            candidate_news,\n",
    "                            candidate_entity,\n",
    "                            entity_mask,\n",
    "                        )\n",
    "                        clicked_origin_emb = (\n",
    "                            x_encoded[mapping_idx, :]\n",
    "                            .masked_fill(~mask.unsqueeze(-1), 0)\n",
    "                            .view(batch_size, num_clicked, client.news_dim)\n",
    "                        )\n",
    "                        graph_emb = server.forward(\n",
    "                            x_encoded, subgraph.edge_index, mapping_idx\n",
    "                        )\n",
    "                        clicked_graph_emb = (\n",
    "                            graph_emb[mapping_idx, :]\n",
    "                            .masked_fill(~mask.unsqueeze(-1), 0)\n",
    "                            .view(batch_size, num_clicked, client.news_dim)\n",
    "                        )\n",
    "                        user_emb = client.combine_embeddings(\n",
    "                            clicked_origin_emb,\n",
    "                            clicked_graph_emb,\n",
    "                            clicked_entity,\n",
    "                            mask,\n",
    "                            batch_size,\n",
    "                            num_clicked,\n",
    "                        )\n",
    "                        score = client.predict_click(\n",
    "                            user_emb, candidate_news, candidate_entity, entity_mask\n",
    "                        )\n",
    "                        bz_loss = client.loss_fn(score, labels)\n",
    "\n",
    "                    scaler.scale(bz_loss).backward()\n",
    "\n",
    "                    if cnt % cfg.accumulation_steps == 0 or cnt == int(\n",
    "                        cfg.dataset.pos_count / cfg.batch_size\n",
    "                    ):\n",
    "                        scaler.step(optimizer)\n",
    "                        old_scaler = scaler.get_scale()\n",
    "                        scaler.update()\n",
    "                        new_scaler = scaler.get_scale()\n",
    "                        if new_scaler >= old_scaler:\n",
    "                            scheduler.step()\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                    sum_loss += bz_loss.data.float()\n",
    "                    sum_auc += area_under_curve(labels, score)\n",
    "\n",
    "                    if cnt % cfg.log_steps == 0:\n",
    "                        print(\n",
    "                            f\"[Client {client.device}] Epoch: {epoch}, Step: {cnt}, Loss: {sum_loss.item() / cfg.log_steps:.5f}\"\n",
    "                        )\n",
    "                        sum_loss.zero_()\n",
    "                        sum_auc.zero_()\n",
    "\n",
    "                return client.model.state_dict()\n",
    "\n",
    "            client_ops.append(client_train(client))\n",
    "\n",
    "        client_models = sf.wait(client_ops)\n",
    "        global_model = server.average(client_models)\n",
    "\n",
    "        update_ops = []\n",
    "        for client in clients:\n",
    "\n",
    "            def client_update(client, global_model):\n",
    "                client.model.load_state_dict(global_model)\n",
    "                return client.model.state_dict()\n",
    "\n",
    "            update_ops.append(client_update(client, global_model))\n",
    "\n",
    "        sf.wait(update_ops)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        distributed_train(epoch)\n",
    "        res = val(clients[0].model, server.model, local_rank, cfg)\n",
    "        early_stop, get_better = early_stopping(res['auc'])\n",
    "        if early_stop:\n",
    "            print(\"Early Stop.\")\n",
    "            break\n",
    "        elif get_better:\n",
    "            print(f\"Better Result at Epoch {epoch}!\")\n",
    "\n",
    "    sf.shutdown()\n",
    "\n",
    "\n",
    "# 分成3个用户\n",
    "sf.init(parties=[\"alice\", \"bob\", \"lisa\", \"server\"], address='local', num_gpus=1)\n",
    "alice_pyu = sf.PYU(\"alice\")\n",
    "bob_pyu = sf.PYU(\"bob\")\n",
    "bob_pyu = sf.PYU(\"lisa\")\n",
    "\n",
    "server_pyu = sf.PYU(\"server\")\n",
    "\n",
    "cfg = type('Config', (), {})()\n",
    "cfg.num_epochs = 5\n",
    "clients = [\n",
    "    GLORYClient(cfg, device=alice_pyu),\n",
    "    GLORYClient(cfg, device=bob_pyu),\n",
    "    GLORYClient(cfg, device=lisa_pyu),\n",
    "]\n",
    "server = GLORYServer(cfg, device=server_pyu)\n",
    "\n",
    "sf_train(\n",
    "    clients=clients,\n",
    "    server=server,\n",
    "    optimizer=torch.optim.Adam(\n",
    "        [p for c in clients for p in c.model.parameters()]\n",
    "        + list(server.model.parameters()),\n",
    "        lr=0.001,\n",
    "    ),\n",
    "    scaler=amp.GradScaler(),\n",
    "    scheduler=torch.optim.lr_scheduler.StepLR(\n",
    "        torch.optim.Adam([p for c in clients for p in c.model.parameters()], lr=0.001),\n",
    "        step_size=1,\n",
    "        gamma=0.1,\n",
    "    ),\n",
    "    dataloader=[\n",
    "        (torch.randn(1, 28, 28), torch.randint(0, 2, (1,))) for _ in range(100)\n",
    "    ],\n",
    "    local_rank=0,\n",
    "    cfg=cfg,\n",
    "    early_stopping=EarlyStopping,\n",
    ")"
   ],
   "id": "8fe4049cde41e3b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练结果",
   "id": "7bb440376244c93f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "根据实验结果，GLORY-split在AUC和NDCG@5这两个关键指标上相较于GLORY有所提升。首先，AUC从66.92提升至67.21，表明拆分策略有效提高了模型区分用户点击与否的能力，增强了对用户兴趣偏好的识别。其次，NDCG@5从35.56提升至35.60，虽然提升幅度较小，但说明拆分策略在前5个推荐项的相关性上有了微弱的改善，使得推荐结果在保护用户隐私的同时，更加精准地匹配了用户的兴趣。",
   "id": "87682a78"
  },
  {
   "cell_type": "markdown",
   "id": "725df8f1",
   "metadata": {},
   "source": [
    "## 实验代码总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf716d9",
   "metadata": {},
   "source": [
    "1. 进行图数据的处理，构建点击全局图\n",
    "2. 采用拆分学习框架，在用户本地处理敏感数据，服务器仅计算全局点击图\n",
    "3. 实验证明该方法在保护隐私的同时，推荐效果仍有小幅提升。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
