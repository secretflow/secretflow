{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐语SecretFlow金融风控全链路能力展示\n",
    "\n",
    "> This tutorial is only available in Chinese."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Last updated: Oct 7, 2023\n",
    ">\n",
    "> 请使用v0.8.3或以上版本的隐语进行实验。\n",
    ">\n",
    "> 注意： 以下代码仅供演示用途，在演示过程中可能会揭露部分信息。请勿直接将此示例代码用于实际生产环境中。在实际部署前，请根据您的具体需求和安全标准进行必要的修改和调整。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实验将会展示如何使用隐语进行在风控领域常用的Logistic Regeression模型和XGB模型的模型研发工作。\n",
    "\n",
    "隐语接下来将会开放模型部署和在线/离线模型预测功能，敬请期待。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验目标\n",
    "\n",
    "在本次实验中，我们将会利用一个开源数据集训练一个金融风控场景常用的线性回归和XGB模型。在此过程中将包含以下步骤：\n",
    "\n",
    "- 样本对齐\n",
    "- 特征预处理\n",
    "- 数据分析\n",
    "- 模型训练\n",
    "- 模型预测\n",
    "- 模型评估\n",
    "\n",
    "请依次执行所有步骤确保实验可以顺利完成。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验前置工作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化隐语框架\n",
    "\n",
    "在本次实验中，我们将会包含两个节点：**alice** 和 **bob** . 在真实业务场景，他们将会代表两个不同实体，他们之间的原始数据不被允许直接相互传输，但是他们的原始数据将会被一起用以研发一个模型。\n",
    "\n",
    "在下面的代码中，我们建立了一个 **SecretFlow Cluster**, 基于 **alice** 和 **bob** 两个节点，我们还创建了三个device：\n",
    "\n",
    "- alice: PYU device, 负责在alice侧的本地计算，计算输入、计算过程和计算结果仅alice可见\n",
    "- bob: PYU device, 负责在bob侧的本地计算，计算输入、计算过程和计算结果仅bob可见\n",
    "- spu: SPU device, 负责alice和bob之间的密态计算，计算输入和计算结果为密态，由alice和bob各掌握一个分片，计算过程为MPC计算，由alice和bob各自的SPU Runtime一起执行。\n",
    "\n",
    ">  如果你尚未理解以上的一些概念，比如SPU设备，请参考这篇[文档](../developer/design/architecture.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# secretflow.utils.simulation.datasets contains mirrors of some popular open dataset.\n",
    "from secretflow.utils.simulation.datasets import dataset\n",
    "\n",
    "df = pd.read_csv(dataset('bank_marketing_full'), sep=';')\n",
    "df['uid'] = df.index + 1\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "/root/miniconda3/envs/sectb0923/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-25 19:29:26,954\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of SecretFlow: 1.9.0b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sectb0923/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2024-11-25 19:29:30,196\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import secretflow as sf\n",
    "\n",
    "# Check the version of your SecretFlow\n",
    "print('The version of SecretFlow: {}'.format(sf.__version__))\n",
    "\n",
    "sf.shutdown()\n",
    "sf.init(['alice', 'bob'], address='local')\n",
    "alice, bob = sf.PYU('alice'), sf.PYU('bob')\n",
    "spu = sf.SPU(sf.utils.testing.cluster_def(['alice', 'bob']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的log中，你应该发现，在**spu**的创建过程中，alice和bob两边都各有一个 **SPURuntime** 被建立并互相创建连接。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集\n",
    "\n",
    "本次实验我们采用的原始数据是来自UCI的[Bank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/bank+marketing). 这个数据集汇集了一家葡萄牙银行机构电话营销的结果。\n",
    "\n",
    "我们添加了**uid**这一列用于接下来隐私求交的实验。\n",
    "\n",
    "我们首先看一下数据集所包含的信息。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该数据集包含了45211个样本，每一个样本代表了一个目标客户。\n",
    "\n",
    "每个样本包含16个feature，我们这里简单描述一下这个数据集所有的feature。\n",
    "\n",
    "\n",
    "| feature | 描述 | 取值 |\n",
    "| :-----| :---- | :---- |\n",
    "| uid | 客户编码 | 数字 |\n",
    "| age | 年龄 | 数字 |\n",
    "| job | 工作类型 |  'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown' |\n",
    "| marital | 婚姻状况 | 'divorced','married','single','unknown' |\n",
    "| education | 教育状况 | 'tertiary', 'secondary', 'unknown', 'primary' |\n",
    "| default | 是否有不良信用记录 | 'no','yes','unknown' |\n",
    "| housing | 是否有房贷 |  'no','yes','unknown' |\n",
    "| loan | 是否有个人贷款 | 'no','yes','unknown' |\n",
    "| contact | 联系方式 | 'cellular','telephone' |\n",
    "| month | 上次联系月份 | 'jan', 'feb', 'mar', ..., 'nov', 'dec' |\n",
    "| day | 上次联系月日 |数字|\n",
    "| duration | 上次沟通时间 | 数字 |\n",
    "| campaign | 本次活动已经沟通的次数 | 数字 |\n",
    "| pdays | 距离上次沟通经过的天数 | 数字 |\n",
    "| previous | 在本次活动之前已经沟通的次数 | 数字 |\n",
    "| poutcome | 之前活动的结果 | 'unknown', 'failure', 'other', 'success' | \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个样本的label - y表示对于目标客户的营销结果（是否签订了定额存款合同），取值是'yes','no'。\n",
    "\n",
    "我们假定以上16个feature由两个机构分别掌握，具体如下。\n",
    "\n",
    "- alice: age, job, marital, education, default, balance, housing, loan\n",
    "- bob: contact, day, month, duration, campaign, pdays, previous, poutcome, y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在真实业务场景中, alice和bob所掌握的数据可能是没有对齐的，为了模拟这种情况，我们将数据集shuffle之后，再随机各取90%来模拟这个状况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_alice \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:, np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m      5\u001b[0m df_alice\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_alice = df.iloc[:, np.r_[0:8, -1]].sample(frac=0.9)\n",
    "\n",
    "df_alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bob = df.iloc[:, 8:].sample(frac=0.9)\n",
    "\n",
    "df_bob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们这里将df_alice和df_bob保存为文件，作为alice和bob两方的原始输入。\n",
    "\n",
    "至此，我们完成了所有实验准备工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "_, alice_path = tempfile.mkstemp()\n",
    "_, bob_path = tempfile.mkstemp()\n",
    "df_alice.reset_index(drop=True).to_csv(alice_path, index=False)\n",
    "df_bob.reset_index(drop=True).to_csv(bob_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样本对齐（隐私求交）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，第一步我们需要将两边的数据对齐。\n",
    "隐私求交（[Private Set Intersection](https://en.wikipedia.org/wiki/Private_set_intersection))是一种密码学方法，可以获取两个集合的交集，而不泄露任何其他信息。\n",
    "在隐语中，SPU设备支持三种隐私求交算法:\n",
    "\n",
    "- [ECDH](https://ieeexplore.ieee.org/document/6234849/)：半诚实模型, 基于公钥密码学，原本适用于小数据集，但是隐语优化后已经能支持10亿量级的数据。\n",
    "- [KKRT](https://eprint.iacr.org/2016/799.pdf)：半诚实模型, 基于布谷鸟哈希（Cuckoo Hashing）以及高效不经意传输扩展（OT Extension），适用于大数据集（比如千万数据集）。\n",
    "- [BC22PCG](https://eprint.iacr.org/2022/334)：半诚实模型, 基于随机相关函数生成器，适用于大数据集。\n",
    "\n",
    "由于我们这里的数据集较小，我们这里采用的是ECDH方法。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方式一：将隐私求交结果保存至文件\n",
    "\n",
    "在一些应用场景场景中，alice和bob可能在隐私求交之后将结果直接保存至文件中，之后再进行后续操作。这个时候，请调用**psi_csv**接口。\n",
    "\n",
    "在以下代码中，我们分别制定了两边需要求交的key以及输入和输出路径。\n",
    "\n",
    "我们需要指定双方的输入文件和输出文件路径。对于ECDH来说，由于双方的地位是平等的，receiver并没有实际含义，你可以任意指定。我们需要设定正确的protocol。sort设为true之后，join的结果将会被排序。\n",
    "\n",
    "> 请阅读 psi_csv 的文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, alice_psi_path = tempfile.mkstemp()\n",
    "_, bob_psi_path = tempfile.mkstemp()\n",
    "\n",
    "spu.psi_csv(\n",
    "    key=\"uid\",\n",
    "    input_path={alice: alice_path, bob: bob_path},\n",
    "    output_path={alice: alice_psi_path, bob: bob_psi_path},\n",
    "    receiver=\"alice\",\n",
    "    protocol=\"ECDH_PSI_2PC\",\n",
    "    sort=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方式二：将求交结果保存至VDataFrame\n",
    "\n",
    "VDataFrame是隐语中保存垂直切分数据的数据结构，在接下来的任务中，我们将会不断使用VDataFrame的数据结构。\n",
    "\n",
    "由于在本次实验中，经过隐私求交之后，我们还有后续操作，所以我们在这里使用 **data.vertical.read_csv** 来将原始数据隐私求交之后的结果直接转化为VDataFrame。\n",
    "\n",
    "> 请阅读data.vertical.read_csv的文档。很多参数和psi_csv是一致的，这里不再赘述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.data.vertical import read_csv as v_read_csv\n",
    "\n",
    "vdf = v_read_csv(\n",
    "    {alice: alice_path, bob: bob_path},\n",
    "    spu=spu,\n",
    "    keys=\"uid\",\n",
    "    drop_keys=\"uid\",\n",
    "    psi_protocl=\"ECDH_PSI_2PC\",\n",
    ")\n",
    "vdf.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更多\n",
    "\n",
    "我们在这里展示的是两方单键的隐私求交，隐语也支持三方和多键的隐私求交技术，想要了解更多信息，你可以：\n",
    "\n",
    "- 阅读这篇[文档](https://www.secretflow.org.cn/docs/spu/en/development/psi.html)了解隐语SPU的隐私求交能力。\n",
    "- 阅读该[教程](./PSI_On_SPU.ipynb)了解使用的例子。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征预处理\n",
    "\n",
    "一般情况下，我们都需要对用于建模的数据进行预处理，合理的预处理对模型训练效果非常关键。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始特征预处理之前，我们先使用 **stats.table_statistics.table_statistics** 来查看一下特征总体情况，我们会在后面专门讨论全表统计模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.stats.table_statistics import table_statistics\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "data_stats = table_statistics(vdf)\n",
    "data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在接下来，我们将会展示隐语以下特征预处理能力：\n",
    "\n",
    "- 值替换\n",
    "- 缺失值填充\n",
    "- WOE分组/分箱转换\n",
    "- one-hot编码\n",
    "- 标准化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 值替换\n",
    "\n",
    "我们先对以下特征做值替换：\n",
    "\n",
    "| feature | 描述 | 取值和值替换规则 |\n",
    "| :-----| :---- | :---- |\n",
    "| education | 教育状况 | 'tertiary' -> 3, 'secondary' -> 2, 'unknown' -> 0, 'primary' -> 1 |\n",
    "| default | 是否有不良信用记录 | 'no' -> 0,'yes' -> 1,'unknown' -> NaN |\n",
    "| housing | 是否有房贷 |  'no' -> 0,'yes' -> 1,'unknown' -> NaN |\n",
    "| loan | 是否有个人贷款 | 'no' -> 0,'yes' -> 1,'unknown' -> NaN |\n",
    "| month | 上次联系月份 | 'jan' -> 1, 'feb' -> 2, 'mar' -> 3, ..., 'nov' -> 11, 'dec' ->12 |\n",
    "| y | label | 'yes' -> 1,'no' -> 0 |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "替换完之后，我们使用 **sf.reveal** 来查看效果，请注意在生产中，**sf.reveal** 将会直接泄露数据，需要严格限制和进行审计。\n",
    "\n",
    "> 在生产中，请严格限制**sf.reveal**的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf['education'] = vdf['education'].replace(\n",
    "    {'tertiary': 3, 'secondary': 2, 'primary': 1, 'unknown': np.NaN}\n",
    ")\n",
    "\n",
    "vdf['default'] = vdf['default'].replace({'no': 0, 'yes': 1, 'unknown': np.NaN})\n",
    "\n",
    "vdf['housing'] = vdf['housing'].replace({'no': 0, 'yes': 1, 'unknown': np.NaN})\n",
    "\n",
    "vdf['loan'] = vdf['loan'].replace({'no': 0, 'yes': 1, 'unknown': np.NaN})\n",
    "\n",
    "vdf['month'] = vdf['month'].replace(\n",
    "    {\n",
    "        'jan': 1,\n",
    "        'feb': 2,\n",
    "        'mar': 3,\n",
    "        'apr': 4,\n",
    "        'may': 5,\n",
    "        'jun': 6,\n",
    "        'jul': 7,\n",
    "        'aug': 8,\n",
    "        'sep': 9,\n",
    "        'oct': 10,\n",
    "        'nov': 11,\n",
    "        'dec': 12,\n",
    "    }\n",
    ")\n",
    "\n",
    "vdf['y'] = vdf['y'].replace(\n",
    "    {\n",
    "        'no': 0,\n",
    "        'yes': 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(sf.reveal(vdf.partitions[alice].data))\n",
    "print(sf.reveal(vdf.partitions[bob].data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "值替换操作由数据所有者的PYU Device执行，不会泄露数据。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值填充\n",
    "\n",
    "接下来我们对缺失值进行填充。我们在这里均填充了众数，其他可选的策略还包括平均数、中位数等。\n",
    "\n",
    "其他可能的处理方法包括删除缺省的行, 或者可以使用数据完整的行作为训练集，以此来预测缺失值。\n",
    "\n",
    "替换完之后，我们使用 **sf.reveal** 来查看效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf[\"education\"] = vdf[\"education\"].fillna(vdf[\"education\"].mode())\n",
    "vdf[\"default\"] = vdf[\"default\"].fillna(vdf[\"default\"].mode())\n",
    "vdf[\"housing\"] = vdf[\"housing\"].fillna(vdf[\"housing\"].mode())\n",
    "vdf[\"loan\"] = vdf[\"loan\"].fillna(vdf[\"loan\"].mode())\n",
    "\n",
    "print(sf.reveal(vdf.partitions[alice].data))\n",
    "print(sf.reveal(vdf.partitions[bob].data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "所填充的缺失值由属于数据所有者的PYU Device执行，并在接下来的缺失值操作中由数据所有者的PYU Device使用，不会泄露数据。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### woe分箱\n",
    "\n",
    "woe分箱用于将连续值替换为离散值。\n",
    "\n",
    "将连续型特征离散化的一个好处是可以有效地克服数据中隐藏的缺陷： 使模型结果更加稳定。例如，数据中的极端值是影响模型效果的一个重要因素。极端值导致模型参数过高或过低，或导致模型被虚假现象\"迷惑\"，把原来不存在的关系作为重要模式来学习。而离散化可以有效地减弱极端值和异常值的影响。\n",
    "\n",
    "变量duration的75%分位数远小于最大值，而且该变量的标准差相对也比较大。因此需要对变量duration进行离散化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.preprocessing.binning.vert_woe_binning import VertWoeBinning\n",
    "from secretflow.preprocessing.binning.vert_bin_substitution import VertBinSubstitution\n",
    "from secretflow.component.dataframe import CompDataFrame\n",
    "\n",
    "vdf = CompDataFrame.from_pandas(vdf, None, 'age', 'y')\n",
    "binning = VertWoeBinning(spu)\n",
    "bin_rules = binning.binning(\n",
    "    vdf,\n",
    "    binning_method=\"chimerge\",\n",
    "    bin_num=4,\n",
    "    bin_names={alice: [], bob: [\"duration\"]},\n",
    "    label_name=\"y\",\n",
    ")\n",
    "\n",
    "woe_sub = VertBinSubstitution()\n",
    "vdf = woe_sub.substitution(vdf, bin_rules)\n",
    "vdf = CompDataFrame.to_pandas(vdf, None)\n",
    "print(sf.reveal(vdf.partitions[alice].data))\n",
    "print(sf.reveal(vdf.partitions[bob].data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "woe分桶需要利用alice和bob两边的数据，因此相关的计算需要使用**SPU device**确保原始数据不被泄露。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot编码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot编码适用于将类型编码转化为数值编码。 对于job、marital等特征我们需要one-hot编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.preprocessing.encoder import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "# for vif and correlation only\n",
    "vdf_hat = vdf.drop(columns=[\"job\", \"marital\", \"contact\", \"month\", \"day\", \"poutcome\"])\n",
    "\n",
    "tranformed_df = encoder.fit_transform(vdf['job'])\n",
    "vdf[tranformed_df.columns] = tranformed_df\n",
    "\n",
    "tranformed_df = encoder.fit_transform(vdf['marital'])\n",
    "vdf[tranformed_df.columns] = tranformed_df\n",
    "\n",
    "tranformed_df = encoder.fit_transform(vdf['contact'])\n",
    "vdf[tranformed_df.columns] = tranformed_df\n",
    "\n",
    "tranformed_df = encoder.fit_transform(vdf['month'])\n",
    "vdf[tranformed_df.columns] = tranformed_df\n",
    "\n",
    "tranformed_df = encoder.fit_transform(vdf['day'])\n",
    "vdf[tranformed_df.columns] = tranformed_df\n",
    "\n",
    "tranformed_df = encoder.fit_transform(vdf['poutcome'])\n",
    "vdf[tranformed_df.columns] = tranformed_df\n",
    "\n",
    "vdf = vdf.drop(columns=[\"job\", \"marital\", \"contact\", \"month\", \"day\", \"poutcome\"])\n",
    "\n",
    "print(sf.reveal(vdf.partitions[alice].data))\n",
    "print(sf.reveal(vdf.partitions[bob].data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "one-hot编码操作由数据所有者的PYU Device执行，不会泄露数据。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准化 \n",
    "特征之间数值差距太大会使得模型收敛困难，我们一般先对数值进行标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.preprocessing import StandardScaler\n",
    "\n",
    "X = vdf.drop(columns=['y'])\n",
    "y = vdf['y']\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "vdf[X.columns] = X\n",
    "print(sf.reveal(vdf.partitions[alice].data))\n",
    "print(sf.reveal(vdf.partitions[bob].data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "标准化操作由数据所有者的PYU Device执行，不会泄露数据。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更多\n",
    "\n",
    "隐语还支持其他更多的特征预处理能力，请参考这篇[文档](./data_preprocessing_with_data_frame.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们已经完成了所有特征预处理工作。\n",
    "\n",
    "> 本文主要目的是为了展示隐语的预处理能力，本文对于数据预处理方法的使用可能是有争议的，敬请谅解。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分析"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在建模之前，我们有必要分析一下我们所使用的数据，以便确认是否需要重复特征预处理的过程。\n",
    "\n",
    "下面我们将会展示隐语以下数据分析能力:\n",
    "\n",
    "- 全表统计\n",
    "- 相关系数矩阵\n",
    "- VIF指标计算\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全表统计\n",
    "\n",
    "我们提供了类似于 **pd.DataFrame.describe** 来展示所有特征的基本统计信息。\n",
    "\n",
    "> 在特征预处理的过程中，你可以不断调用全表统计来关注预处理效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.stats.table_statistics import table_statistics\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "data_stats = table_statistics(vdf)\n",
    "data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "请注意，全表统计会暴露数据整体统计结果，其背后实际上蕴含了**sf.reveal**,请谨慎使用。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相关系数矩阵\n",
    "\n",
    "我们接下来计算特征和特征之间，特征和标签之间的相关系数矩阵。\n",
    "\n",
    "> 计算相关系数矩阵时，one-hot编码各列无需参与计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.stats.ss_pearsonr_v import PearsonR\n",
    "\n",
    "pearson_r_calculator = PearsonR(spu)\n",
    "corr_matrix = pearson_r_calculator.pearsonr(vdf_hat)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "corr_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "相关系数矩阵的计算需要利用alice和bob两边的数据，因此相关的计算需要使用**SPU device**确保原始数据不被泄露。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF指标计算\n",
    "\n",
    "隐语还支持VIF的计算来进行多重共线性检验。\n",
    "\n",
    "> 计算VIF指标时，one-hot编码各列无需参与计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.stats.ss_vif_v import VIF\n",
    "\n",
    "vif_calculator = VIF(spu)\n",
    "vif_results = vif_calculator.vif(vdf_hat)\n",
    "print(vdf_hat.columns)\n",
    "print(vif_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "VIF指标的计算需要利用alice和bob两边的数据，因此相关的计算需要使用**SPU device**确保原始数据不被泄露。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "接下来，我们将会分别训练一个逻辑回归模型和一个XGB模型。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机分割\n",
    "\n",
    "在训练之前，我们需要将数据分割为训练集和验证集。\n",
    "\n",
    "其中train_x和train_y为训练集的特征和标签。test_x和test_y为训练集的特征和标签。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.data.split import train_test_split\n",
    "\n",
    "random_state = 1234\n",
    "\n",
    "train_vdf, test_vdf = train_test_split(vdf, train_size=0.8, random_state=random_state)\n",
    "\n",
    "train_x = train_vdf.drop(columns=['y'])\n",
    "train_y = train_vdf['y']\n",
    "\n",
    "test_x = test_vdf.drop(columns=['y'])\n",
    "test_y = test_vdf['y']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "随机分割时，每一方会共享随机数种子，并由每一方数据的owner分别执行各自的数据分割并且确保最终分割结果仍然是对齐的。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSI（人群稳定性分析）\n",
    "\n",
    "样本稳定指数是衡量样本变化所产生的偏移量的一种重要指标，通常用来衡量样本的稳定程度，比如样本在两个月份之间的变化是否稳定。通常变量的PSI值在0.1以下表示变化不太显著，在0.1到0.25之间表示有比较显著的变化，大于0.25表示变量变化比较剧烈，需要特殊关注。\n",
    "\n",
    "接下来以`balance`为例子，确认两次抽样的样本分布是否接近。\n",
    "\n",
    "> 根据业务需求，PSI分析也可以在数据分析或者特征预处理的时候进行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = table_statistics(train_x['balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val, max_val = stats_df['min'], stats_df['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.stats import psi_eval\n",
    "from secretflow.stats.core.utils import equal_range\n",
    "import jax.numpy as jnp\n",
    "\n",
    "split_points = equal_range(jnp.array([min_val, max_val]), 3)\n",
    "balance_psi_score = psi_eval(train_x['balance'], test_x['balance'], split_points)\n",
    "\n",
    "sf.reveal(balance_psi_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "PSI分析是一个单方运算，由数据owner的PYU Device执行计算。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归模型\n",
    "\n",
    "使用 **ml.linear.ss_sgd.SSRegression** 可以进行密态逻辑回归模型的训练。\n",
    "\n",
    "请参考相关的API文档。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.ml.linear.ss_sgd import SSRegression\n",
    "\n",
    "lr_model = SSRegression(spu)\n",
    "lr_model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    epochs=3,\n",
    "    learning_rate=0.1,\n",
    "    batch_size=1024,\n",
    "    sig_type='t1',\n",
    "    reg_type='logistic',\n",
    "    penalty='l2',\n",
    "    l2_norm=0.5,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能会对为何上面的语句很快就执行完毕感到困惑，原因是在隐语中，语句都是lazy evaluation的，在上面的例子中，直到lr_model被真正被使用的时候，**lr_model.fit**才会被执行。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "SSRegression的训练基于SPU Device，双方的原始数据将会被保护。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 **ml.boost.ss_xgb_v.Xgb** 可以进行密态XGBoost模型的训练。\n",
    "\n",
    "请参考相关的API文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.ml.boost.ss_xgb_v import Xgb\n",
    "\n",
    "xgb = Xgb(spu)\n",
    "params = {\n",
    "    'num_boost_round': 3,\n",
    "    'max_depth': 5,\n",
    "    'sketch_eps': 0.25,\n",
    "    'objective': 'logistic',\n",
    "    'reg_lambda': 0.2,\n",
    "    'subsample': 1,\n",
    "    'colsample_by_tree': 1,\n",
    "    'base_score': 0.5,\n",
    "}\n",
    "xgb_model = xgb.train(params=params, dtrain=train_x, label=train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xgb.train将会直接执行，请耐心等待。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "Xgb的训练基于SPU Device，双方的原始数据将会被保护。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测\n",
    "\n",
    "接下来，我们将会分别利用刚刚训练好的模型来预测测试集。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归模型\n",
    "\n",
    "由于在我们的场景下，数据集标签的持有者是bob，因此我们在这里将预测结果**reveal**给bob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_y_hat = lr_model.predict(x=test_x, batch_size=1024, to_pyu=bob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "逻辑回归的预测基于SPU Device，双方的原始数据将会被保护。\n",
    "\n",
    "当设置**to_pyu**，预测结果将会被reveal给该方，否则将仍然保持秘密分享的状态。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost模型\n",
    "\n",
    "由于在我们的场景下，数据集标签的持有者是bob，因此我们在这里将预测结果**reveal**给bob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_y_hat = xgb_model.predict(dtrain=test_x, to_pyu=bob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安全性讨论\n",
    "\n",
    "XGBoost模型的预测基于SPU Device，双方的原始数据将会被保护。\n",
    "\n",
    "当设置**to_pyu**，预测结果将会被reveal给该方，否则将仍然保持秘密分享的状态。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "\n",
    "接下来，我们将利用测试数据集对模型效果进行评估，包括：\n",
    "\n",
    "- 二分类评估\n",
    "- PVA\n",
    "- P-Value\n",
    "- 评分卡转换"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二分类评估\n",
    "\n",
    "隐语中对二分类的评估有集成的支持。\n",
    "\n",
    "`BiClassificationEval` 将计算 `AUC`, `KS`, `F1 Score`, `Lift`, `K-S`, `Gain`, `Precision`, `Recall` 等统计数值， 并提供（基于prediction score的）等频和等距分箱的统计报告和总报告。\n",
    "\n",
    "不同分桶中评估模型的预测的`threshold`不同。总报告中依赖`threshold`的统计取的是各个分桶的最佳值。\n",
    "\n",
    "详情可以参考API文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.stats.biclassification_eval import BiClassificationEval\n",
    "\n",
    "biclassification_evaluator = BiClassificationEval(\n",
    "    y_true=test_y, y_score=lr_y_hat, bucket_size=20\n",
    ")\n",
    "lr_report = sf.reveal(biclassification_evaluator.get_all_reports())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'positive_samples: {lr_report.summary_report.positive_samples}')\n",
    "print(f'negative_samples: {lr_report.summary_report.negative_samples}')\n",
    "print(f'total_samples: {lr_report.summary_report.total_samples}')\n",
    "print(f'auc: {lr_report.summary_report.auc}')\n",
    "print(f'ks: {lr_report.summary_report.ks}')\n",
    "print(f'f1_score: {lr_report.summary_report.f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biclassification_evaluator = BiClassificationEval(\n",
    "    y_true=test_y, y_score=xgb_y_hat, bucket_size=20\n",
    ")\n",
    "xgb_report = sf.reveal(biclassification_evaluator.get_all_reports())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'positive_samples: {xgb_report.summary_report.positive_samples}')\n",
    "print(f'negative_samples: {xgb_report.summary_report.negative_samples}')\n",
    "print(f'total_samples: {xgb_report.summary_report.total_samples}')\n",
    "print(f'auc: {xgb_report.summary_report.auc}')\n",
    "print(f'ks: {xgb_report.summary_report.ks}')\n",
    "print(f'f1_score: {xgb_report.summary_report.f1_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测偏差\n",
    "\n",
    "结果由`abs(mean(Acutal) - mean(Prediction))`计算获得, 值越小越好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.stats import prediction_bias_eval\n",
    "\n",
    "prediction_bias = prediction_bias_eval(\n",
    "    test_y, lr_y_hat, bucket_num=4, absolute=True, bucket_method='equal_width'\n",
    ")\n",
    "\n",
    "sf.reveal(prediction_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pva_score = prediction_bias_eval(\n",
    "    test_y, xgb_y_hat, bucket_num=4, absolute=True, bucket_method='equal_width'\n",
    ")\n",
    "\n",
    "sf.reveal(xgb_pva_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-Value\n",
    "双方可通过p-value的值来判断参数是否显著，即该自变量是否可以有效预测因变量的变异, 从而判定对应的解释变量是否应包括在模型中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.stats import SSPValue\n",
    "\n",
    "model = lr_model.save_model()\n",
    "sspv = SSPValue(spu)\n",
    "pvalues = sspv.pvalues(test_x, test_y, model)\n",
    "\n",
    "pvalues"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评分卡转换\n",
    "\n",
    "> 严格来说，评分卡转化是对预测结果的后续处理，并不属于模型评估。\n",
    "\n",
    "\n",
    "我们将 `y = 1` 的概率设为`p`， `odds = p / (1 - p)`, 评分卡设定的分值刻度可以通过将分值表示为比率对数的线性表达式来定义，即可表示为下式：\n",
    "\n",
    "`Score = A - B log(odds)`， A 和 B 是可以设定的常数。隐语中提供了评分卡转换功能，详情可以参考API文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secretflow.stats import BiClassificationEval, ScoreCard\n",
    "\n",
    "sc = ScoreCard(20, 600, 20)\n",
    "score = sc.transform(xgb_y_hat)\n",
    "\n",
    "sf.reveal(score.partitions[bob])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安全性讨论\n",
    "\n",
    "以上所有模型评估的方法均为单方运算，由label拥有者的PYU Device进行运算。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验结束\n",
    "\n",
    "最后，我们需要清理临时文件，并关闭隐语cluster。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=20486)\u001b[0m Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:29:40,094 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15005519872; capacity: 422622425088. Object creation will fail if spilling is required.\n",
      "\u001b[36m(pid=20487)\u001b[0m Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:29:50,101 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15005265920; capacity: 422622425088. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:30:00,107 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15005065216; capacity: 422622425088. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:30:10,114 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15004782592; capacity: 422622425088. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:30:20,120 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15004672000; capacity: 422622425088. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:30:30,126 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15004442624; capacity: 422622425088. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:30:40,133 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15004200960; capacity: 422622425088. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:30:50,139 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15003992064; capacity: 422622425088. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:31:00,146 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15003795456; capacity: 422622425088. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:31:10,152 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15003533312; capacity: 422622425088. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-11-25 19:31:20,159 E 18230 18264] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-11-25_19-29-28_220953_17801 is over 95% full, available space: 15003406336; capacity: 422622425088. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.remove(alice_path)\n",
    "    os.remove(alice_psi_path)\n",
    "    os.remove(bob_path)\n",
    "    os.remove(bob_psi_path)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "sf.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜！你已经完成了隐语金融风控全链路的全部实验内容。\n",
    "\n",
    "如果你对本实验有任何建议和问题，请在[Github Issues](https://github.com/secretflow/secretflow/issues)上联系我们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sectb0923]",
   "language": "python",
   "name": "conda-env-sectb0923-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "02db3bab010a384e41503da74327ad4dd04080832919be62bcff46931ddfd4bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
