## Attack
[An Overview of Federated Deep Learning Privacy Attacks and Defensive Strategies. 2020-04-01](https://arxiv.org/pdf/2004.04676.pdf)
### Backdoor Attack
- [How To Backdoor Federated Learning](https://arxiv.org/pdf/1807.00459.pdf)

- [Can You Really Backdoor Federated Learning?](https://arxiv.org/abs/1911.07963)
- [Attack of the Tails: Yes, You Really Can Backdoor Federated Learning](https://papers.nips.cc/paper/2020/file/b8ffa41d4e492f0fad2f13e29e1762eb-Paper.pdf)

- [DBA: Distributed Backdoor Attacks against Federated Learning](https://openreview.net/pdf?id=rkgyS0VFvr)

- [CRFL: Certifiably Robust Federated Learning against Backdoor Attacks. ICML 2021.](https://arxiv.org/pdf/2106.08283.pdf)


- [NeurIPS 2020 Submission: Backdoor Attacks on Federated Meta-Learning](https://arxiv.org/pdf/2006.07026.pdf)

### Gradients Attack
- [Deep Leakage from Gradients](https://arxiv.org/pdf/1906.08935.pdf)
- [Gradient Disaggregation: Breaking Privacy in Federated Learning by Reconstructing the User Participant Matrix](https://arxiv.org/pdf/2106.06089.pdf)
-[iDLG: Improved Deep Leakage from Gradients](https://arxiv.org/pdf/2001.02610.pdf)

- [Inverting Gradients - How easy is it to break Privacy in Federated Learning?](https://arxiv.org/pdf/2003.14053.pdf)

- [CAFE: Catastrophic Data Leakage in Vertical Federated Learning](https://papers.neurips.cc/paper/2021/file/08040837089cdf46631a10aca5258e16-Paper.pdf)

- [Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning.](https://arxiv.org/pdf/1702.07464.pdf)
- [A Framework for Evaluating Gradient Leakage Attacks in Federated Learning](https://arxiv.org/pdf/2004.10397.pdf)
- [Inverting Gradients - How easy is it to break privacy in federated learning? 2020-03-31](https://arxiv.org/pdf/2003.14053.pdf)

### Model Poison Attack
- [Analyzing Federated Learning through an Adversarial Lens](https://arxiv.org/abs/1811.12470)
- [(*) Local Model Poisoning Attacks to Byzantine-Robust Federated Learning. 2019-11-26](https://arxiv.org/pdf/1911.11815.pdf)
- [Data Poisoning Attacks on Federated Machine Learning. 2020-04-19](https://arxiv.org/pdf/2004.10020.pdf)

### Free Rider Attack
- [NeurIPS 2020 submission: Free-rider Attacks on Model Aggregation in Federated Learning](https://arxiv.org/pdf/2006.11901.pdf)
- [Free-riders in Federated Learning: Attacks and Defenses. 2019-11-28](https://arxiv.org/pdf/1911.12560.pdf)

### Membership Inference Attack

### Feature Inference Attack

## Defense

### With DP
- [Differentially Private Federated Learning: A Client Level Perspective. NIPS 2017 Workshop](https://arxiv.org/pdf/1712.07557.pdf)

- [Federated Learning with Bayesian Differential Privacy.](https://arxiv.org/pdf/1911.10071.pdf)

- [FedSel: Federated SGD under Local Differential Privacy with Top-k Dimension Selection.](https://arxiv.org/pdf/2003.10637.pdf)

- [LDP-Fed: Federated Learning with Local Differential Privacy.](https://arxiv.org/pdf/2006.03637.pdf)

- [Differentially Private Learning with Adaptive Clipping](https://arxiv.org/pdf/1905.03871.pdf)

### With HE

### With TEE


### Algorithm
- [RSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets, AAAI 2019](https://arxiv.org/abs/1811.03761)

- [Towards Realistic Byzantine-Robust Federated Learning. 2020-04-10](https://arxiv.org/pdf/2004.04986.pdf)

- [Secure Byzantine-Robust Machine Learning. 2020-06-08](https://arxiv.org/pdf/2006.04747.pdf)

- [Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates](https://arxiv.org/pdf/1803.01498.pdf)

## Fairness
- [Fair Resource Allocation in Federated Learning. ICLR 2020.](https://arxiv.org/pdf/1905.10497.pdf)

- [Hierarchically Fair Federated Learning](https://arxiv.org/pdf/2004.10386.pdf)

- [Towards Fair and Privacy-Preserving Federated Deep Models](https://arxiv.org/pdf/1906.01167.pdf)
