# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Ant Group Co., Ltd.
# This file is distributed under the same license as the SecretFlow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: SecretFlow \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-09-14 22:41+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.10.3\n"

#: ../../user_guide/mpc_ml/decision_tree.rst:2
msgid "Decision Trees"
msgstr "å†³ç­–æ ‘æ¨¡å‹"

#: ../../user_guide/mpc_ml/decision_tree.rst:4
msgid ""
"With the help of Secret Sharing, a secure multi-party computation "
"technique, SecretFlow implements provably secure gradient boosting model "
":py:meth:`~secretflow.ml.boost.ss_xgb_v.model.Xgb` to support both "
"regression and binary classification machine learning tasks."
msgstr ""
"SecretFlowä½¿ç”¨å¤šæ–¹å®‰å…¨è®¡ç®—çš„ç§˜å¯†åˆ†äº«æŠ€æœ¯å®ç°äº†å¯è¯å®‰å…¨çš„æ¢¯åº¦ä¸‹é™å†³ç­–æ ‘æ¨¡å‹ "
":py:meth:`~secretflow.ml.boost.ss_xgb_v.model.Xgb` ï¼Œ"
"ç›®å‰æ”¯æŒçº¿æ€§å›å½’é—®é¢˜è®­ç»ƒå’ŒäºŒåˆ†ç±»é—®é¢˜è®­ç»ƒã€‚"

#: ../../user_guide/mpc_ml/decision_tree.rst:10
msgid "Dataset Settings"
msgstr "æ•°æ®è®¾å®š"

#: ../../user_guide/mpc_ml/decision_tree.rst:11
msgid "vertically partitioned dataset:"
msgstr "å‚ç›´åˆ’åˆ†çš„æ•°æ®é›†"

#: ../../user_guide/mpc_ml/decision_tree.rst:13
msgid "samples are aligned among the participants"
msgstr "æ‰€æœ‰æ•°æ®æ–¹çš„æ ·æœ¬ä¸€è‡´"

#: ../../user_guide/mpc_ml/decision_tree.rst:14
msgid "different participant obtains different features"
msgstr "ä½†æ˜¯æ‹¥æœ‰æ ·æœ¬çš„ä¸åŒç‰¹å¾"

#: ../../user_guide/mpc_ml/decision_tree.rst:15
msgid "one participant owns the label"
msgstr "åªæœ‰ä¸€æ–¹æŒæœ‰æ ‡ç­¾"

#: ../../user_guide/mpc_ml/decision_tree.rst:21
msgid "XGBoost Training Algorithm"
msgstr "XGBoost è®­ç»ƒç®—æ³•"

#: ../../user_guide/mpc_ml/decision_tree.rst:22
msgid ""
"Algorithm details can be found in `the official documents "
"<https://xgboost.readthedocs.io/en/stable/tutorials/model.html>`_. The "
"main process of building a single tree is as follows:"
msgstr ""
"è¯¦ç»†åŸç†åŠæ¨å¯¼å¯è§"
" `å®˜æ–¹æ–‡æ¡£ <https://xgboost.readthedocs.io/en/stable/tutorials/model.html>`_ ã€‚"
"å•æ£µæ ‘åˆ†è£‚çš„ä¸»è¦è¿‡ç¨‹å¦‚ä¸‹ï¼š"

#: ../../user_guide/mpc_ml/decision_tree.rst:25
msgid ""
"Statistics calculating: calculate the first-order gradient :math:`g_{i}` "
"and second-order gradient :math:`h_{i}` for each sample with current "
"prediction and label, according to the definition of loss function."
msgstr ""
"é¢„è®¡ç®—ï¼šæ ¹æ®æŸå¤±å‡½æ•°å®šä¹‰ã€æ ·æœ¬æ ‡ç­¾ã€å½“å‰é¢„æµ‹å€¼ï¼Œ"
"è®¡ç®—æ¯ä¸ªæ ·æœ¬å¯ä»¥æ±‚å¾—å…¶ä¸€é˜¶å¯¼ :math:`g_{i}` å’ŒäºŒé˜¶å¯¼  :math:`h_{i}`"

#: ../../user_guide/mpc_ml/decision_tree.rst:28
msgid ""
"Node splitting: enumerates all possible split candidates and choose the "
"best one with the maximal gain. A split candidate is consisted of a split"
" feature and a split value, which divides the samples in current node "
":math:`I` into two child nodes :math:`I_{L}` and :math:`I_{R}`, according"
" to their feature values. Then, a split gain is computed with the "
"following formula:"
msgstr ""
"èŠ‚ç‚¹åˆ†è£‚ï¼šé€šè¿‡æšä¸¾æ‰€æœ‰åˆ†è£‚æ–¹æ¡ˆï¼Œé€‰å‡ºå¸¦æ¥æœ€ä¼˜å¢ç›Šå€¼çš„æ–¹å¼æ‰§è¡Œåˆ†è£‚ã€‚"
"åˆ†è£‚æ–¹æ¡ˆåŒ…å«åˆ†è£‚ç‰¹å¾å’Œåˆ†è£‚é˜ˆå€¼ï¼Œå¯ä»¥å°†å½“å‰èŠ‚ç‚¹æ ·æœ¬é›†åˆ :math:`I` åˆ†è£‚ä¸º "
"å·¦å­æ ‘æ ·æœ¬é›†åˆ :math:`I_{L}` å’Œå³å­æ ‘æ ·æœ¬é›†åˆ :math:`I_{R}`ï¼Œ "
"å¹¶ç”±å¦‚ä¸‹å…¬å¼è®¡ç®—å‡ºæ­¤åˆ†è£‚æ–¹æ¡ˆçš„å¢ç›Šå€¼ï¼š"

#: ../../user_guide/mpc_ml/decision_tree.rst:38
msgid ""
"where :math:`\\lambda` and :math:`\\gamma` are the regularizers for the "
"leaf number and leaf weights respectively. In this way, we can split the "
"nodes recursively until the leaf."
msgstr ""
"å…¶ä¸­ï¼š:math:`\\lambda` å’Œ :math:`\\gamma` åˆ†åˆ«ä¸ºå¶èŠ‚ç‚¹æ•°å’Œå¶èŠ‚ç‚¹æƒé‡çš„æƒ©ç½šå› å­ã€‚"

#: ../../user_guide/mpc_ml/decision_tree.rst:42
msgid ""
"Weight calculating: calculate the weights of leaf nodes with the "
"following formula:"
msgstr ""
"æƒé‡è®¡ç®—ï¼šç”±è½å…¥è¯¥èŠ‚ç‚¹çš„æ ·æœ¬è®¡ç®—å¾—åˆ°ï¼Œå…¬å¼å¦‚ä¸‹ï¼š"

#: ../../user_guide/mpc_ml/decision_tree.rst:49
msgid "Regression and classification share the same training process except:"
msgstr ""
"å›å½’é—®é¢˜å’Œåˆ†ç±»é—®é¢˜çš„è®­ç»ƒæµç¨‹æ˜¯ç›¸åŒçš„ï¼Œé™¤äº†ï¼š"

#: ../../user_guide/mpc_ml/decision_tree.rst:51
msgid ""
"they employs different loss functions, i.e. MSE for regression and "
"Logloss for classification."
msgstr ""
"æŸå¤±å‡½æ•°çš„é€‰æ‹©ï¼ˆå›å½’-MSEï¼Œåˆ†ç±»-Logloss)ã€‚"

#: ../../user_guide/mpc_ml/decision_tree.rst:52
msgid ""
"classification executes an extra sigmoid function to transform the "
"prediction into a probability."
msgstr ""
"åˆ†ç±»é—®é¢˜éœ€è¦å°†é¢„æµ‹å€¼é€šè¿‡sigmoidå‡½æ•°è½¬åŒ–ä¸ºæ¦‚ç‡ã€‚"

#: ../../user_guide/mpc_ml/decision_tree.rst:55
msgid "SS-XGB Training"
msgstr "SS-XGB è®­ç»ƒç®—æ³•"

#: ../../user_guide/mpc_ml/decision_tree.rst:56
msgid ""
"SS-XGB :py:meth:`~secretflow.ml.boost.ss_xgb_v.model.Xgb` use secret "
"sharing to compute the split gain and leaf weights."
msgstr ""
"SS-XGB :py:meth:`~secretflow.ml.boost.ss_xgb_v.model.Xgb` "
"ä½¿ç”¨ç§˜å¯†åˆ†äº«è®¡ç®—åˆ†è£‚å¢ç›Šå€¼å’Œå¶æƒé‡ã€‚"

#: ../../user_guide/mpc_ml/decision_tree.rst:58
msgid ""
"In order to implement a secure joint training, we replace all the "
"computations with secret sharing protocols, e.g. Addition, "
"Multiplication, etc. In addition, we have to take special care to "
"accumulate the gradients without leaking out the feature partial order of"
" samples."
msgstr ""
"æˆ‘ä»¬ä½¿ç”¨ç§˜å¯†åˆ†äº«åè®®æä¾›çš„åŠ /ä¹˜ç­‰æ“ä½œæ¥å®ç°å®‰å…¨çš„å¤šæ–¹è”åˆè®¡ç®—ã€‚"
"ç‰¹åˆ«éœ€è¦å…³æ³¨çš„é—®é¢˜æ˜¯ï¼šå¦‚ä½•åœ¨è®¡ç®—åˆ†æ¡¶åŠ å’Œæ—¶ï¼Œä¸æ³„æ¼ä»»ä½•æ ·æœ¬åˆ†å¸ƒç›¸å…³çš„ä¿¡æ¯ã€‚"

#: ../../user_guide/mpc_ml/decision_tree.rst:62
msgid "This problem can be solved by introducing an indicator vector ğ‘†."
msgstr "é€šè¿‡å¼•å…¥ä¸€ä¸ªå¯†æ€ä¸‹çš„å‘é‡ğ‘†å°±å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚"

#: ../../user_guide/mpc_ml/decision_tree.rst:66
msgid ""
"The samples to be accumulated is marked as 1 in ğ‘† and 0 otherwise. To "
"preserve privacy, the indicator vector also transformed to secret shares."
" In this way, the sum of the gradients of the samples can be computed as "
"the inner product of the indicator vector and the gradient vector, which "
"can be securely computed by secret sharing protocols."
msgstr ""
"å‘é‡ğ‘†ä¸­æ ‡è®°ä¸º1çš„æ ·æœ¬æ˜¯è¢«é€‰ä¸­çš„æ ·æœ¬éœ€è¦åŠ å’Œï¼Œ0ç›¸åã€‚ä¸ºäº†ä¿è¯æ ·æœ¬åˆ†å¸ƒä¸æ³„æ¼ï¼Œ"
"è¿™ä¸ªå‘é‡ä¹Ÿæ˜¯é€šè¿‡ç§˜å¯†åˆ†äº«åè®®ä¿æŠ¤çš„ã€‚åœ¨ç§˜å¯†åˆ†äº«åè®®çš„ä¿æŠ¤ä¸‹ï¼Œè®¡ç®—å‘é‡ğ‘†å’Œæ¢¯åº¦å‘é‡çš„å†…ç§¯ï¼Œ"
"å³å¯å¾—åˆ°æ¢¯åº¦åœ¨åˆ†æ¡¶å†…çš„ç´¯åŠ å’Œã€‚"

#: ../../user_guide/mpc_ml/decision_tree.rst:70
msgid ""
"Similarly, the indicator trick can be used to hide the instance "
"distribution on nodes. Refer to our paper `Large-Scale Secure XGB for "
"Vertical Federated Learning <https://arxiv.org/pdf/2005.08479.pdf>`_ for "
"more details about SS-XGB algorithm and security analysis."
msgstr ""
"é€šè¿‡è¿™ä¸ªæ–¹æ³•æˆ‘ä»¬å°±å¯ä»¥ä¿æŠ¤æ ·æœ¬çš„åˆ†å¸ƒä¿¡æ¯ä¸æ³„æ¼ã€‚æ›´å¤šçš„ç®—æ³•ç»†èŠ‚å’Œå®‰å…¨åˆ†æï¼š"
"`Large-Scale Secure XGB for Vertical Federated Learning <https://arxiv.org/pdf/2005.08479.pdf>`_"

#: ../../user_guide/mpc_ml/decision_tree.rst:75
msgid "Example"
msgstr "ç”¨ä¾‹"

#: ../../user_guide/mpc_ml/decision_tree.rst:77
msgid ""
"A local cluster(Standalone Mode) needs to be initialized as the running "
"environment for this example. See `Deployment "
"<../../getting_started/deployment>`_ and refer to the 'Cluster "
"Mode'."
msgstr ""
"åœ¨æœ¬ç¤ºä¾‹ä¸­ä½¿ç”¨å•èŠ‚ç‚¹æ¨¡å¼åšç¤ºèŒƒã€‚é›†ç¾¤æ¨¡å¼çš„éƒ¨ç½²æ–¹å¼ï¼š "
"`Deployment <../../getting_started/deployment>`_ "


#: ../../user_guide/mpc_ml/decision_tree.rst:80
msgid ""
"For more details about the APIs, see "
":py:meth:`~secretflow.ml.boost.ss_xgb_v.model.Xgb`"
msgstr ""
"APIè¯¦æƒ…ï¼š:py:meth:`~secretflow.ml.boost.ss_xgb_v.model.Xgb`"


