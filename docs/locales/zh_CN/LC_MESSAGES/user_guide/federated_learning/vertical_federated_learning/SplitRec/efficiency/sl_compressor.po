# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Ant Group Co., Ltd.
# This file is distributed under the same license as the SecretFlow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
msgid ""
msgstr ""
"Project-Id-Version: SecretFlow \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-06-25 11:09+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:9
msgid "SplitRec：在隐语拆分学习中使用通信压缩"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:11
msgid "以下代码仅作为示例，请勿在生产环境直接使用。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:15
msgid "本示例基于基于“拆分学习：银行营销”教程制作，建议先观看那个教程。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:17
msgid "在拆分学习中，由于模型被拆分在多个设备当中，进行训练的时候，各方需要对特征和梯度进行多次传输，带来很高的网络通讯消耗。为了减少通讯过程中的数据量，可以进行一些压缩处理。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:19
msgid "SecretFlow提供了Compressor对拆分学习中的数据进行压缩。同时也提供了多种基类，可以在此基础上实现自己的压缩算法。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:21
msgid "下面我们来试试一些算法的可用性，首先，我们在secretflow环境中创造2个实体alice和bob。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:74
msgid "接下来我们准备要学习的数据。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:76
msgid "我们使用“拆分学习：银行营销”中的数据准备和处理方法，下载银行营销数据集并进行处理。alice和bob的角色和之前的教程完全相同："
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:161
msgid "接下来我们创建联邦模型，同样地，我们使用“拆分学习：银行营销”中的建模，构建出base_model和fuse_model，然后就可以定义SLModel用于训练："
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:299
msgid "使用通讯压缩算法"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:301
msgid "SecretFlow提供了Compressor，里面实现了各种基础的通讯压缩算法，可以直接使用。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:303
msgid "只要导入想使用的压缩算法并实例化,定义SLModel时将实例化的方法作为参数传入就可以在训练中实现通讯压缩。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:305
msgid "我们以QuantizedFP为例，该算法会将浮点数量化到8位以降低传输消耗。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:363
msgid "我们分别对没有使用通讯压缩的模型和使用了量化压缩的模型进行训练，并把训练轮次拉高到40轮，看看效果如何。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1283
msgid "可以看到，两个模型的验证集auc均在0.85左右波动，使用8位量化对此任务的训练精度影响不大，而理论通讯消耗减少了3/4（从32位减少到了8位）。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1295
msgid "自定义通讯压缩算法"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1297
msgid "我们也可以自定义一个压缩算法，SecretFlow提供了SparseCompressor和QuantizedCompressor基类，对应稀疏化方法和量化压缩方法。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1299
msgid "这里以量化压缩方法为例，来实现一个基于K-means的压缩算法。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1301
msgid ""
"K-means压缩论文是”Deep Compression: Compressing Deep Neural Networks with "
"Pruning, Trained Quantization and Huffman "
"Coding”提出的方法中的其中一个步骤，其思想是把对传输参数进行聚类，保存聚类中心的值，然后把其他值用聚类序号来表示。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1303
msgid ""
"继承QuantizedCompressor后，只要实现_compress_one（将一个numpy向量打包为QuantizedCompressedData）"
" 和 \\_decompress_one（将QuantizedCompressedData还原回numpy向量）函数即可。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1377
msgid "我们来实例化这个算法，再跑一遍联邦学习模型："
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1868
msgid "最终验证集auc在0.855左右，也还不错~"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1880
msgid "压缩算法的压缩效果"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1882
msgid "我们在ImageNet预训练的ResNet网络为例，试一下Int8、Fp8和Kmeans方法对模型参数的压缩效果，看看有什么差异。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:1999
msgid "可以看到，kmeans压缩在控制精度损失方面表现最好，但压缩时间非常长。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:2001
msgid "浮点数(Fp8-M4E3)对ResNet模型参数压缩的效果略优于整型(Int8)压缩，时间消耗是整型压缩的3倍。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:2003
msgid "实际应用压缩算法时，可根据计算资源和压缩精度进行平衡。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:2015
msgid "总结"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:2017
msgid "本篇示例介绍了通讯压缩算法，并在拆分学习的基础之上使用了SecretFlow提供和自行设计的压缩算法。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:2019
msgid "从实验数据可以看出，将32位数压缩为8位的精度损失不大，而理论通信消耗仅为不作压缩时的1/4，因此在需要频繁传输数据和梯度的拆分学习中，加入通讯压缩不失为一个好的选择。"
msgstr ""

#: ../../user_guide/federated_learning/vertical_federated_learning/SplitRec/efficiency/sl_compressor.ipynb:2021
msgid "本教程使用明文聚合来做演示，同时没有考虑隐藏层的泄露问题，SecretFlow提供了聚合层AggLayer，通过MPC,TEE,HE，以及DP等方式规避隐层明文传输泄露的问题。如果您感兴趣，可以看相关文档。"
msgstr ""

#~ msgid ""
#~ "K-means压缩论文是“Deep Compression: Compressing Deep "
#~ "Neural Networks with Pruning, Trained "
#~ "Quantization and Huffman "
#~ "Coding”提出的方法中的其中一个步骤，其思想是把对传输参数进行聚类，保存聚类中心的值，然后把其他值用聚类序号来表示。"
#~ msgstr ""
