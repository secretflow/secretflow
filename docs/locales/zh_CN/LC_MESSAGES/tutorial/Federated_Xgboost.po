# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Ant Group Co., Ltd.
# This file is distributed under the same license as the SecretFlow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: SecretFlow \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-01-07 11:23+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"

#: ../../tutorial/Federated_Xgboost.ipynb:9
msgid "Federated XGBoost"
msgstr "联邦XGBoost"

#: ../../tutorial/Federated_Xgboost.ipynb:20
msgid ""
"The following codes are demos only. It’s **NOT for production** due to "
"system security concerns, please **DO NOT** use it directly in "
"production."
msgstr "以下代码仅作为演示用，请勿直接在生产环境使用。"

#: ../../tutorial/Federated_Xgboost.ipynb:31
msgid ""
"In this tutorial, we will learn how to use ``SecretFlow`` to train tree "
"models for horizontal federation. Secretflow provides ``tree modeling`` "
"capabilities for horizontal scenarios(``SFXgboost``), The usage of "
"``SFXgboost`` is similar to ``XGBoost``, you can easily convert your "
"existing XGBoost program into a federated model for ``SecretFlow``."
msgstr ""
"在本教程中，我们将学习如何使用 `SecretFlow` 来训练水平联邦的树模型。Secretflow 为水平场景提供了 ``tree "
"modeling`` 能力（\\ ``SFXgboost``），``SFXgboost`` 类似于 ``XGBoost``，您可以轻松地将现有的 "
"XGBoost 程序转换为 `SecretFlow` 的联合模型。"

#: ../../tutorial/Federated_Xgboost.ipynb:43
msgid "Xgboost"
msgstr ""

#: ../../tutorial/Federated_Xgboost.ipynb:54
msgid ""
"XGBoost is an optimized distributed gradient boosting library designed to"
" be highly efficient, flexible and portable. It implements machine "
"learning algorithms under the Gradient Boosting framework"
msgstr "XGBoost 是一个优化的分布式梯度提升库，旨在高效、灵活和便携。 它在 Gradient Boosting 框架下实现机器学习算法"

#: ../../tutorial/Federated_Xgboost.ipynb:56
msgid ""
"official tutorial `XGBoost tutorials "
"<https://xgboost.readthedocs.io/en/latest/tutorials/index.html>`__"
msgstr ""
"官方文档 `XGBoost tutorials "
"<https://xgboost.readthedocs.io/en/latest/tutorials/index.html>`__ "

#: ../../tutorial/Federated_Xgboost.ipynb:68
msgid "prepare secretflow devices"
msgstr "准备secretflow devices"

#: ../../tutorial/Federated_Xgboost.ipynb:123
msgid "XGBoost Example"
msgstr "XGBoost示例"

#: ../../tutorial/Federated_Xgboost.ipynb:344
msgid "Then, How to do federated xgboost in secretflow?"
msgstr "那么，我们在SecretFlow中应该怎么做联邦XGBoost呢"

#: ../../tutorial/Federated_Xgboost.ipynb:346
msgid ""
"Use federate Binning method based on iteration to calculate the global "
"bucket information combined with the data of all sides, which was used as"
" the candidate to enter the subsequent construction procedure"
msgstr "使用基于迭代的federate binning方法联合各方数据计算全局分桶信息，作为candidate splits进入后续的建树流程"

#: ../../tutorial/Federated_Xgboost.ipynb:347
msgid "The data is input into each Client XGBoost engine to calculate G & H"
msgstr "数据输入到各个Client xgboost引擎中，计算G & H"

#: ../../tutorial/Federated_Xgboost.ipynb:348
msgid "Train federated boosting model"
msgstr "进行联邦建树流程 "

#: ../../tutorial/Federated_Xgboost.ipynb:350
msgid "Data is reassigned to the node to be split"
msgstr "进行数据reassign，分配到待分裂的节点上"

#: ../../tutorial/Federated_Xgboost.ipynb:351
msgid ""
"The sum of grad and the sum of hess are calculated according to the "
"previously calculated binning buckets"
msgstr "根据之前计算好的binning分桶计算sum_of_grad 和sum_of_hess"

#: ../../tutorial/Federated_Xgboost.ipynb:352
msgid ""
"Send the sum of grad and the sum of hess to server，server use secure "
"aggregation to produce global summary，then choose best split point，Send "
"best split info back to clients."
msgstr "发送给server端，server端做secure aggregation，挑选分裂信息发送回client端"

#: ../../tutorial/Federated_Xgboost.ipynb:353
msgid "Clients Updates local model"
msgstr "Clients更新本地分裂"

#: ../../tutorial/Federated_Xgboost.ipynb:355
msgid "Finish training，and save model"
msgstr "完成训练，并保存模型。"

#: ../../tutorial/Federated_Xgboost.ipynb:366
msgid ""
"Create 3 entities in the Secretflow environment [Alice, Bob, Charlie] "
"Where ‘Alice’ and ‘Bob’ are clients, and ``Charlie`` is the server,then "
"you can happily start ``Federate Boosting``."
msgstr ""
"在 Secretflow 环境中创建 3 个实体 [Alice, Bob, "
"Charlie]'Alice'和'Bob'是客户端，'Charlie'是服务器，那么你可以愉快地开始 ``Federate Boosting`` "
"了。"

#: ../../tutorial/Federated_Xgboost.ipynb:378
msgid "Prepare Data"
msgstr "准备数据"

#: ../../tutorial/Federated_Xgboost.ipynb:438
msgid "Prepare Params"
msgstr "准备超参"

#: ../../tutorial/Federated_Xgboost.ipynb:492
msgid "Create SFXgboost"
msgstr "创建SFXgboost"

#: ../../tutorial/Federated_Xgboost.ipynb:516
msgid "run SFXgboost"
msgstr "运行SFXgboost"

#: ../../tutorial/Federated_Xgboost.ipynb:732
msgid ""
"Now our Federated XGBoost training is complete, where the BST is the "
"federated Boost object"
msgstr "到这里我们的联邦XGBoost训练就已经完成，bst就是我们这里构建好的FedBoost对象"

#: ../../tutorial/Federated_Xgboost.ipynb:744
msgid "Conclusion"
msgstr "总结"

#: ../../tutorial/Federated_Xgboost.ipynb:746
msgid "This tutorial introduces how to use tree models for training etc"
msgstr "本教程介绍如何使用树模型进行训练等"

#: ../../tutorial/Federated_Xgboost.ipynb:747
msgid ""
"SFXgboost encapsulates the logic of the federated subtree model. "
"Sfxgboost trained models remain compatible with XGBoost, and we can "
"directly use the existing infrastructure for online prediction and so on."
msgstr ""
"SFXgboost 封装了联邦子树模型的建树逻辑。经过 SFXgboost 训练的模型仍然与 XGBoost "
"兼容，我们可以直接使用现有的基础设施进行在线预测等。"

#: ../../tutorial/Federated_Xgboost.ipynb:748
msgid ""
"Next, you can try SFXgboost on your data, just need to follow this "
"tutorial"
msgstr "下一步，您可以将自己的数据应用在SFXgboost上面，只需要follow这个文档即可完成"

