# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Ant Group Co., Ltd.
# This file is distributed under the same license as the SecretFlow package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: SecretFlow \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-07-03 15:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../tutorial/Federated_Xgboost.ipynb:9
msgid "Horizontally Federated XGBoost"
msgstr "水平联邦XGBoost"

#: ../../tutorial/Federated_Xgboost.ipynb:20
msgid ""
"The following codes are demos only. It’s **NOT for production** due to "
"system security concerns, please **DO NOT** use it directly in "
"production."
msgstr "注意： 以下代码仅供演示用途，在演示过程中可能会揭露部分信息。请勿直接将此示例代码用于实际生产环境中。在实际部署前，请根据您的具体需求和安全标准进行必要的修改和调整。"

#: ../../tutorial/Federated_Xgboost.ipynb:31
msgid ""
"In this tutorial, we will learn how to use SecretFlow to train tree "
"models for horizontal federation. Secretflow provides tree modeling "
"capabilities for horizontal scenarios(SFXgboost), The usage of SFXgboost "
"is similar to XGBoost, you can easily convert your existing XGBoost "
"program into a federated model for SecretFlow."
msgstr ""
"在本教程中，我们将学习如何使用 `SecretFlow` 来训练水平联邦的树模型。Secretflow 为水平场景提供了 ``tree "
"modeling`` 能力（\\ ``SFXgboost``），``SFXgboost`` 类似于 ``XGBoost``，您可以轻松地将现有的 "
"XGBoost 程序转换为 `SecretFlow` 的联合模型。"

#: ../../tutorial/Federated_Xgboost.ipynb:43
msgid "Xgboost"
msgstr ""

#: ../../tutorial/Federated_Xgboost.ipynb:54
msgid ""
"XGBoost is an optimized distributed gradient boosting library designed to"
" be highly efficient, flexible and portable. It implements machine "
"learning algorithms under the Gradient Boosting framework."
msgstr "XGBoost 是一个优化的分布式梯度提升库，旨在高效、灵活和便携。 它在 Gradient Boosting 框架下实现机器学习算法。"

#: ../../tutorial/Federated_Xgboost.ipynb:56
msgid ""
"official tutorial `XGBoost tutorials "
"<https://xgboost.readthedocs.io/en/latest/tutorials/index.html>`__."
msgstr ""
"官方文档 `XGBoost "
"tutorials<https://xgboost.readthedocs.io/en/latest/tutorials/index.html>`__"
" 。"

#: ../../tutorial/Federated_Xgboost.ipynb:68
msgid "prepare secretflow devices"
msgstr "准备secretflow devices"

#: ../../tutorial/Federated_Xgboost.ipynb:127
msgid "XGBoost Example"
msgstr "XGBoost示例"

#: ../../tutorial/Federated_Xgboost.ipynb:348
msgid "Then, How to do federated xgboost in secretflow?"
msgstr "那么，我们在SecretFlow中应该怎么做联邦XGBoost呢"

#: ../../tutorial/Federated_Xgboost.ipynb:350
msgid ""
"Use federate Binning method based on iteration to calculate the global "
"bucket information combined with the data of all sides, which was used as"
" the candidate to enter the subsequent construction procedure."
msgstr "使用基于迭代的federate binning方法联合各方数据计算全局分桶信息，作为candidate splits进入后续的建树流程。"

#: ../../tutorial/Federated_Xgboost.ipynb:351
msgid "The data is input into each Client XGBoost engine to calculate G & H."
msgstr "数据输入到各个Client xgboost引擎中，计算G & H。"

#: ../../tutorial/Federated_Xgboost.ipynb:352
msgid "Train federated boosting model"
msgstr "进行联邦建树流程 "

#: ../../tutorial/Federated_Xgboost.ipynb:354
msgid "Data is reassigned to the node to be split."
msgstr "进行数据reassign，分配到待分裂的节点上；"

#: ../../tutorial/Federated_Xgboost.ipynb:355
msgid ""
"The sum of grad and the sum of hess are calculated according to the "
"previously calculated binning buckets."
msgstr "根据之前计算好的binning分桶计算sum_of_grad 和sum_of_hess；"

#: ../../tutorial/Federated_Xgboost.ipynb:356
msgid ""
"Send the sum of grad and the sum of hess to server，server use secure "
"aggregation to produce global summary，then choose best split point，Send "
"best split info back to clients."
msgstr "发送给server端，server端做secure aggregation，挑选分裂信息发送回client端；"

#: ../../tutorial/Federated_Xgboost.ipynb:357
msgid "Clients Updates local model."
msgstr "Clients更新本地模型；"

#: ../../tutorial/Federated_Xgboost.ipynb:359
msgid "Finish training，and save model."
msgstr "完成训练，并保存模型。"

#: ../../tutorial/Federated_Xgboost.ipynb:370
msgid ""
"Create 3 entities in the Secretflow environment [Alice, Bob, Charlie]. "
"Where ``Alice`` and ``Bob`` are clients, and ``Charlie`` is the "
"server,then you can happily start ``Federate Boosting``."
msgstr ""
"在 Secretflow 环境中创建 3 个实体 [Alice, Bob, Charlie]， ``Alice`` 和 ``Bob`` 是客户端，"
" ``Charlie`` 是服务器，那么你可以愉快地开始 ``Federate Boosting`` 。了。"

#: ../../tutorial/Federated_Xgboost.ipynb:382
msgid "Prepare Data"
msgstr "准备数据"

#: ../../tutorial/Federated_Xgboost.ipynb:442
msgid "Prepare Params"
msgstr "准备超参"

#: ../../tutorial/Federated_Xgboost.ipynb:496
msgid "Create SFXgboost"
msgstr "创建SFXgboost"

#: ../../tutorial/Federated_Xgboost.ipynb:520
msgid "run SFXgboost"
msgstr "运行SFXgboost"

#: ../../tutorial/Federated_Xgboost.ipynb:736
msgid ""
"Now our Federated XGBoost training is complete, where the BST is the "
"federated Boost object."
msgstr "到这里我们的联邦XGBoost训练就已经完成，bst就是我们这里构建好的FedBoost对象。"

#: ../../tutorial/Federated_Xgboost.ipynb:748
msgid "Conclusion"
msgstr "总结"

#: ../../tutorial/Federated_Xgboost.ipynb:750
msgid "This tutorial introduces how to use tree models for training etc."
msgstr "本教程介绍如何使用树模型进行训练等。"

#: ../../tutorial/Federated_Xgboost.ipynb:751
msgid ""
"SFXgboost encapsulates the logic of the federated subtree model. "
"Sfxgboost trained models remain compatible with XGBoost, and we can "
"directly use the existing infrastructure for online prediction and so on."
msgstr ""
"SFXgboost 封装了联邦子树模型的建树逻辑。经过 SFXgboost 训练的模型仍然与 XGBoost "
"兼容，我们可以直接使用现有的基础设施进行在线预测等。"

#: ../../tutorial/Federated_Xgboost.ipynb:752
msgid ""
"Next, you can try SFXgboost on your data, just need to follow this "
"tutorial."
msgstr "下一步，您可以将自己的数据应用在SFXgboost上面，只需要follow这个文档即可完成。"

